Started training, total epoch : 30
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.683808
Trained 51 batches 	Training Loss: 0.233203
Trained 101 batches 	Training Loss: 0.197336
Trained 151 batches 	Training Loss: 0.129443
Trained 201 batches 	Training Loss: 0.219727
Trained 251 batches 	Training Loss: 0.171745
Trained 301 batches 	Training Loss: 0.218825
Trained 351 batches 	Training Loss: 0.154574
Trained 401 batches 	Training Loss: 0.137704
Trained 451 batches 	Training Loss: 0.166531
Trained 501 batches 	Training Loss: 0.221214
Trained 551 batches 	Training Loss: 0.210286
Trained 601 batches 	Training Loss: 0.148170
Trained 651 batches 	Training Loss: 0.255045
Trained 701 batches 	Training Loss: 0.173791
Trained 751 batches 	Training Loss: 0.148783
Trained 801 batches 	Training Loss: 0.209376
Trained 851 batches 	Training Loss: 0.224012
Trained 901 batches 	Training Loss: 0.092560
Trained 951 batches 	Training Loss: 0.123361
Trained 1001 batches 	Training Loss: 0.143348
Trained 1051 batches 	Training Loss: 0.131663
Trained 1101 batches 	Training Loss: 0.124167
Trained 1151 batches 	Training Loss: 0.104950
Trained 1201 batches 	Training Loss: 0.135979
Trained 1251 batches 	Training Loss: 0.138983
Trained 1301 batches 	Training Loss: 0.165022
Trained 1351 batches 	Training Loss: 0.153379
Trained 1401 batches 	Training Loss: 0.173309
Trained 1451 batches 	Training Loss: 0.182499
Trained 1501 batches 	Training Loss: 0.227183
Trained 1551 batches 	Training Loss: 0.181419
Trained 1601 batches 	Training Loss: 0.146489
Trained 1651 batches 	Training Loss: 0.099014
Trained 1701 batches 	Training Loss: 0.156037
Trained 1751 batches 	Training Loss: 0.099227
Trained 1801 batches 	Training Loss: 0.126520
Trained 1851 batches 	Training Loss: 0.130766
Trained 1901 batches 	Training Loss: 0.235071
Trained 1951 batches 	Training Loss: 0.137555
Trained 2001 batches 	Training Loss: 0.147731
Trained 2051 batches 	Training Loss: 0.097774
Trained 2101 batches 	Training Loss: 0.163287
Trained 2151 batches 	Training Loss: 0.079656
Trained 2201 batches 	Training Loss: 0.234972
Trained 2251 batches 	Training Loss: 0.056786
Trained 2301 batches 	Training Loss: 0.161005
Trained 2351 batches 	Training Loss: 0.095996
Trained 2401 batches 	Training Loss: 0.085059
Trained 2451 batches 	Training Loss: 0.090144
Trained 2501 batches 	Training Loss: 0.136419
Trained 2551 batches 	Training Loss: 0.170122
Trained 2601 batches 	Training Loss: 0.212682
Trained 2651 batches 	Training Loss: 0.153132
Trained 2701 batches 	Training Loss: 0.106019
Trained 2751 batches 	Training Loss: 0.173001
Trained 2801 batches 	Training Loss: 0.213999
Trained 2851 batches 	Training Loss: 0.157161
Trained 2901 batches 	Training Loss: 0.173577
Trained 2951 batches 	Training Loss: 0.156615
Trained 3001 batches 	Training Loss: 0.172168
Trained 3051 batches 	Training Loss: 0.184935
Trained 3101 batches 	Training Loss: 0.143273
Trained 3151 batches 	Training Loss: 0.074847
Trained 3201 batches 	Training Loss: 0.116783
Trained 3251 batches 	Training Loss: 0.132034
Trained 3301 batches 	Training Loss: 0.149354
Trained 3351 batches 	Training Loss: 0.199546
Trained 3401 batches 	Training Loss: 0.149816
Trained 3451 batches 	Training Loss: 0.158038
Trained 3501 batches 	Training Loss: 0.198608
Trained 3551 batches 	Training Loss: 0.133099
Trained 3601 batches 	Training Loss: 0.110664
Trained 3651 batches 	Training Loss: 0.127943
Trained 3701 batches 	Training Loss: 0.189394
Trained 3751 batches 	Training Loss: 0.159530
Trained 3801 batches 	Training Loss: 0.126318
Trained 3851 batches 	Training Loss: 0.174381
Trained 3901 batches 	Training Loss: 0.182378
Trained 3951 batches 	Training Loss: 0.132448
Trained 4001 batches 	Training Loss: 0.320566
Trained 4051 batches 	Training Loss: 0.157004
Trained 4101 batches 	Training Loss: 0.176781
Trained 4151 batches 	Training Loss: 0.187557
Trained 4201 batches 	Training Loss: 0.137996
Trained 4251 batches 	Training Loss: 0.256543
Trained 4301 batches 	Training Loss: 0.200778
Trained 4351 batches 	Training Loss: 0.132678
Trained 4401 batches 	Training Loss: 0.160506
Trained 4451 batches 	Training Loss: 0.104835
Trained 4501 batches 	Training Loss: 0.158636
Trained 4551 batches 	Training Loss: 0.151646
Trained 4601 batches 	Training Loss: 0.122021
Trained 4651 batches 	Training Loss: 0.183940
Trained 4701 batches 	Training Loss: 0.189934
Trained 4751 batches 	Training Loss: 0.159506
Trained 4801 batches 	Training Loss: 0.179306
Trained 4851 batches 	Training Loss: 0.159290
Trained 4901 batches 	Training Loss: 0.115047
Epoch: 1 	Training Loss: 0.158347
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1541284
The average AUROC is 0.812
The AUROC of Atelectasis is 0.7893581913339751
The AUROC of Cardiomegaly is 0.9064846423948753
The AUROC of Effusion is 0.884717656892648
The AUROC of Infiltration is 0.7033211153835514
The AUROC of Mass is 0.8126041175598154
The AUROC of Nodule is 0.7513448437489967
The AUROC of Pneumonia is 0.728990763757929
The AUROC of Pneumothorax is 0.8705943722352131
The AUROC of Consolidation is 0.7607223718904046
The AUROC of Edema is 0.9094679037277361
The AUROC of Emphysema is 0.874991903096256
The AUROC of Fibrosis is 0.7761560178345929
The AUROC of Pleural_Thickening is 0.7658175055128036
The AUROC of Hernia is 0.8265665369113644
Epoch: 1 	Learning Rate for first group: 0.0001000000
Started epoch 2
Trained 1 batches 	Training Loss: 0.167787
Trained 51 batches 	Training Loss: 0.117552
Trained 101 batches 	Training Loss: 0.179967
Trained 151 batches 	Training Loss: 0.097518
Trained 201 batches 	Training Loss: 0.200592
Trained 251 batches 	Training Loss: 0.107075
Trained 301 batches 	Training Loss: 0.112157
Trained 351 batches 	Training Loss: 0.086067
Trained 401 batches 	Training Loss: 0.100472
Trained 451 batches 	Training Loss: 0.136284
Trained 501 batches 	Training Loss: 0.166508
Trained 551 batches 	Training Loss: 0.083851
Trained 601 batches 	Training Loss: 0.145061
Trained 651 batches 	Training Loss: 0.192579
Trained 701 batches 	Training Loss: 0.097048
Trained 751 batches 	Training Loss: 0.146817
Trained 801 batches 	Training Loss: 0.154246
Trained 851 batches 	Training Loss: 0.138111
Trained 901 batches 	Training Loss: 0.203684
Trained 951 batches 	Training Loss: 0.162062
Trained 1001 batches 	Training Loss: 0.187875
Trained 1051 batches 	Training Loss: 0.135994
Trained 1101 batches 	Training Loss: 0.148920
Trained 1151 batches 	Training Loss: 0.124196
Trained 1201 batches 	Training Loss: 0.178174
Trained 1251 batches 	Training Loss: 0.127416
Trained 1301 batches 	Training Loss: 0.170621
Trained 1351 batches 	Training Loss: 0.191977
Trained 1401 batches 	Training Loss: 0.188242
Trained 1451 batches 	Training Loss: 0.184591
Trained 1501 batches 	Training Loss: 0.157370
Trained 1551 batches 	Training Loss: 0.066593
Trained 1601 batches 	Training Loss: 0.134319
Trained 1651 batches 	Training Loss: 0.196239
Trained 1701 batches 	Training Loss: 0.140452
Trained 1751 batches 	Training Loss: 0.079187
Trained 1801 batches 	Training Loss: 0.152820
Trained 1851 batches 	Training Loss: 0.111134
Trained 1901 batches 	Training Loss: 0.150510
Trained 1951 batches 	Training Loss: 0.119714
Trained 2001 batches 	Training Loss: 0.179908
Trained 2051 batches 	Training Loss: 0.121802
Trained 2101 batches 	Training Loss: 0.136186
Trained 2151 batches 	Training Loss: 0.140114
Trained 2201 batches 	Training Loss: 0.165727
Trained 2251 batches 	Training Loss: 0.218849
Trained 2301 batches 	Training Loss: 0.231855
Trained 2351 batches 	Training Loss: 0.192894
Trained 2401 batches 	Training Loss: 0.092698
Trained 2451 batches 	Training Loss: 0.139355
Trained 2501 batches 	Training Loss: 0.123916
Trained 2551 batches 	Training Loss: 0.122562
Trained 2601 batches 	Training Loss: 0.173380
Trained 2651 batches 	Training Loss: 0.148361
Trained 2701 batches 	Training Loss: 0.122711
Trained 2751 batches 	Training Loss: 0.150811
Trained 2801 batches 	Training Loss: 0.214726
Trained 2851 batches 	Training Loss: 0.185590
Trained 2901 batches 	Training Loss: 0.118321
Trained 2951 batches 	Training Loss: 0.176635
Trained 3001 batches 	Training Loss: 0.130380
Trained 3051 batches 	Training Loss: 0.152538
Trained 3101 batches 	Training Loss: 0.175065
Trained 3151 batches 	Training Loss: 0.089184
Trained 3201 batches 	Training Loss: 0.116154
Trained 3251 batches 	Training Loss: 0.147884
Trained 3301 batches 	Training Loss: 0.165076
Trained 3351 batches 	Training Loss: 0.102116
Trained 3401 batches 	Training Loss: 0.104928
Trained 3451 batches 	Training Loss: 0.189779
Trained 3501 batches 	Training Loss: 0.104659
Trained 3551 batches 	Training Loss: 0.131413
Trained 3601 batches 	Training Loss: 0.127667
Trained 3651 batches 	Training Loss: 0.154423
Trained 3701 batches 	Training Loss: 0.196236
Trained 3751 batches 	Training Loss: 0.119721
Trained 3801 batches 	Training Loss: 0.116921
Trained 3851 batches 	Training Loss: 0.161179
Trained 3901 batches 	Training Loss: 0.164118
Trained 3951 batches 	Training Loss: 0.110805
Trained 4001 batches 	Training Loss: 0.150245
Trained 4051 batches 	Training Loss: 0.090227
Trained 4101 batches 	Training Loss: 0.196640
Trained 4151 batches 	Training Loss: 0.173441
Trained 4201 batches 	Training Loss: 0.125022
Trained 4251 batches 	Training Loss: 0.139786
Trained 4301 batches 	Training Loss: 0.144187
Trained 4351 batches 	Training Loss: 0.112958
Trained 4401 batches 	Training Loss: 0.150580
Trained 4451 batches 	Training Loss: 0.150439
Trained 4501 batches 	Training Loss: 0.130585
Trained 4551 batches 	Training Loss: 0.119217
Trained 4601 batches 	Training Loss: 0.166139
Trained 4651 batches 	Training Loss: 0.106176
Trained 4701 batches 	Training Loss: 0.092712
Trained 4751 batches 	Training Loss: 0.118585
Trained 4801 batches 	Training Loss: 0.124271
Trained 4851 batches 	Training Loss: 0.147699
Trained 4901 batches 	Training Loss: 0.131540
Epoch: 2 	Training Loss: 0.146020
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1543077
The average AUROC is 0.813
The AUROC of Atelectasis is 0.8030010017281993
The AUROC of Cardiomegaly is 0.8953524247712533
The AUROC of Effusion is 0.8862370674623165
The AUROC of Infiltration is 0.6981092976292567
The AUROC of Mass is 0.8082911842056475
The AUROC of Nodule is 0.7472611217768304
The AUROC of Pneumonia is 0.7380195534982952
The AUROC of Pneumothorax is 0.8791351594845476
The AUROC of Consolidation is 0.7515884331970397
The AUROC of Edema is 0.9123659206780109
The AUROC of Emphysema is 0.8531107568102321
The AUROC of Fibrosis is 0.795033017590614
The AUROC of Pleural_Thickening is 0.768004047829829
The AUROC of Hernia is 0.8436850505816023
Epoch: 2 	Learning Rate for first group: 0.0001000000
Started epoch 3
Trained 1 batches 	Training Loss: 0.130774
Trained 51 batches 	Training Loss: 0.125341
Trained 101 batches 	Training Loss: 0.103508
Trained 151 batches 	Training Loss: 0.142192
Trained 201 batches 	Training Loss: 0.148073
Trained 251 batches 	Training Loss: 0.121104
Trained 301 batches 	Training Loss: 0.097366
Trained 351 batches 	Training Loss: 0.189601
Trained 401 batches 	Training Loss: 0.088894
Trained 451 batches 	Training Loss: 0.157752
Trained 501 batches 	Training Loss: 0.158963
Trained 551 batches 	Training Loss: 0.108851
Trained 601 batches 	Training Loss: 0.132258
Trained 651 batches 	Training Loss: 0.120255
Trained 701 batches 	Training Loss: 0.207837
Trained 751 batches 	Training Loss: 0.193092
Trained 801 batches 	Training Loss: 0.209295
Trained 851 batches 	Training Loss: 0.090308
Trained 901 batches 	Training Loss: 0.115863
Trained 951 batches 	Training Loss: 0.184989
Trained 1001 batches 	Training Loss: 0.109870
Trained 1051 batches 	Training Loss: 0.142703
Trained 1101 batches 	Training Loss: 0.140544
Trained 1151 batches 	Training Loss: 0.096736
Trained 1201 batches 	Training Loss: 0.091601
Trained 1251 batches 	Training Loss: 0.098443
Trained 1301 batches 	Training Loss: 0.155369
Trained 1351 batches 	Training Loss: 0.189717
Trained 1401 batches 	Training Loss: 0.169391
Trained 1451 batches 	Training Loss: 0.101220
Trained 1501 batches 	Training Loss: 0.130956
Trained 1551 batches 	Training Loss: 0.146813
Trained 1601 batches 	Training Loss: 0.117178
Trained 1651 batches 	Training Loss: 0.128939
Trained 1701 batches 	Training Loss: 0.127079
Trained 1751 batches 	Training Loss: 0.124794
Trained 1801 batches 	Training Loss: 0.115458
Trained 1851 batches 	Training Loss: 0.175391
Trained 1901 batches 	Training Loss: 0.178375
Trained 1951 batches 	Training Loss: 0.172740
Trained 2001 batches 	Training Loss: 0.166061
Trained 2051 batches 	Training Loss: 0.166216
Trained 2101 batches 	Training Loss: 0.217182
Trained 2151 batches 	Training Loss: 0.181159
Trained 2201 batches 	Training Loss: 0.220901
Trained 2251 batches 	Training Loss: 0.145528
Trained 2301 batches 	Training Loss: 0.174130
Trained 2351 batches 	Training Loss: 0.124897
Trained 2401 batches 	Training Loss: 0.121715
Trained 2451 batches 	Training Loss: 0.115093
Trained 2501 batches 	Training Loss: 0.186824
Trained 2551 batches 	Training Loss: 0.214119
Trained 2601 batches 	Training Loss: 0.192407
Trained 2651 batches 	Training Loss: 0.080930
Trained 2701 batches 	Training Loss: 0.181274
Trained 2751 batches 	Training Loss: 0.083274
Trained 2801 batches 	Training Loss: 0.154602
Trained 2851 batches 	Training Loss: 0.177316
Trained 2901 batches 	Training Loss: 0.127184
Trained 2951 batches 	Training Loss: 0.145612
Trained 3001 batches 	Training Loss: 0.109188
Trained 3051 batches 	Training Loss: 0.110339
Trained 3101 batches 	Training Loss: 0.187049
Trained 3151 batches 	Training Loss: 0.138719
Trained 3201 batches 	Training Loss: 0.195798
Trained 3251 batches 	Training Loss: 0.141267
Trained 3301 batches 	Training Loss: 0.129644
Trained 3351 batches 	Training Loss: 0.240219
Trained 3401 batches 	Training Loss: 0.129573
Trained 3451 batches 	Training Loss: 0.125805
Trained 3501 batches 	Training Loss: 0.146822
Trained 3551 batches 	Training Loss: 0.125281
Trained 3601 batches 	Training Loss: 0.154290
Trained 3651 batches 	Training Loss: 0.183963
Trained 3701 batches 	Training Loss: 0.155140
Trained 3751 batches 	Training Loss: 0.131581
Trained 3801 batches 	Training Loss: 0.118194
Trained 3851 batches 	Training Loss: 0.259798
Trained 3901 batches 	Training Loss: 0.175516
Trained 3951 batches 	Training Loss: 0.188659
Trained 4001 batches 	Training Loss: 0.125961
Trained 4051 batches 	Training Loss: 0.123648
Trained 4101 batches 	Training Loss: 0.086852
Trained 4151 batches 	Training Loss: 0.107882
Trained 4201 batches 	Training Loss: 0.218387
Trained 4251 batches 	Training Loss: 0.114650
Trained 4301 batches 	Training Loss: 0.196839
Trained 4351 batches 	Training Loss: 0.072779
Trained 4401 batches 	Training Loss: 0.094527
Trained 4451 batches 	Training Loss: 0.102840
Trained 4501 batches 	Training Loss: 0.198235
Trained 4551 batches 	Training Loss: 0.170838
Trained 4601 batches 	Training Loss: 0.135948
Trained 4651 batches 	Training Loss: 0.156494
Trained 4701 batches 	Training Loss: 0.115639
Trained 4751 batches 	Training Loss: 0.109248
Trained 4801 batches 	Training Loss: 0.095242
Trained 4851 batches 	Training Loss: 0.162072
Trained 4901 batches 	Training Loss: 0.192889
Epoch: 3 	Training Loss: 0.140142
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1510032
The average AUROC is 0.825
The AUROC of Atelectasis is 0.8001721199104348
The AUROC of Cardiomegaly is 0.9136421765022563
The AUROC of Effusion is 0.8908726130045926
The AUROC of Infiltration is 0.6977323577415696
The AUROC of Mass is 0.8358575082499509
The AUROC of Nodule is 0.7608902700816944
The AUROC of Pneumonia is 0.7610184867706727
The AUROC of Pneumothorax is 0.8813703188425602
The AUROC of Consolidation is 0.7653258626619284
The AUROC of Edema is 0.90825829305436
The AUROC of Emphysema is 0.8914897125157523
The AUROC of Fibrosis is 0.7877501119353965
The AUROC of Pleural_Thickening is 0.7717345853230458
The AUROC of Hernia is 0.8819111922560199
Epoch: 3 	Learning Rate for first group: 0.0001000000
Started epoch 4
Trained 1 batches 	Training Loss: 0.090082
Trained 51 batches 	Training Loss: 0.142548
Trained 101 batches 	Training Loss: 0.143065
Trained 151 batches 	Training Loss: 0.105075
Trained 201 batches 	Training Loss: 0.146442
Trained 251 batches 	Training Loss: 0.114317
Trained 301 batches 	Training Loss: 0.115907
Trained 351 batches 	Training Loss: 0.201319
Trained 401 batches 	Training Loss: 0.122211
Trained 451 batches 	Training Loss: 0.213829
Trained 501 batches 	Training Loss: 0.118607
Trained 551 batches 	Training Loss: 0.100688
Trained 601 batches 	Training Loss: 0.149900
Trained 651 batches 	Training Loss: 0.125168
Trained 701 batches 	Training Loss: 0.165103
Trained 751 batches 	Training Loss: 0.120928
Trained 801 batches 	Training Loss: 0.077528
Trained 851 batches 	Training Loss: 0.125203
Trained 901 batches 	Training Loss: 0.101837
Trained 951 batches 	Training Loss: 0.117785
Trained 1001 batches 	Training Loss: 0.169023
Trained 1051 batches 	Training Loss: 0.127108
Trained 1101 batches 	Training Loss: 0.130152
Trained 1151 batches 	Training Loss: 0.129614
Trained 1201 batches 	Training Loss: 0.114634
Trained 1251 batches 	Training Loss: 0.078161
Trained 1301 batches 	Training Loss: 0.128671
Trained 1351 batches 	Training Loss: 0.124226
Trained 1401 batches 	Training Loss: 0.234905
Trained 1451 batches 	Training Loss: 0.125459
Trained 1501 batches 	Training Loss: 0.160567
Trained 1551 batches 	Training Loss: 0.106241
Trained 1601 batches 	Training Loss: 0.204282
Trained 1651 batches 	Training Loss: 0.121693
Trained 1701 batches 	Training Loss: 0.067993
Trained 1751 batches 	Training Loss: 0.227618
Trained 1801 batches 	Training Loss: 0.129820
Trained 1851 batches 	Training Loss: 0.126710
Trained 1901 batches 	Training Loss: 0.134943
Trained 1951 batches 	Training Loss: 0.107595
Trained 2001 batches 	Training Loss: 0.088029
Trained 2051 batches 	Training Loss: 0.101720
Trained 2101 batches 	Training Loss: 0.137862
Trained 2151 batches 	Training Loss: 0.122381
Trained 2201 batches 	Training Loss: 0.185001
Trained 2251 batches 	Training Loss: 0.091342
Trained 2301 batches 	Training Loss: 0.160526
Trained 2351 batches 	Training Loss: 0.184896
Trained 2401 batches 	Training Loss: 0.110549
Trained 2451 batches 	Training Loss: 0.156754
Trained 2501 batches 	Training Loss: 0.153673
Trained 2551 batches 	Training Loss: 0.107128
Trained 2601 batches 	Training Loss: 0.086631
Trained 2651 batches 	Training Loss: 0.130714
Trained 2701 batches 	Training Loss: 0.119571
Trained 2751 batches 	Training Loss: 0.153451
Trained 2801 batches 	Training Loss: 0.091643
Trained 2851 batches 	Training Loss: 0.141945
Trained 2901 batches 	Training Loss: 0.211556
Trained 2951 batches 	Training Loss: 0.208238
Trained 3001 batches 	Training Loss: 0.228399
Trained 3051 batches 	Training Loss: 0.094644
Trained 3101 batches 	Training Loss: 0.153638
Trained 3151 batches 	Training Loss: 0.157660
Trained 3201 batches 	Training Loss: 0.166597
Trained 3251 batches 	Training Loss: 0.161817
Trained 3301 batches 	Training Loss: 0.180770
Trained 3351 batches 	Training Loss: 0.112041
Trained 3401 batches 	Training Loss: 0.089057
Trained 3451 batches 	Training Loss: 0.103677
Trained 3501 batches 	Training Loss: 0.130122
Trained 3551 batches 	Training Loss: 0.140377
Trained 3601 batches 	Training Loss: 0.090472
Trained 3651 batches 	Training Loss: 0.121578
Trained 3701 batches 	Training Loss: 0.129690
Trained 3751 batches 	Training Loss: 0.133740
Trained 3801 batches 	Training Loss: 0.066312
Trained 3851 batches 	Training Loss: 0.136093
Trained 3901 batches 	Training Loss: 0.072264
Trained 3951 batches 	Training Loss: 0.113594
Trained 4001 batches 	Training Loss: 0.140661
Trained 4051 batches 	Training Loss: 0.108284
Trained 4101 batches 	Training Loss: 0.187031
Trained 4151 batches 	Training Loss: 0.084557
Trained 4201 batches 	Training Loss: 0.150035
Trained 4251 batches 	Training Loss: 0.074263
Trained 4301 batches 	Training Loss: 0.127013
Trained 4351 batches 	Training Loss: 0.099556
Trained 4401 batches 	Training Loss: 0.097924
Trained 4451 batches 	Training Loss: 0.109448
Trained 4501 batches 	Training Loss: 0.187778
Trained 4551 batches 	Training Loss: 0.126734
Trained 4601 batches 	Training Loss: 0.090609
Trained 4651 batches 	Training Loss: 0.174377
Trained 4701 batches 	Training Loss: 0.122772
Trained 4751 batches 	Training Loss: 0.116555
Trained 4801 batches 	Training Loss: 0.093092
Trained 4851 batches 	Training Loss: 0.080893
Trained 4901 batches 	Training Loss: 0.090464
Epoch: 4 	Training Loss: 0.133380
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1531381
The average AUROC is 0.828
The AUROC of Atelectasis is 0.8023749540121976
The AUROC of Cardiomegaly is 0.9073773481097807
The AUROC of Effusion is 0.8881614622832122
The AUROC of Infiltration is 0.7085461838501755
The AUROC of Mass is 0.8174408665229436
The AUROC of Nodule is 0.7659290554881368
The AUROC of Pneumonia is 0.753469841133779
The AUROC of Pneumothorax is 0.879644936833794
The AUROC of Consolidation is 0.7680213826474072
The AUROC of Edema is 0.9054695259220022
The AUROC of Emphysema is 0.8950206986302982
The AUROC of Fibrosis is 0.7964223570511256
The AUROC of Pleural_Thickening is 0.7943050489535024
The AUROC of Hernia is 0.9091368401713229
Epoch: 4 	Learning Rate for first group: 0.0001000000
Started epoch 5
Trained 1 batches 	Training Loss: 0.091797
Trained 51 batches 	Training Loss: 0.105303
Trained 101 batches 	Training Loss: 0.080228
Trained 151 batches 	Training Loss: 0.118509
Trained 201 batches 	Training Loss: 0.121218
Trained 251 batches 	Training Loss: 0.111125
Trained 301 batches 	Training Loss: 0.146952
Trained 351 batches 	Training Loss: 0.119283
Trained 401 batches 	Training Loss: 0.099351
Trained 451 batches 	Training Loss: 0.132389
Trained 501 batches 	Training Loss: 0.105797
Trained 551 batches 	Training Loss: 0.129708
Trained 601 batches 	Training Loss: 0.118402
Trained 651 batches 	Training Loss: 0.125559
Trained 701 batches 	Training Loss: 0.098887
Trained 751 batches 	Training Loss: 0.166953
Trained 801 batches 	Training Loss: 0.097455
Trained 851 batches 	Training Loss: 0.117907
Trained 901 batches 	Training Loss: 0.091580
Trained 951 batches 	Training Loss: 0.098319
Trained 1001 batches 	Training Loss: 0.171993
Trained 1051 batches 	Training Loss: 0.092261
Trained 1101 batches 	Training Loss: 0.141845
Trained 1151 batches 	Training Loss: 0.105665
Trained 1201 batches 	Training Loss: 0.071929
Trained 1251 batches 	Training Loss: 0.238155
Trained 1301 batches 	Training Loss: 0.092936
Trained 1351 batches 	Training Loss: 0.194157
Trained 1401 batches 	Training Loss: 0.228691
Trained 1451 batches 	Training Loss: 0.183820
Trained 1501 batches 	Training Loss: 0.075225
Trained 1551 batches 	Training Loss: 0.064442
Trained 1601 batches 	Training Loss: 0.179305
Trained 1651 batches 	Training Loss: 0.129846
Trained 1701 batches 	Training Loss: 0.118566
Trained 1751 batches 	Training Loss: 0.145800
Trained 1801 batches 	Training Loss: 0.133436
Trained 1851 batches 	Training Loss: 0.115095
Trained 1901 batches 	Training Loss: 0.116798
Trained 1951 batches 	Training Loss: 0.121867
Trained 2001 batches 	Training Loss: 0.082920
Trained 2051 batches 	Training Loss: 0.091017
Trained 2101 batches 	Training Loss: 0.115215
Trained 2151 batches 	Training Loss: 0.140539
Trained 2201 batches 	Training Loss: 0.129871
Trained 2251 batches 	Training Loss: 0.125210
Trained 2301 batches 	Training Loss: 0.110130
Trained 2351 batches 	Training Loss: 0.088649
Trained 2401 batches 	Training Loss: 0.134397
Trained 2451 batches 	Training Loss: 0.114422
Trained 2501 batches 	Training Loss: 0.174751
Trained 2551 batches 	Training Loss: 0.117472
Trained 2601 batches 	Training Loss: 0.097152
Trained 2651 batches 	Training Loss: 0.108415
Trained 2701 batches 	Training Loss: 0.078585
Trained 2751 batches 	Training Loss: 0.100800
Trained 2801 batches 	Training Loss: 0.151437
Trained 2851 batches 	Training Loss: 0.177448
Trained 2901 batches 	Training Loss: 0.113380
Trained 2951 batches 	Training Loss: 0.181100
Trained 3001 batches 	Training Loss: 0.167490
Trained 3051 batches 	Training Loss: 0.178010
Trained 3101 batches 	Training Loss: 0.095937
Trained 3151 batches 	Training Loss: 0.102868
Trained 3201 batches 	Training Loss: 0.155784
Trained 3251 batches 	Training Loss: 0.127477
Trained 3301 batches 	Training Loss: 0.142835
Trained 3351 batches 	Training Loss: 0.151101
Trained 3401 batches 	Training Loss: 0.144136
Trained 3451 batches 	Training Loss: 0.223110
Trained 3501 batches 	Training Loss: 0.235052
Trained 3551 batches 	Training Loss: 0.093916
Trained 3601 batches 	Training Loss: 0.150702
Trained 3651 batches 	Training Loss: 0.144010
Trained 3701 batches 	Training Loss: 0.109089
Trained 3751 batches 	Training Loss: 0.161224
Trained 3801 batches 	Training Loss: 0.082960
Trained 3851 batches 	Training Loss: 0.135466
Trained 3901 batches 	Training Loss: 0.124084
Trained 3951 batches 	Training Loss: 0.104534
Trained 4001 batches 	Training Loss: 0.152118
Trained 4051 batches 	Training Loss: 0.122904
Trained 4101 batches 	Training Loss: 0.123126
Trained 4151 batches 	Training Loss: 0.129640
Trained 4201 batches 	Training Loss: 0.103819
Trained 4251 batches 	Training Loss: 0.086533
Trained 4301 batches 	Training Loss: 0.188334
Trained 4351 batches 	Training Loss: 0.134140
Trained 4401 batches 	Training Loss: 0.123417
Trained 4451 batches 	Training Loss: 0.151074
Trained 4501 batches 	Training Loss: 0.131156
Trained 4551 batches 	Training Loss: 0.095946
Trained 4601 batches 	Training Loss: 0.157598
Trained 4651 batches 	Training Loss: 0.082454
Trained 4701 batches 	Training Loss: 0.137792
Trained 4751 batches 	Training Loss: 0.098168
Trained 4801 batches 	Training Loss: 0.164440
Trained 4851 batches 	Training Loss: 0.153981
Trained 4901 batches 	Training Loss: 0.134456
Epoch: 5 	Training Loss: 0.124888
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1596903
The average AUROC is 0.817
The AUROC of Atelectasis is 0.7920100789879667
The AUROC of Cardiomegaly is 0.8999423214560858
The AUROC of Effusion is 0.882570878702161
The AUROC of Infiltration is 0.6955159478823757
The AUROC of Mass is 0.8218042033698997
The AUROC of Nodule is 0.7618700893994165
The AUROC of Pneumonia is 0.7252878735927768
The AUROC of Pneumothorax is 0.8556975804002551
The AUROC of Consolidation is 0.7633690307870636
The AUROC of Edema is 0.9071769681520228
The AUROC of Emphysema is 0.8682170029089968
The AUROC of Fibrosis is 0.7911937005171817
The AUROC of Pleural_Thickening is 0.7902649403068156
The AUROC of Hernia is 0.8784388025767337
Epoch: 5 	Learning Rate for first group: 0.0001000000
Started epoch 6
Trained 1 batches 	Training Loss: 0.060223
Trained 51 batches 	Training Loss: 0.144951
Trained 101 batches 	Training Loss: 0.092720
Trained 151 batches 	Training Loss: 0.072157
Trained 201 batches 	Training Loss: 0.101762
Trained 251 batches 	Training Loss: 0.078386
Trained 301 batches 	Training Loss: 0.162971
Trained 351 batches 	Training Loss: 0.088780
Trained 401 batches 	Training Loss: 0.076061
Trained 451 batches 	Training Loss: 0.099404
Trained 501 batches 	Training Loss: 0.148209
Trained 551 batches 	Training Loss: 0.106227
Trained 601 batches 	Training Loss: 0.095720
Trained 651 batches 	Training Loss: 0.106157
Trained 701 batches 	Training Loss: 0.080864
Trained 751 batches 	Training Loss: 0.107680
Trained 801 batches 	Training Loss: 0.090516
Trained 851 batches 	Training Loss: 0.074254
Trained 901 batches 	Training Loss: 0.099865
Trained 951 batches 	Training Loss: 0.060274
Trained 1001 batches 	Training Loss: 0.159166
Trained 1051 batches 	Training Loss: 0.182097
Trained 1101 batches 	Training Loss: 0.128064
Trained 1151 batches 	Training Loss: 0.082506
Trained 1201 batches 	Training Loss: 0.175776
Trained 1251 batches 	Training Loss: 0.095681
Trained 1301 batches 	Training Loss: 0.080642
Trained 1351 batches 	Training Loss: 0.077764
Trained 1401 batches 	Training Loss: 0.111508
Trained 1451 batches 	Training Loss: 0.073248
Trained 1501 batches 	Training Loss: 0.098899
Trained 1551 batches 	Training Loss: 0.131623
Trained 1601 batches 	Training Loss: 0.069862
Trained 1651 batches 	Training Loss: 0.126327
Trained 1701 batches 	Training Loss: 0.108640
Trained 1751 batches 	Training Loss: 0.110832
Trained 1801 batches 	Training Loss: 0.089240
Trained 1851 batches 	Training Loss: 0.103214
Trained 1901 batches 	Training Loss: 0.114521
Trained 1951 batches 	Training Loss: 0.114347
Trained 2001 batches 	Training Loss: 0.107371
Trained 2051 batches 	Training Loss: 0.119092
Trained 2101 batches 	Training Loss: 0.082354
Trained 2151 batches 	Training Loss: 0.077842
Trained 2201 batches 	Training Loss: 0.076766
Trained 2251 batches 	Training Loss: 0.115658
Trained 2301 batches 	Training Loss: 0.176216
Trained 2351 batches 	Training Loss: 0.070596
Trained 2401 batches 	Training Loss: 0.104173
Trained 2451 batches 	Training Loss: 0.103185
Trained 2501 batches 	Training Loss: 0.058434
Trained 2551 batches 	Training Loss: 0.113929
Trained 2601 batches 	Training Loss: 0.094152
Trained 2651 batches 	Training Loss: 0.088183
Trained 2701 batches 	Training Loss: 0.131091
Trained 2751 batches 	Training Loss: 0.120997
Trained 2801 batches 	Training Loss: 0.103585
Trained 2851 batches 	Training Loss: 0.111330
Trained 2901 batches 	Training Loss: 0.093862
Trained 2951 batches 	Training Loss: 0.081671
Trained 3001 batches 	Training Loss: 0.050725
Trained 3051 batches 	Training Loss: 0.169450
Trained 3101 batches 	Training Loss: 0.082548
Trained 3151 batches 	Training Loss: 0.067374
Trained 3201 batches 	Training Loss: 0.080125
Trained 3251 batches 	Training Loss: 0.094803
Trained 3301 batches 	Training Loss: 0.104366
Trained 3351 batches 	Training Loss: 0.168062
Trained 3401 batches 	Training Loss: 0.076960
Trained 3451 batches 	Training Loss: 0.074028
Trained 3501 batches 	Training Loss: 0.093728
Trained 3551 batches 	Training Loss: 0.093071
Trained 3601 batches 	Training Loss: 0.081935
Trained 3651 batches 	Training Loss: 0.064382
Trained 3701 batches 	Training Loss: 0.069212
Trained 3751 batches 	Training Loss: 0.094599
Trained 3801 batches 	Training Loss: 0.085970
Trained 3851 batches 	Training Loss: 0.140417
Trained 3901 batches 	Training Loss: 0.081772
Trained 3951 batches 	Training Loss: 0.121355
Trained 4001 batches 	Training Loss: 0.064777
Trained 4051 batches 	Training Loss: 0.091745
Trained 4101 batches 	Training Loss: 0.070943
Trained 4151 batches 	Training Loss: 0.073582
Trained 4201 batches 	Training Loss: 0.099281
Trained 4251 batches 	Training Loss: 0.076682
Trained 4301 batches 	Training Loss: 0.068992
Trained 4351 batches 	Training Loss: 0.069912
Trained 4401 batches 	Training Loss: 0.056081
Trained 4451 batches 	Training Loss: 0.067961
Trained 4501 batches 	Training Loss: 0.077806
Trained 4551 batches 	Training Loss: 0.123517
Trained 4601 batches 	Training Loss: 0.123915
Trained 4651 batches 	Training Loss: 0.093367
Trained 4701 batches 	Training Loss: 0.088348
Trained 4751 batches 	Training Loss: 0.098837
Trained 4801 batches 	Training Loss: 0.111311
Trained 4851 batches 	Training Loss: 0.146083
Trained 4901 batches 	Training Loss: 0.137436
Epoch: 6 	Training Loss: 0.099886
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1643001
The average AUROC is 0.813
The AUROC of Atelectasis is 0.783400972240695
The AUROC of Cardiomegaly is 0.8849529204889632
The AUROC of Effusion is 0.8765338307989526
The AUROC of Infiltration is 0.6872909900688815
The AUROC of Mass is 0.8164589502036665
The AUROC of Nodule is 0.7606864442632342
The AUROC of Pneumonia is 0.736623640398499
The AUROC of Pneumothorax is 0.8573524983044327
The AUROC of Consolidation is 0.7533625584342796
The AUROC of Edema is 0.8937715189035291
The AUROC of Emphysema is 0.8801293885218293
The AUROC of Fibrosis is 0.7964673993184676
The AUROC of Pleural_Thickening is 0.7787374563964327
The AUROC of Hernia is 0.8804574735609219
Epoch: 6 	Learning Rate for first group: 0.0000100000
Started epoch 7
Trained 1 batches 	Training Loss: 0.099085
Trained 51 batches 	Training Loss: 0.077115
Trained 101 batches 	Training Loss: 0.082211
Trained 151 batches 	Training Loss: 0.059414
Trained 201 batches 	Training Loss: 0.104592
Trained 251 batches 	Training Loss: 0.081778
Trained 301 batches 	Training Loss: 0.072066
Trained 351 batches 	Training Loss: 0.076377
Trained 401 batches 	Training Loss: 0.090301
Trained 451 batches 	Training Loss: 0.118982
Trained 501 batches 	Training Loss: 0.066311
Trained 551 batches 	Training Loss: 0.125145
Trained 601 batches 	Training Loss: 0.083168
Trained 651 batches 	Training Loss: 0.082678
Trained 701 batches 	Training Loss: 0.086644
Trained 751 batches 	Training Loss: 0.060189
Trained 801 batches 	Training Loss: 0.065825
Trained 851 batches 	Training Loss: 0.085616
Trained 901 batches 	Training Loss: 0.047183
Trained 951 batches 	Training Loss: 0.120591
Trained 1001 batches 	Training Loss: 0.126000
Trained 1051 batches 	Training Loss: 0.075641
Trained 1101 batches 	Training Loss: 0.124942
Trained 1151 batches 	Training Loss: 0.072613
Trained 1201 batches 	Training Loss: 0.107850
Trained 1251 batches 	Training Loss: 0.076913
Trained 1301 batches 	Training Loss: 0.084233
Trained 1351 batches 	Training Loss: 0.055228
Trained 1401 batches 	Training Loss: 0.042930
Trained 1451 batches 	Training Loss: 0.070315
Trained 1501 batches 	Training Loss: 0.059122
Trained 1551 batches 	Training Loss: 0.095183
Trained 1601 batches 	Training Loss: 0.111480
Trained 1651 batches 	Training Loss: 0.084778
Trained 1701 batches 	Training Loss: 0.101397
Trained 1751 batches 	Training Loss: 0.106062
Trained 1801 batches 	Training Loss: 0.067540
Trained 1851 batches 	Training Loss: 0.089170
Trained 1901 batches 	Training Loss: 0.107402
Trained 1951 batches 	Training Loss: 0.068870
Trained 2001 batches 	Training Loss: 0.090849
Trained 2051 batches 	Training Loss: 0.103765
Trained 2101 batches 	Training Loss: 0.101377
Trained 2151 batches 	Training Loss: 0.048714
Trained 2201 batches 	Training Loss: 0.147711
Trained 2251 batches 	Training Loss: 0.124341
Trained 2301 batches 	Training Loss: 0.101890
Trained 2351 batches 	Training Loss: 0.081400
Trained 2401 batches 	Training Loss: 0.125437
Trained 2451 batches 	Training Loss: 0.133755
Trained 2501 batches 	Training Loss: 0.092333
Trained 2551 batches 	Training Loss: 0.069609
Trained 2601 batches 	Training Loss: 0.117319
Trained 2651 batches 	Training Loss: 0.069154
Trained 2701 batches 	Training Loss: 0.106255
Trained 2751 batches 	Training Loss: 0.035713
Trained 2801 batches 	Training Loss: 0.092676
Trained 2851 batches 	Training Loss: 0.094655
Trained 2901 batches 	Training Loss: 0.065275
Trained 2951 batches 	Training Loss: 0.084926
Trained 3001 batches 	Training Loss: 0.077379
Trained 3051 batches 	Training Loss: 0.073841
Trained 3101 batches 	Training Loss: 0.064106
Trained 3151 batches 	Training Loss: 0.106223
Trained 3201 batches 	Training Loss: 0.080777
Trained 3251 batches 	Training Loss: 0.062333
Trained 3301 batches 	Training Loss: 0.059136
Trained 3351 batches 	Training Loss: 0.088498
Trained 3401 batches 	Training Loss: 0.099192
Trained 3451 batches 	Training Loss: 0.105404
Trained 3501 batches 	Training Loss: 0.111100
Trained 3551 batches 	Training Loss: 0.110287
Trained 3601 batches 	Training Loss: 0.058908
Trained 3651 batches 	Training Loss: 0.083054
Trained 3701 batches 	Training Loss: 0.106845
Trained 3751 batches 	Training Loss: 0.092468
Trained 3801 batches 	Training Loss: 0.095853
Trained 3851 batches 	Training Loss: 0.151035
Trained 3901 batches 	Training Loss: 0.070935
Trained 3951 batches 	Training Loss: 0.113624
Trained 4001 batches 	Training Loss: 0.081694
Trained 4051 batches 	Training Loss: 0.066930
Trained 4101 batches 	Training Loss: 0.039908
Trained 4151 batches 	Training Loss: 0.066529
Trained 4201 batches 	Training Loss: 0.042613
Trained 4251 batches 	Training Loss: 0.110899
Trained 4301 batches 	Training Loss: 0.102779
Trained 4351 batches 	Training Loss: 0.114696
Trained 4401 batches 	Training Loss: 0.053741
Trained 4451 batches 	Training Loss: 0.074993
Trained 4501 batches 	Training Loss: 0.093181
Trained 4551 batches 	Training Loss: 0.111802
Trained 4601 batches 	Training Loss: 0.086640
Trained 4651 batches 	Training Loss: 0.052790
Trained 4701 batches 	Training Loss: 0.066646
Trained 4751 batches 	Training Loss: 0.072322
Trained 4801 batches 	Training Loss: 0.059362
Trained 4851 batches 	Training Loss: 0.061392
Trained 4901 batches 	Training Loss: 0.082406
Epoch: 7 	Training Loss: 0.087630
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1735133
The average AUROC is 0.807
The AUROC of Atelectasis is 0.7764631121433467
The AUROC of Cardiomegaly is 0.8717182626675716
The AUROC of Effusion is 0.8728097027938155
The AUROC of Infiltration is 0.6836275415640819
The AUROC of Mass is 0.8100317186117836
The AUROC of Nodule is 0.7528192813374169
The AUROC of Pneumonia is 0.7324489970855477
The AUROC of Pneumothorax is 0.8500359359024973
The AUROC of Consolidation is 0.751643762658107
The AUROC of Edema is 0.8875451069323975
The AUROC of Emphysema is 0.8764405127842749
The AUROC of Fibrosis is 0.789167870921731
The AUROC of Pleural_Thickening is 0.773141770800081
The AUROC of Hernia is 0.8646273266962922
Epoch: 7 	Learning Rate for first group: 0.0000100000
Started epoch 8
Trained 1 batches 	Training Loss: 0.072701
Trained 51 batches 	Training Loss: 0.069879
Trained 101 batches 	Training Loss: 0.078456
Trained 151 batches 	Training Loss: 0.109155
Trained 201 batches 	Training Loss: 0.063778
Trained 251 batches 	Training Loss: 0.073140
Trained 301 batches 	Training Loss: 0.057552
Trained 351 batches 	Training Loss: 0.105072
Trained 401 batches 	Training Loss: 0.096699
Trained 451 batches 	Training Loss: 0.035628
Trained 501 batches 	Training Loss: 0.041306
Trained 551 batches 	Training Loss: 0.108537
Trained 601 batches 	Training Loss: 0.062307
Trained 651 batches 	Training Loss: 0.079754
Trained 701 batches 	Training Loss: 0.089934
Trained 751 batches 	Training Loss: 0.060338
Trained 801 batches 	Training Loss: 0.035625
Trained 851 batches 	Training Loss: 0.104967
Trained 901 batches 	Training Loss: 0.102446
Trained 951 batches 	Training Loss: 0.106327
Trained 1001 batches 	Training Loss: 0.094550
Trained 1051 batches 	Training Loss: 0.058791
Trained 1101 batches 	Training Loss: 0.123333
Trained 1151 batches 	Training Loss: 0.088009
Trained 1201 batches 	Training Loss: 0.113688
Trained 1251 batches 	Training Loss: 0.070953
Trained 1301 batches 	Training Loss: 0.093666
Trained 1351 batches 	Training Loss: 0.039855
Trained 1401 batches 	Training Loss: 0.091434
Trained 1451 batches 	Training Loss: 0.061627
Trained 1501 batches 	Training Loss: 0.041254
Trained 1551 batches 	Training Loss: 0.106323
Trained 1601 batches 	Training Loss: 0.050286
Trained 1651 batches 	Training Loss: 0.071442
Trained 1701 batches 	Training Loss: 0.113929
Trained 1751 batches 	Training Loss: 0.050417
Trained 1801 batches 	Training Loss: 0.055485
Trained 1851 batches 	Training Loss: 0.048710
Trained 1901 batches 	Training Loss: 0.078687
Trained 1951 batches 	Training Loss: 0.053586
Trained 2001 batches 	Training Loss: 0.076046
Trained 2051 batches 	Training Loss: 0.122239
Trained 2101 batches 	Training Loss: 0.123751
Trained 2151 batches 	Training Loss: 0.061346
Trained 2201 batches 	Training Loss: 0.058040
Trained 2251 batches 	Training Loss: 0.067711
Trained 2301 batches 	Training Loss: 0.084299
Trained 2351 batches 	Training Loss: 0.054018
Trained 2401 batches 	Training Loss: 0.064883
Trained 2451 batches 	Training Loss: 0.096048
Trained 2501 batches 	Training Loss: 0.072392
Trained 2551 batches 	Training Loss: 0.077683
Trained 2601 batches 	Training Loss: 0.079096
Trained 2651 batches 	Training Loss: 0.058858
Trained 2701 batches 	Training Loss: 0.102926
Trained 2751 batches 	Training Loss: 0.078500
Trained 2801 batches 	Training Loss: 0.075510
Trained 2851 batches 	Training Loss: 0.108470
Trained 2901 batches 	Training Loss: 0.122740
Trained 2951 batches 	Training Loss: 0.071618
Trained 3001 batches 	Training Loss: 0.070191
Trained 3051 batches 	Training Loss: 0.083283
Trained 3101 batches 	Training Loss: 0.074367
Trained 3151 batches 	Training Loss: 0.066983
Trained 3201 batches 	Training Loss: 0.066184
Trained 3251 batches 	Training Loss: 0.051033
Trained 3301 batches 	Training Loss: 0.038915
Trained 3351 batches 	Training Loss: 0.070200
Trained 3401 batches 	Training Loss: 0.067583
Trained 3451 batches 	Training Loss: 0.114275
Trained 3501 batches 	Training Loss: 0.110906
Trained 3551 batches 	Training Loss: 0.104632
Trained 3601 batches 	Training Loss: 0.074213
Trained 3651 batches 	Training Loss: 0.117292
Trained 3701 batches 	Training Loss: 0.048821
Trained 3751 batches 	Training Loss: 0.117425
Trained 3801 batches 	Training Loss: 0.116464
Trained 3851 batches 	Training Loss: 0.103824
Trained 3901 batches 	Training Loss: 0.109018
Trained 3951 batches 	Training Loss: 0.055964
Trained 4001 batches 	Training Loss: 0.090793
Trained 4051 batches 	Training Loss: 0.096867
Trained 4101 batches 	Training Loss: 0.061737
Trained 4151 batches 	Training Loss: 0.128177
Trained 4201 batches 	Training Loss: 0.103599
Trained 4251 batches 	Training Loss: 0.077032
Trained 4301 batches 	Training Loss: 0.130721
Trained 4351 batches 	Training Loss: 0.094874
Trained 4401 batches 	Training Loss: 0.074668
Trained 4451 batches 	Training Loss: 0.062828
Trained 4501 batches 	Training Loss: 0.086958
Trained 4551 batches 	Training Loss: 0.057711
Trained 4601 batches 	Training Loss: 0.129387
Trained 4651 batches 	Training Loss: 0.068738
Trained 4701 batches 	Training Loss: 0.094909
Trained 4751 batches 	Training Loss: 0.046716
Trained 4801 batches 	Training Loss: 0.096870
Trained 4851 batches 	Training Loss: 0.059254
Trained 4901 batches 	Training Loss: 0.119731
Epoch: 8 	Training Loss: 0.078085
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1736181
The average AUROC is 0.803
The AUROC of Atelectasis is 0.7743979497715106
The AUROC of Cardiomegaly is 0.867900430797017
The AUROC of Effusion is 0.8689124213253325
The AUROC of Infiltration is 0.677011590915378
The AUROC of Mass is 0.8067722099804543
The AUROC of Nodule is 0.7529689581197514
The AUROC of Pneumonia is 0.7235859906280357
The AUROC of Pneumothorax is 0.8470471823215605
The AUROC of Consolidation is 0.7462493392308965
The AUROC of Edema is 0.8807232006886048
The AUROC of Emphysema is 0.8775781277603081
The AUROC of Fibrosis is 0.7873787813385597
The AUROC of Pleural_Thickening is 0.7688017424778191
The AUROC of Hernia is 0.8647306716272234
Epoch: 8 	Learning Rate for first group: 0.0000010000
Started epoch 9
Trained 1 batches 	Training Loss: 0.093271
Trained 51 batches 	Training Loss: 0.062167
Trained 101 batches 	Training Loss: 0.076552
Trained 151 batches 	Training Loss: 0.053754
Trained 201 batches 	Training Loss: 0.070225
Trained 251 batches 	Training Loss: 0.065208
Trained 301 batches 	Training Loss: 0.057326
Trained 351 batches 	Training Loss: 0.034933
Trained 401 batches 	Training Loss: 0.051967
Trained 451 batches 	Training Loss: 0.076813
Trained 501 batches 	Training Loss: 0.071426
Trained 551 batches 	Training Loss: 0.090820
Trained 601 batches 	Training Loss: 0.110319
Trained 651 batches 	Training Loss: 0.057782
Trained 701 batches 	Training Loss: 0.099814
Trained 751 batches 	Training Loss: 0.063960
Trained 801 batches 	Training Loss: 0.109783
Trained 851 batches 	Training Loss: 0.107746
Trained 901 batches 	Training Loss: 0.059466
Trained 951 batches 	Training Loss: 0.095655
Trained 1001 batches 	Training Loss: 0.099692
Trained 1051 batches 	Training Loss: 0.105520
Trained 1101 batches 	Training Loss: 0.068038
Trained 1151 batches 	Training Loss: 0.104554
Trained 1201 batches 	Training Loss: 0.056121
Trained 1251 batches 	Training Loss: 0.032737
Trained 1301 batches 	Training Loss: 0.073977
Trained 1351 batches 	Training Loss: 0.060138
Trained 1401 batches 	Training Loss: 0.059033
Trained 1451 batches 	Training Loss: 0.040553
Trained 1501 batches 	Training Loss: 0.054128
Trained 1551 batches 	Training Loss: 0.086349
Trained 1601 batches 	Training Loss: 0.093149
Trained 1651 batches 	Training Loss: 0.099381
Trained 1701 batches 	Training Loss: 0.120241
Trained 1751 batches 	Training Loss: 0.034645
Trained 1801 batches 	Training Loss: 0.035814
Trained 1851 batches 	Training Loss: 0.059980
Trained 1901 batches 	Training Loss: 0.093019
Trained 1951 batches 	Training Loss: 0.073114
Trained 2001 batches 	Training Loss: 0.060274
Trained 2051 batches 	Training Loss: 0.066951
Trained 2101 batches 	Training Loss: 0.051378
Trained 2151 batches 	Training Loss: 0.065846
Trained 2201 batches 	Training Loss: 0.072370
Trained 2251 batches 	Training Loss: 0.067400
Trained 2301 batches 	Training Loss: 0.095897
Trained 2351 batches 	Training Loss: 0.066564
Trained 2401 batches 	Training Loss: 0.038870
Trained 2451 batches 	Training Loss: 0.083743
Trained 2501 batches 	Training Loss: 0.050820
Trained 2551 batches 	Training Loss: 0.084275
Trained 2601 batches 	Training Loss: 0.084669
Trained 2651 batches 	Training Loss: 0.103780
Trained 2701 batches 	Training Loss: 0.048722
Trained 2751 batches 	Training Loss: 0.046839
Trained 2801 batches 	Training Loss: 0.048202
Trained 2851 batches 	Training Loss: 0.062980
Trained 2901 batches 	Training Loss: 0.059902
Trained 2951 batches 	Training Loss: 0.102108
Trained 3001 batches 	Training Loss: 0.076646
Trained 3051 batches 	Training Loss: 0.058180
Trained 3101 batches 	Training Loss: 0.050218
Trained 3151 batches 	Training Loss: 0.044850
Trained 3201 batches 	Training Loss: 0.087530
Trained 3251 batches 	Training Loss: 0.067008
Trained 3301 batches 	Training Loss: 0.061828
Trained 3351 batches 	Training Loss: 0.088847
Trained 3401 batches 	Training Loss: 0.064155
Trained 3451 batches 	Training Loss: 0.086263
Trained 3501 batches 	Training Loss: 0.054519
Trained 3551 batches 	Training Loss: 0.095275
Trained 3601 batches 	Training Loss: 0.048088
Trained 3651 batches 	Training Loss: 0.100743
Trained 3701 batches 	Training Loss: 0.110569
Trained 3751 batches 	Training Loss: 0.068357
Trained 3801 batches 	Training Loss: 0.088564
Trained 3851 batches 	Training Loss: 0.090427
Trained 3901 batches 	Training Loss: 0.051945
Trained 3951 batches 	Training Loss: 0.069848
Trained 4001 batches 	Training Loss: 0.105519
Trained 4051 batches 	Training Loss: 0.068113
Trained 4101 batches 	Training Loss: 0.114793
Trained 4151 batches 	Training Loss: 0.104624
Trained 4201 batches 	Training Loss: 0.057860
Trained 4251 batches 	Training Loss: 0.088387
Trained 4301 batches 	Training Loss: 0.077254
Trained 4351 batches 	Training Loss: 0.058841
Trained 4401 batches 	Training Loss: 0.089180
Trained 4451 batches 	Training Loss: 0.040962
Trained 4501 batches 	Training Loss: 0.108980
Trained 4551 batches 	Training Loss: 0.067052
Trained 4601 batches 	Training Loss: 0.106722
Trained 4651 batches 	Training Loss: 0.086517
Trained 4701 batches 	Training Loss: 0.055510
Trained 4751 batches 	Training Loss: 0.059106
Trained 4801 batches 	Training Loss: 0.057727
Trained 4851 batches 	Training Loss: 0.075256
Trained 4901 batches 	Training Loss: 0.063762
Epoch: 9 	Training Loss: 0.076744
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1762251
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7733160210916894
The AUROC of Cardiomegaly is 0.8687238557496937
The AUROC of Effusion is 0.8680141416042527
The AUROC of Infiltration is 0.6765429195828377
The AUROC of Mass is 0.8041240816467061
The AUROC of Nodule is 0.7506607395231603
The AUROC of Pneumonia is 0.723709211954969
The AUROC of Pneumothorax is 0.8428315150272809
The AUROC of Consolidation is 0.7454339262740901
The AUROC of Edema is 0.8758487552141959
The AUROC of Emphysema is 0.873296705885124
The AUROC of Fibrosis is 0.7852288173991844
The AUROC of Pleural_Thickening is 0.7683548790512984
The AUROC of Hernia is 0.863733967182243
Epoch: 9 	Learning Rate for first group: 0.0000010000
Started epoch 10
Trained 1 batches 	Training Loss: 0.040642
Trained 51 batches 	Training Loss: 0.110432
Trained 101 batches 	Training Loss: 0.072888
Trained 151 batches 	Training Loss: 0.124669
Trained 201 batches 	Training Loss: 0.076623
Trained 251 batches 	Training Loss: 0.087159
Trained 301 batches 	Training Loss: 0.063775
Trained 351 batches 	Training Loss: 0.078002
Trained 401 batches 	Training Loss: 0.067734
Trained 451 batches 	Training Loss: 0.080380
Trained 501 batches 	Training Loss: 0.067640
Trained 551 batches 	Training Loss: 0.071332
Trained 601 batches 	Training Loss: 0.062312
Trained 651 batches 	Training Loss: 0.107945
Trained 701 batches 	Training Loss: 0.082464
Trained 751 batches 	Training Loss: 0.060665
Trained 801 batches 	Training Loss: 0.057381
Trained 851 batches 	Training Loss: 0.091762
Trained 901 batches 	Training Loss: 0.055159
Trained 951 batches 	Training Loss: 0.071000
Trained 1001 batches 	Training Loss: 0.061434
Trained 1051 batches 	Training Loss: 0.086413
Trained 1101 batches 	Training Loss: 0.066461
Trained 1151 batches 	Training Loss: 0.044680
Trained 1201 batches 	Training Loss: 0.097322
Trained 1251 batches 	Training Loss: 0.056696
Trained 1301 batches 	Training Loss: 0.029602
Trained 1351 batches 	Training Loss: 0.069871
Trained 1401 batches 	Training Loss: 0.123883
Trained 1451 batches 	Training Loss: 0.050776
Trained 1501 batches 	Training Loss: 0.085269
Trained 1551 batches 	Training Loss: 0.140958
Trained 1601 batches 	Training Loss: 0.045910
Trained 1651 batches 	Training Loss: 0.074933
Trained 1701 batches 	Training Loss: 0.063356
Trained 1751 batches 	Training Loss: 0.103798
Trained 1801 batches 	Training Loss: 0.048379
Trained 1851 batches 	Training Loss: 0.054349
Trained 1901 batches 	Training Loss: 0.063063
Trained 1951 batches 	Training Loss: 0.120463
Trained 2001 batches 	Training Loss: 0.084214
Trained 2051 batches 	Training Loss: 0.103211
Trained 2101 batches 	Training Loss: 0.057007
Trained 2151 batches 	Training Loss: 0.082571
Trained 2201 batches 	Training Loss: 0.125915
Trained 2251 batches 	Training Loss: 0.094104
Trained 2301 batches 	Training Loss: 0.110193
Trained 2351 batches 	Training Loss: 0.121202
Trained 2401 batches 	Training Loss: 0.046663
Trained 2451 batches 	Training Loss: 0.062501
Trained 2501 batches 	Training Loss: 0.061849
Trained 2551 batches 	Training Loss: 0.085012
Trained 2601 batches 	Training Loss: 0.062329
Trained 2651 batches 	Training Loss: 0.049435
Trained 2701 batches 	Training Loss: 0.074291
Trained 2751 batches 	Training Loss: 0.134359
Trained 2801 batches 	Training Loss: 0.083558
Trained 2851 batches 	Training Loss: 0.064905
Trained 2901 batches 	Training Loss: 0.067188
Trained 2951 batches 	Training Loss: 0.074829
Trained 3001 batches 	Training Loss: 0.102863
Trained 3051 batches 	Training Loss: 0.035143
Trained 3101 batches 	Training Loss: 0.094551
Trained 3151 batches 	Training Loss: 0.077897
Trained 3201 batches 	Training Loss: 0.055221
Trained 3251 batches 	Training Loss: 0.032603
Trained 3301 batches 	Training Loss: 0.072698
Trained 3351 batches 	Training Loss: 0.128788
Trained 3401 batches 	Training Loss: 0.070869
Trained 3451 batches 	Training Loss: 0.117652
Trained 3501 batches 	Training Loss: 0.051231
Trained 3551 batches 	Training Loss: 0.051393
Trained 3601 batches 	Training Loss: 0.069299
Trained 3651 batches 	Training Loss: 0.056979
Trained 3701 batches 	Training Loss: 0.109989
Trained 3751 batches 	Training Loss: 0.075887
Trained 3801 batches 	Training Loss: 0.087933
Trained 3851 batches 	Training Loss: 0.069889
Trained 3901 batches 	Training Loss: 0.074027
Trained 3951 batches 	Training Loss: 0.061874
Trained 4001 batches 	Training Loss: 0.068427
Trained 4051 batches 	Training Loss: 0.060555
Trained 4101 batches 	Training Loss: 0.097295
Trained 4151 batches 	Training Loss: 0.081200
Trained 4201 batches 	Training Loss: 0.061270
Trained 4251 batches 	Training Loss: 0.086549
Trained 4301 batches 	Training Loss: 0.046405
Trained 4351 batches 	Training Loss: 0.115757
Trained 4401 batches 	Training Loss: 0.105052
Trained 4451 batches 	Training Loss: 0.064576
Trained 4501 batches 	Training Loss: 0.064180
Trained 4551 batches 	Training Loss: 0.045488
Trained 4601 batches 	Training Loss: 0.124705
Trained 4651 batches 	Training Loss: 0.066598
Trained 4701 batches 	Training Loss: 0.056785
Trained 4751 batches 	Training Loss: 0.070601
Trained 4801 batches 	Training Loss: 0.070137
Trained 4851 batches 	Training Loss: 0.061106
Trained 4901 batches 	Training Loss: 0.093371
Epoch: 10 	Training Loss: 0.075526
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1759710
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7726127544863657
The AUROC of Cardiomegaly is 0.8666772096591054
The AUROC of Effusion is 0.8670847887339537
The AUROC of Infiltration is 0.6758616282608094
The AUROC of Mass is 0.802509018004227
The AUROC of Nodule is 0.7508644226564133
The AUROC of Pneumonia is 0.7215388498390384
The AUROC of Pneumothorax is 0.8428837106603095
The AUROC of Consolidation is 0.7453648639611755
The AUROC of Edema is 0.8752104300470104
The AUROC of Emphysema is 0.8714958808842408
The AUROC of Fibrosis is 0.7858990892346299
The AUROC of Pleural_Thickening is 0.7656762320051211
The AUROC of Hernia is 0.8659845901225213
Epoch: 10 	Learning Rate for first group: 0.0000001000
Started epoch 11
Trained 1 batches 	Training Loss: 0.068781
Trained 51 batches 	Training Loss: 0.099056
Trained 101 batches 	Training Loss: 0.103680
Trained 151 batches 	Training Loss: 0.050605
Trained 201 batches 	Training Loss: 0.030567
Trained 251 batches 	Training Loss: 0.085193
Trained 301 batches 	Training Loss: 0.064599
Trained 351 batches 	Training Loss: 0.034878
Trained 401 batches 	Training Loss: 0.059521
Trained 451 batches 	Training Loss: 0.083394
Trained 501 batches 	Training Loss: 0.076730
Trained 551 batches 	Training Loss: 0.041535
Trained 601 batches 	Training Loss: 0.045505
Trained 651 batches 	Training Loss: 0.048290
Trained 701 batches 	Training Loss: 0.084061
Trained 751 batches 	Training Loss: 0.152089
Trained 801 batches 	Training Loss: 0.055022
Trained 851 batches 	Training Loss: 0.066744
Trained 901 batches 	Training Loss: 0.091526
Trained 951 batches 	Training Loss: 0.135634
Trained 1001 batches 	Training Loss: 0.064271
Trained 1051 batches 	Training Loss: 0.061371
Trained 1101 batches 	Training Loss: 0.053884
Trained 1151 batches 	Training Loss: 0.092866
Trained 1201 batches 	Training Loss: 0.071056
Trained 1251 batches 	Training Loss: 0.070795
Trained 1301 batches 	Training Loss: 0.047928
Trained 1351 batches 	Training Loss: 0.038463
Trained 1401 batches 	Training Loss: 0.067947
Trained 1451 batches 	Training Loss: 0.088622
Trained 1501 batches 	Training Loss: 0.100860
Trained 1551 batches 	Training Loss: 0.107894
Trained 1601 batches 	Training Loss: 0.083455
Trained 1651 batches 	Training Loss: 0.094547
Trained 1701 batches 	Training Loss: 0.073560
Trained 1751 batches 	Training Loss: 0.096581
Trained 1801 batches 	Training Loss: 0.055729
Trained 1851 batches 	Training Loss: 0.111216
Trained 1901 batches 	Training Loss: 0.054966
Trained 1951 batches 	Training Loss: 0.074635
Trained 2001 batches 	Training Loss: 0.060583
Trained 2051 batches 	Training Loss: 0.064088
Trained 2101 batches 	Training Loss: 0.040943
Trained 2151 batches 	Training Loss: 0.064842
Trained 2201 batches 	Training Loss: 0.115779
Trained 2251 batches 	Training Loss: 0.039739
Trained 2301 batches 	Training Loss: 0.097254
Trained 2351 batches 	Training Loss: 0.134832
Trained 2401 batches 	Training Loss: 0.105601
Trained 2451 batches 	Training Loss: 0.067989
Trained 2501 batches 	Training Loss: 0.073262
Trained 2551 batches 	Training Loss: 0.060222
Trained 2601 batches 	Training Loss: 0.078235
Trained 2651 batches 	Training Loss: 0.122888
Trained 2701 batches 	Training Loss: 0.078196
Trained 2751 batches 	Training Loss: 0.080776
Trained 2801 batches 	Training Loss: 0.059546
Trained 2851 batches 	Training Loss: 0.059940
Trained 2901 batches 	Training Loss: 0.078741
Trained 2951 batches 	Training Loss: 0.044539
Trained 3001 batches 	Training Loss: 0.099523
Trained 3051 batches 	Training Loss: 0.072835
Trained 3101 batches 	Training Loss: 0.122831
Trained 3151 batches 	Training Loss: 0.061221
Trained 3201 batches 	Training Loss: 0.043224
Trained 3251 batches 	Training Loss: 0.080479
Trained 3301 batches 	Training Loss: 0.093532
Trained 3351 batches 	Training Loss: 0.104514
Trained 3401 batches 	Training Loss: 0.062152
Trained 3451 batches 	Training Loss: 0.079167
Trained 3501 batches 	Training Loss: 0.070715
Trained 3551 batches 	Training Loss: 0.080478
Trained 3601 batches 	Training Loss: 0.106461
Trained 3651 batches 	Training Loss: 0.059979
Trained 3701 batches 	Training Loss: 0.062511
Trained 3751 batches 	Training Loss: 0.094500
Trained 3801 batches 	Training Loss: 0.134076
Trained 3851 batches 	Training Loss: 0.092945
Trained 3901 batches 	Training Loss: 0.056193
Trained 3951 batches 	Training Loss: 0.064973
Trained 4001 batches 	Training Loss: 0.125525
Trained 4051 batches 	Training Loss: 0.078682
Trained 4101 batches 	Training Loss: 0.054776
Trained 4151 batches 	Training Loss: 0.052911
Trained 4201 batches 	Training Loss: 0.055951
Trained 4251 batches 	Training Loss: 0.054629
Trained 4301 batches 	Training Loss: 0.077975
Trained 4351 batches 	Training Loss: 0.097202
Trained 4401 batches 	Training Loss: 0.053724
Trained 4451 batches 	Training Loss: 0.086036
Trained 4501 batches 	Training Loss: 0.079273
Trained 4551 batches 	Training Loss: 0.134530
Trained 4601 batches 	Training Loss: 0.047464
Trained 4651 batches 	Training Loss: 0.050357
Trained 4701 batches 	Training Loss: 0.066271
Trained 4751 batches 	Training Loss: 0.048798
Trained 4801 batches 	Training Loss: 0.102188
Trained 4851 batches 	Training Loss: 0.099728
Trained 4901 batches 	Training Loss: 0.049062
Epoch: 11 	Training Loss: 0.075613
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1779758
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7712915639206038
The AUROC of Cardiomegaly is 0.864901405161657
The AUROC of Effusion is 0.8656011750226497
The AUROC of Infiltration is 0.6773537857202135
The AUROC of Mass is 0.804997907717081
The AUROC of Nodule is 0.7507815225510405
The AUROC of Pneumonia is 0.7217335038192658
The AUROC of Pneumothorax is 0.84291898225475
The AUROC of Consolidation is 0.7436480584533863
The AUROC of Edema is 0.8752677448189101
The AUROC of Emphysema is 0.8717947302406105
The AUROC of Fibrosis is 0.7861012432202004
The AUROC of Pleural_Thickening is 0.7668871305504744
The AUROC of Hernia is 0.8669721083514187
Epoch: 11 	Learning Rate for first group: 0.0000001000
Started epoch 12
Trained 1 batches 	Training Loss: 0.067855
Trained 51 batches 	Training Loss: 0.062265
Trained 101 batches 	Training Loss: 0.088623
Trained 151 batches 	Training Loss: 0.086607
Trained 201 batches 	Training Loss: 0.068715
Trained 251 batches 	Training Loss: 0.057308
Trained 301 batches 	Training Loss: 0.052466
Trained 351 batches 	Training Loss: 0.073363
Trained 401 batches 	Training Loss: 0.108138
Trained 451 batches 	Training Loss: 0.106102
Trained 501 batches 	Training Loss: 0.093948
Trained 551 batches 	Training Loss: 0.078312
Trained 601 batches 	Training Loss: 0.079646
Trained 651 batches 	Training Loss: 0.089537
Trained 701 batches 	Training Loss: 0.094906
Trained 751 batches 	Training Loss: 0.074378
Trained 801 batches 	Training Loss: 0.094890
Trained 851 batches 	Training Loss: 0.068089
Trained 901 batches 	Training Loss: 0.068973
Trained 951 batches 	Training Loss: 0.107790
Trained 1001 batches 	Training Loss: 0.066781
Trained 1051 batches 	Training Loss: 0.123447
Trained 1101 batches 	Training Loss: 0.086261
Trained 1151 batches 	Training Loss: 0.083951
Trained 1201 batches 	Training Loss: 0.070494
Trained 1251 batches 	Training Loss: 0.111806
Trained 1301 batches 	Training Loss: 0.059660
Trained 1351 batches 	Training Loss: 0.076983
Trained 1401 batches 	Training Loss: 0.076779
Trained 1451 batches 	Training Loss: 0.053842
Trained 1501 batches 	Training Loss: 0.052163
Trained 1551 batches 	Training Loss: 0.072880
Trained 1601 batches 	Training Loss: 0.070586
Trained 1651 batches 	Training Loss: 0.142868
Trained 1701 batches 	Training Loss: 0.055760
Trained 1751 batches 	Training Loss: 0.096408
Trained 1801 batches 	Training Loss: 0.059704
Trained 1851 batches 	Training Loss: 0.112529
Trained 1901 batches 	Training Loss: 0.059905
Trained 1951 batches 	Training Loss: 0.057695
Trained 2001 batches 	Training Loss: 0.082494
Trained 2051 batches 	Training Loss: 0.063173
Trained 2101 batches 	Training Loss: 0.124156
Trained 2151 batches 	Training Loss: 0.063952
Trained 2201 batches 	Training Loss: 0.064683
Trained 2251 batches 	Training Loss: 0.071018
Trained 2301 batches 	Training Loss: 0.103375
Trained 2351 batches 	Training Loss: 0.047733
Trained 2401 batches 	Training Loss: 0.036958
Trained 2451 batches 	Training Loss: 0.058295
Trained 2501 batches 	Training Loss: 0.083880
Trained 2551 batches 	Training Loss: 0.051596
Trained 2601 batches 	Training Loss: 0.074446
Trained 2651 batches 	Training Loss: 0.074006
Trained 2701 batches 	Training Loss: 0.053717
Trained 2751 batches 	Training Loss: 0.072497
Trained 2801 batches 	Training Loss: 0.064530
Trained 2851 batches 	Training Loss: 0.070484
Trained 2901 batches 	Training Loss: 0.058124
Trained 2951 batches 	Training Loss: 0.111169
Trained 3001 batches 	Training Loss: 0.083745
Trained 3051 batches 	Training Loss: 0.052124
Trained 3101 batches 	Training Loss: 0.073517
Trained 3151 batches 	Training Loss: 0.080811
Trained 3201 batches 	Training Loss: 0.109958
Trained 3251 batches 	Training Loss: 0.092100
Trained 3301 batches 	Training Loss: 0.038054
Trained 3351 batches 	Training Loss: 0.078602
Trained 3401 batches 	Training Loss: 0.105815
Trained 3451 batches 	Training Loss: 0.042557
Trained 3501 batches 	Training Loss: 0.073674
Trained 3551 batches 	Training Loss: 0.067579
Trained 3601 batches 	Training Loss: 0.057628
Trained 3651 batches 	Training Loss: 0.064274
Trained 3701 batches 	Training Loss: 0.053534
Trained 3751 batches 	Training Loss: 0.055222
Trained 3801 batches 	Training Loss: 0.084943
Trained 3851 batches 	Training Loss: 0.080822
Trained 3901 batches 	Training Loss: 0.070003
Trained 3951 batches 	Training Loss: 0.099849
Trained 4001 batches 	Training Loss: 0.078415
Trained 4051 batches 	Training Loss: 0.080398
Trained 4101 batches 	Training Loss: 0.068448
Trained 4151 batches 	Training Loss: 0.082385
Trained 4201 batches 	Training Loss: 0.082933
Trained 4251 batches 	Training Loss: 0.060439
Trained 4301 batches 	Training Loss: 0.073596
Trained 4351 batches 	Training Loss: 0.068582
Trained 4401 batches 	Training Loss: 0.057168
Trained 4451 batches 	Training Loss: 0.071083
Trained 4501 batches 	Training Loss: 0.109049
Trained 4551 batches 	Training Loss: 0.044797
Trained 4601 batches 	Training Loss: 0.071676
Trained 4651 batches 	Training Loss: 0.095623
Trained 4701 batches 	Training Loss: 0.064794
Trained 4751 batches 	Training Loss: 0.054980
Trained 4801 batches 	Training Loss: 0.043457
Trained 4851 batches 	Training Loss: 0.081038
Trained 4901 batches 	Training Loss: 0.069746
Epoch: 12 	Training Loss: 0.075575
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1762074
The average AUROC is 0.799
The AUROC of Atelectasis is 0.7688815740519641
The AUROC of Cardiomegaly is 0.8649110120940193
The AUROC of Effusion is 0.8640030482685576
The AUROC of Infiltration is 0.6760853412265899
The AUROC of Mass is 0.8012721874685497
The AUROC of Nodule is 0.7492712709756173
The AUROC of Pneumonia is 0.7165504695506411
The AUROC of Pneumothorax is 0.8415898916355391
The AUROC of Consolidation is 0.7393448991809648
The AUROC of Edema is 0.872152883533073
The AUROC of Emphysema is 0.872342375366569
The AUROC of Fibrosis is 0.7847510476348789
The AUROC of Pleural_Thickening is 0.7641287381681929
The AUROC of Hernia is 0.8707522362694776
Epoch: 12 	Learning Rate for first group: 0.0000000100
Started epoch 13
Trained 1 batches 	Training Loss: 0.045405
Trained 51 batches 	Training Loss: 0.070358
Trained 101 batches 	Training Loss: 0.045298
Trained 151 batches 	Training Loss: 0.036497
Trained 201 batches 	Training Loss: 0.079423
Trained 251 batches 	Training Loss: 0.050239
Trained 301 batches 	Training Loss: 0.066359
Trained 351 batches 	Training Loss: 0.081194
Trained 401 batches 	Training Loss: 0.074459
Trained 451 batches 	Training Loss: 0.111524
Trained 501 batches 	Training Loss: 0.106141
Trained 551 batches 	Training Loss: 0.063147
Trained 601 batches 	Training Loss: 0.067537
Trained 651 batches 	Training Loss: 0.066319
Trained 701 batches 	Training Loss: 0.099175
Trained 751 batches 	Training Loss: 0.093187
Trained 801 batches 	Training Loss: 0.082945
Trained 851 batches 	Training Loss: 0.078407
Trained 901 batches 	Training Loss: 0.049134
Trained 951 batches 	Training Loss: 0.086737
Trained 1001 batches 	Training Loss: 0.056066
Trained 1051 batches 	Training Loss: 0.112454
Trained 1101 batches 	Training Loss: 0.092364
Trained 1151 batches 	Training Loss: 0.065354
Trained 1201 batches 	Training Loss: 0.083239
Trained 1251 batches 	Training Loss: 0.094550
Trained 1301 batches 	Training Loss: 0.054781
Trained 1351 batches 	Training Loss: 0.067463
Trained 1401 batches 	Training Loss: 0.094486
Trained 1451 batches 	Training Loss: 0.068146
Trained 1501 batches 	Training Loss: 0.106689
Trained 1551 batches 	Training Loss: 0.057135
Trained 1601 batches 	Training Loss: 0.071082
Trained 1651 batches 	Training Loss: 0.094174
Trained 1701 batches 	Training Loss: 0.122555
Trained 1751 batches 	Training Loss: 0.060669
Trained 1801 batches 	Training Loss: 0.081470
Trained 1851 batches 	Training Loss: 0.056831
Trained 1901 batches 	Training Loss: 0.047687
Trained 1951 batches 	Training Loss: 0.101214
Trained 2001 batches 	Training Loss: 0.066865
Trained 2051 batches 	Training Loss: 0.100883
Trained 2101 batches 	Training Loss: 0.067229
Trained 2151 batches 	Training Loss: 0.097821
Trained 2201 batches 	Training Loss: 0.094149
Trained 2251 batches 	Training Loss: 0.072428
Trained 2301 batches 	Training Loss: 0.074591
Trained 2351 batches 	Training Loss: 0.064470
Trained 2401 batches 	Training Loss: 0.082376
Trained 2451 batches 	Training Loss: 0.059340
Trained 2501 batches 	Training Loss: 0.073986
Trained 2551 batches 	Training Loss: 0.065065
Trained 2601 batches 	Training Loss: 0.066564
Trained 2651 batches 	Training Loss: 0.092222
Trained 2701 batches 	Training Loss: 0.050857
Trained 2751 batches 	Training Loss: 0.045420
Trained 2801 batches 	Training Loss: 0.059257
Trained 2851 batches 	Training Loss: 0.095948
Trained 2901 batches 	Training Loss: 0.059995
Trained 2951 batches 	Training Loss: 0.058549
Trained 3001 batches 	Training Loss: 0.049870
Trained 3051 batches 	Training Loss: 0.102999
Trained 3101 batches 	Training Loss: 0.085018
Trained 3151 batches 	Training Loss: 0.053975
Trained 3201 batches 	Training Loss: 0.066347
Trained 3251 batches 	Training Loss: 0.064305
Trained 3301 batches 	Training Loss: 0.077183
Trained 3351 batches 	Training Loss: 0.062201
Trained 3401 batches 	Training Loss: 0.082669
Trained 3451 batches 	Training Loss: 0.150848
Trained 3501 batches 	Training Loss: 0.114110
Trained 3551 batches 	Training Loss: 0.079868
Trained 3601 batches 	Training Loss: 0.057272
Trained 3651 batches 	Training Loss: 0.076763
Trained 3701 batches 	Training Loss: 0.060778
Trained 3751 batches 	Training Loss: 0.101480
Trained 3801 batches 	Training Loss: 0.059466
Trained 3851 batches 	Training Loss: 0.052641
Trained 3901 batches 	Training Loss: 0.055304
Trained 3951 batches 	Training Loss: 0.049347
Trained 4001 batches 	Training Loss: 0.028281
Trained 4051 batches 	Training Loss: 0.046533
Trained 4101 batches 	Training Loss: 0.067229
Trained 4151 batches 	Training Loss: 0.085378
Trained 4201 batches 	Training Loss: 0.075014
Trained 4251 batches 	Training Loss: 0.093684
Trained 4301 batches 	Training Loss: 0.079224
Trained 4351 batches 	Training Loss: 0.091557
Trained 4401 batches 	Training Loss: 0.105914
Trained 4451 batches 	Training Loss: 0.063011
Trained 4501 batches 	Training Loss: 0.030866
Trained 4551 batches 	Training Loss: 0.049396
Trained 4601 batches 	Training Loss: 0.108252
Trained 4651 batches 	Training Loss: 0.063808
Trained 4701 batches 	Training Loss: 0.065219
Trained 4751 batches 	Training Loss: 0.078877
Trained 4801 batches 	Training Loss: 0.065340
Trained 4851 batches 	Training Loss: 0.114842
Trained 4901 batches 	Training Loss: 0.110947
Epoch: 13 	Training Loss: 0.075257
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1750456
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7700660435400456
The AUROC of Cardiomegaly is 0.8658255181554395
The AUROC of Effusion is 0.8662061050250303
The AUROC of Infiltration is 0.6766255221444576
The AUROC of Mass is 0.8007486201526574
The AUROC of Nodule is 0.7489428096286831
The AUROC of Pneumonia is 0.7188065508505248
The AUROC of Pneumothorax is 0.8417731299411865
The AUROC of Consolidation is 0.7436866696600304
The AUROC of Edema is 0.8735797523670793
The AUROC of Emphysema is 0.8729592122154306
The AUROC of Fibrosis is 0.7861366335731119
The AUROC of Pleural_Thickening is 0.7646448509486077
The AUROC of Hernia is 0.8640302226509123
Epoch: 13 	Learning Rate for first group: 0.0000000100
Started epoch 14
Trained 1 batches 	Training Loss: 0.069240
Trained 51 batches 	Training Loss: 0.048043
Trained 101 batches 	Training Loss: 0.080616
Trained 151 batches 	Training Loss: 0.077024
Trained 201 batches 	Training Loss: 0.062464
Trained 251 batches 	Training Loss: 0.050063
Trained 301 batches 	Training Loss: 0.064086
Trained 351 batches 	Training Loss: 0.078924
Trained 401 batches 	Training Loss: 0.063125
Trained 451 batches 	Training Loss: 0.058992
Trained 501 batches 	Training Loss: 0.113620
Trained 551 batches 	Training Loss: 0.088964
Trained 601 batches 	Training Loss: 0.052295
Trained 651 batches 	Training Loss: 0.046767
Trained 701 batches 	Training Loss: 0.061076
Trained 751 batches 	Training Loss: 0.053358
Trained 801 batches 	Training Loss: 0.114184
Trained 851 batches 	Training Loss: 0.064560
Trained 901 batches 	Training Loss: 0.063024
Trained 951 batches 	Training Loss: 0.062823
Trained 1001 batches 	Training Loss: 0.064613
Trained 1051 batches 	Training Loss: 0.083029
Trained 1101 batches 	Training Loss: 0.081301
Trained 1151 batches 	Training Loss: 0.076370
Trained 1201 batches 	Training Loss: 0.105769
Trained 1251 batches 	Training Loss: 0.077399
Trained 1301 batches 	Training Loss: 0.060287
Trained 1351 batches 	Training Loss: 0.052196
Trained 1401 batches 	Training Loss: 0.106491
Trained 1451 batches 	Training Loss: 0.064288
Trained 1501 batches 	Training Loss: 0.053722
Trained 1551 batches 	Training Loss: 0.068786
Trained 1601 batches 	Training Loss: 0.113749
Trained 1651 batches 	Training Loss: 0.039365
Trained 1701 batches 	Training Loss: 0.079290
Trained 1751 batches 	Training Loss: 0.087991
Trained 1801 batches 	Training Loss: 0.081267
Trained 1851 batches 	Training Loss: 0.085945
Trained 1901 batches 	Training Loss: 0.091864
Trained 1951 batches 	Training Loss: 0.048608
Trained 2001 batches 	Training Loss: 0.060333
Trained 2051 batches 	Training Loss: 0.048813
Trained 2101 batches 	Training Loss: 0.100080
Trained 2151 batches 	Training Loss: 0.046952
Trained 2201 batches 	Training Loss: 0.118898
Trained 2251 batches 	Training Loss: 0.072694
Trained 2301 batches 	Training Loss: 0.040947
Trained 2351 batches 	Training Loss: 0.050488
Trained 2401 batches 	Training Loss: 0.071854
Trained 2451 batches 	Training Loss: 0.070367
Trained 2501 batches 	Training Loss: 0.125806
Trained 2551 batches 	Training Loss: 0.108741
Trained 2601 batches 	Training Loss: 0.080926
Trained 2651 batches 	Training Loss: 0.081165
Trained 2701 batches 	Training Loss: 0.075801
Trained 2751 batches 	Training Loss: 0.073013
Trained 2801 batches 	Training Loss: 0.065004
Trained 2851 batches 	Training Loss: 0.089250
Trained 2901 batches 	Training Loss: 0.109919
Trained 2951 batches 	Training Loss: 0.044006
Trained 3001 batches 	Training Loss: 0.086506
Trained 3051 batches 	Training Loss: 0.100300
Trained 3101 batches 	Training Loss: 0.074031
Trained 3151 batches 	Training Loss: 0.081661
Trained 3201 batches 	Training Loss: 0.063755
Trained 3251 batches 	Training Loss: 0.096582
Trained 3301 batches 	Training Loss: 0.076802
Trained 3351 batches 	Training Loss: 0.082026
Trained 3401 batches 	Training Loss: 0.148471
Trained 3451 batches 	Training Loss: 0.050334
Trained 3501 batches 	Training Loss: 0.057420
Trained 3551 batches 	Training Loss: 0.056745
Trained 3601 batches 	Training Loss: 0.043342
Trained 3651 batches 	Training Loss: 0.117038
Trained 3701 batches 	Training Loss: 0.166574
Trained 3751 batches 	Training Loss: 0.099310
Trained 3801 batches 	Training Loss: 0.092054
Trained 3851 batches 	Training Loss: 0.098964
Trained 3901 batches 	Training Loss: 0.067242
Trained 3951 batches 	Training Loss: 0.121323
Trained 4001 batches 	Training Loss: 0.069128
Trained 4051 batches 	Training Loss: 0.061242
Trained 4101 batches 	Training Loss: 0.054589
Trained 4151 batches 	Training Loss: 0.095124
Trained 4201 batches 	Training Loss: 0.056015
Trained 4251 batches 	Training Loss: 0.061449
Trained 4301 batches 	Training Loss: 0.064987
Trained 4351 batches 	Training Loss: 0.059615
Trained 4401 batches 	Training Loss: 0.108197
Trained 4451 batches 	Training Loss: 0.047091
Trained 4501 batches 	Training Loss: 0.064564
Trained 4551 batches 	Training Loss: 0.120245
Trained 4601 batches 	Training Loss: 0.052021
Trained 4651 batches 	Training Loss: 0.071033
Trained 4701 batches 	Training Loss: 0.106814
Trained 4751 batches 	Training Loss: 0.061958
Trained 4801 batches 	Training Loss: 0.126957
Trained 4851 batches 	Training Loss: 0.051676
Trained 4901 batches 	Training Loss: 0.075615
Epoch: 14 	Training Loss: 0.075386
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1759199
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7704261246842645
The AUROC of Cardiomegaly is 0.8658347555904032
The AUROC of Effusion is 0.8665528547994545
The AUROC of Infiltration is 0.6753334255442751
The AUROC of Mass is 0.8030072594271912
The AUROC of Nodule is 0.7504401481928562
The AUROC of Pneumonia is 0.7209060755852715
The AUROC of Pneumothorax is 0.8447237648931538
The AUROC of Consolidation is 0.7434110174888865
The AUROC of Edema is 0.8757337118453287
The AUROC of Emphysema is 0.8737015510723245
The AUROC of Fibrosis is 0.7869742052586847
The AUROC of Pleural_Thickening is 0.7660520364256427
The AUROC of Hernia is 0.8644895334550508
Epoch: 14 	Learning Rate for first group: 0.0000000100
Started epoch 15
Trained 1 batches 	Training Loss: 0.093452
Trained 51 batches 	Training Loss: 0.080875
Trained 101 batches 	Training Loss: 0.087639
Trained 151 batches 	Training Loss: 0.050645
Trained 201 batches 	Training Loss: 0.057661
Trained 251 batches 	Training Loss: 0.073770
Trained 301 batches 	Training Loss: 0.077284
Trained 351 batches 	Training Loss: 0.036297
Trained 401 batches 	Training Loss: 0.064006
Trained 451 batches 	Training Loss: 0.077377
Trained 501 batches 	Training Loss: 0.069545
Trained 551 batches 	Training Loss: 0.091560
Trained 601 batches 	Training Loss: 0.033793
Trained 651 batches 	Training Loss: 0.075718
Trained 701 batches 	Training Loss: 0.095862
Trained 751 batches 	Training Loss: 0.070479
Trained 801 batches 	Training Loss: 0.058899
Trained 851 batches 	Training Loss: 0.056688
Trained 901 batches 	Training Loss: 0.097951
Trained 951 batches 	Training Loss: 0.045785
Trained 1001 batches 	Training Loss: 0.049135
Trained 1051 batches 	Training Loss: 0.058765
Trained 1101 batches 	Training Loss: 0.054023
Trained 1151 batches 	Training Loss: 0.057188
Trained 1201 batches 	Training Loss: 0.065221
Trained 1251 batches 	Training Loss: 0.059223
Trained 1301 batches 	Training Loss: 0.120308
Trained 1351 batches 	Training Loss: 0.049209
Trained 1401 batches 	Training Loss: 0.070976
Trained 1451 batches 	Training Loss: 0.107712
Trained 1501 batches 	Training Loss: 0.056226
Trained 1551 batches 	Training Loss: 0.080594
Trained 1601 batches 	Training Loss: 0.051110
Trained 1651 batches 	Training Loss: 0.047938
Trained 1701 batches 	Training Loss: 0.050054
Trained 1751 batches 	Training Loss: 0.067112
Trained 1801 batches 	Training Loss: 0.074188
Trained 1851 batches 	Training Loss: 0.036906
Trained 1901 batches 	Training Loss: 0.047514
Trained 1951 batches 	Training Loss: 0.053463
Trained 2001 batches 	Training Loss: 0.088071
Trained 2051 batches 	Training Loss: 0.081798
Trained 2101 batches 	Training Loss: 0.074284
Trained 2151 batches 	Training Loss: 0.074370
Trained 2201 batches 	Training Loss: 0.070015
Trained 2251 batches 	Training Loss: 0.057736
Trained 2301 batches 	Training Loss: 0.046134
Trained 2351 batches 	Training Loss: 0.068043
Trained 2401 batches 	Training Loss: 0.082863
Trained 2451 batches 	Training Loss: 0.062526
Trained 2501 batches 	Training Loss: 0.071266
Trained 2551 batches 	Training Loss: 0.056869
Trained 2601 batches 	Training Loss: 0.069101
Trained 2651 batches 	Training Loss: 0.116748
Trained 2701 batches 	Training Loss: 0.082446
Trained 2751 batches 	Training Loss: 0.086300
Trained 2801 batches 	Training Loss: 0.082057
Trained 2851 batches 	Training Loss: 0.064373
Trained 2901 batches 	Training Loss: 0.087278
Trained 2951 batches 	Training Loss: 0.066826
Trained 3001 batches 	Training Loss: 0.089932
Trained 3051 batches 	Training Loss: 0.067114
Trained 3101 batches 	Training Loss: 0.050466
Trained 3151 batches 	Training Loss: 0.091204
Trained 3201 batches 	Training Loss: 0.087391
Trained 3251 batches 	Training Loss: 0.107027
Trained 3301 batches 	Training Loss: 0.051357
Trained 3351 batches 	Training Loss: 0.096122
Trained 3401 batches 	Training Loss: 0.156939
Trained 3451 batches 	Training Loss: 0.056708
Trained 3501 batches 	Training Loss: 0.052681
Trained 3551 batches 	Training Loss: 0.099518
Trained 3601 batches 	Training Loss: 0.050356
Trained 3651 batches 	Training Loss: 0.048992
Trained 3701 batches 	Training Loss: 0.071994
Trained 3751 batches 	Training Loss: 0.053217
Trained 3801 batches 	Training Loss: 0.038427
Trained 3851 batches 	Training Loss: 0.082052
Trained 3901 batches 	Training Loss: 0.068178
Trained 3951 batches 	Training Loss: 0.078663
Trained 4001 batches 	Training Loss: 0.094328
Trained 4051 batches 	Training Loss: 0.079809
Trained 4101 batches 	Training Loss: 0.057420
Trained 4151 batches 	Training Loss: 0.064576
Trained 4201 batches 	Training Loss: 0.056318
Trained 4251 batches 	Training Loss: 0.077758
Trained 4301 batches 	Training Loss: 0.077739
Trained 4351 batches 	Training Loss: 0.095071
Trained 4401 batches 	Training Loss: 0.081383
Trained 4451 batches 	Training Loss: 0.095153
Trained 4501 batches 	Training Loss: 0.103639
Trained 4551 batches 	Training Loss: 0.095690
Trained 4601 batches 	Training Loss: 0.046820
Trained 4651 batches 	Training Loss: 0.103707
Trained 4701 batches 	Training Loss: 0.054458
Trained 4751 batches 	Training Loss: 0.066658
Trained 4801 batches 	Training Loss: 0.049045
Trained 4851 batches 	Training Loss: 0.047511
Trained 4901 batches 	Training Loss: 0.058746
Epoch: 15 	Training Loss: 0.075282
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1768962
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7720399000847205
The AUROC of Cardiomegaly is 0.8642296588910866
The AUROC of Effusion is 0.8673411975709802
The AUROC of Infiltration is 0.6757965088937452
The AUROC of Mass is 0.8042022111987457
The AUROC of Nodule is 0.7506130826639614
The AUROC of Pneumonia is 0.72121502181077
The AUROC of Pneumothorax is 0.8445164849625963
The AUROC of Consolidation is 0.7454883601143848
The AUROC of Edema is 0.8761525028140104
The AUROC of Emphysema is 0.8750401164776407
The AUROC of Fibrosis is 0.7860519112131116
The AUROC of Pleural_Thickening is 0.76733447655089
The AUROC of Hernia is 0.8620253309908483
Epoch: 15 	Learning Rate for first group: 0.0000000100
Started epoch 16
Trained 1 batches 	Training Loss: 0.094048
Trained 51 batches 	Training Loss: 0.078682
Trained 101 batches 	Training Loss: 0.090264
Trained 151 batches 	Training Loss: 0.058441
Trained 201 batches 	Training Loss: 0.068190
Trained 251 batches 	Training Loss: 0.081632
Trained 301 batches 	Training Loss: 0.085272
Trained 351 batches 	Training Loss: 0.044612
Trained 401 batches 	Training Loss: 0.087827
Trained 451 batches 	Training Loss: 0.034072
Trained 501 batches 	Training Loss: 0.118054
Trained 551 batches 	Training Loss: 0.073864
Trained 601 batches 	Training Loss: 0.067523
Trained 651 batches 	Training Loss: 0.046429
Trained 701 batches 	Training Loss: 0.067515
Trained 751 batches 	Training Loss: 0.090962
Trained 801 batches 	Training Loss: 0.088583
Trained 851 batches 	Training Loss: 0.079474
Trained 901 batches 	Training Loss: 0.065710
Trained 951 batches 	Training Loss: 0.125759
Trained 1001 batches 	Training Loss: 0.111135
Trained 1051 batches 	Training Loss: 0.065976
Trained 1101 batches 	Training Loss: 0.092339
Trained 1151 batches 	Training Loss: 0.072270
Trained 1201 batches 	Training Loss: 0.040081
Trained 1251 batches 	Training Loss: 0.079187
Trained 1301 batches 	Training Loss: 0.072380
Trained 1351 batches 	Training Loss: 0.083311
Trained 1401 batches 	Training Loss: 0.080771
Trained 1451 batches 	Training Loss: 0.068098
Trained 1501 batches 	Training Loss: 0.072775
Trained 1551 batches 	Training Loss: 0.111410
Trained 1601 batches 	Training Loss: 0.093218
Trained 1651 batches 	Training Loss: 0.078896
Trained 1701 batches 	Training Loss: 0.076582
Trained 1751 batches 	Training Loss: 0.064255
Trained 1801 batches 	Training Loss: 0.133388
Trained 1851 batches 	Training Loss: 0.095331
Trained 1901 batches 	Training Loss: 0.102391
Trained 1951 batches 	Training Loss: 0.071671
Trained 2001 batches 	Training Loss: 0.101506
Trained 2051 batches 	Training Loss: 0.070599
Trained 2101 batches 	Training Loss: 0.055419
Trained 2151 batches 	Training Loss: 0.045119
Trained 2201 batches 	Training Loss: 0.114715
Trained 2251 batches 	Training Loss: 0.117650
Trained 2301 batches 	Training Loss: 0.092972
Trained 2351 batches 	Training Loss: 0.047861
Trained 2401 batches 	Training Loss: 0.109752
Trained 2451 batches 	Training Loss: 0.051799
Trained 2501 batches 	Training Loss: 0.035179
Trained 2551 batches 	Training Loss: 0.085554
Trained 2601 batches 	Training Loss: 0.115619
Trained 2651 batches 	Training Loss: 0.132827
Trained 2701 batches 	Training Loss: 0.087127
Trained 2751 batches 	Training Loss: 0.048681
Trained 2801 batches 	Training Loss: 0.080493
Trained 2851 batches 	Training Loss: 0.081838
Trained 2901 batches 	Training Loss: 0.070075
Trained 2951 batches 	Training Loss: 0.066090
Trained 3001 batches 	Training Loss: 0.081891
Trained 3051 batches 	Training Loss: 0.057367
Trained 3101 batches 	Training Loss: 0.075275
Trained 3151 batches 	Training Loss: 0.088534
Trained 3201 batches 	Training Loss: 0.085202
Trained 3251 batches 	Training Loss: 0.115122
Trained 3301 batches 	Training Loss: 0.096617
Trained 3351 batches 	Training Loss: 0.049303
Trained 3401 batches 	Training Loss: 0.060300
Trained 3451 batches 	Training Loss: 0.052208
Trained 3501 batches 	Training Loss: 0.058137
Trained 3551 batches 	Training Loss: 0.063260
Trained 3601 batches 	Training Loss: 0.072310
Trained 3651 batches 	Training Loss: 0.106742
Trained 3701 batches 	Training Loss: 0.083429
Trained 3751 batches 	Training Loss: 0.082451
Trained 3801 batches 	Training Loss: 0.054451
Trained 3851 batches 	Training Loss: 0.070730
Trained 3901 batches 	Training Loss: 0.069116
Trained 3951 batches 	Training Loss: 0.053939
Trained 4001 batches 	Training Loss: 0.128937
Trained 4051 batches 	Training Loss: 0.116740
Trained 4101 batches 	Training Loss: 0.098486
Trained 4151 batches 	Training Loss: 0.048067
Trained 4201 batches 	Training Loss: 0.044534
Trained 4251 batches 	Training Loss: 0.072326
Trained 4301 batches 	Training Loss: 0.044381
Trained 4351 batches 	Training Loss: 0.084035
Trained 4401 batches 	Training Loss: 0.058715
Trained 4451 batches 	Training Loss: 0.060445
Trained 4501 batches 	Training Loss: 0.062903
Trained 4551 batches 	Training Loss: 0.066184
Trained 4601 batches 	Training Loss: 0.071231
Trained 4651 batches 	Training Loss: 0.045456
Trained 4701 batches 	Training Loss: 0.077837
Trained 4751 batches 	Training Loss: 0.053736
Trained 4801 batches 	Training Loss: 0.137445
Trained 4851 batches 	Training Loss: 0.042629
Trained 4901 batches 	Training Loss: 0.055458
Epoch: 16 	Training Loss: 0.075348
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1766379
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7708965247005812
The AUROC of Cardiomegaly is 0.8671139555841957
The AUROC of Effusion is 0.8664054603125397
The AUROC of Infiltration is 0.6757052753879775
The AUROC of Mass is 0.8024721051014623
The AUROC of Nodule is 0.7504614082887264
The AUROC of Pneumonia is 0.7192809827228221
The AUROC of Pneumothorax is 0.8421664161276281
The AUROC of Consolidation is 0.7432088062211014
The AUROC of Edema is 0.8751063530424419
The AUROC of Emphysema is 0.8733905563603387
The AUROC of Fibrosis is 0.7852926272779188
The AUROC of Pleural_Thickening is 0.7668323584134225
The AUROC of Hernia is 0.8629921802335596
Epoch: 16 	Learning Rate for first group: 0.0000000100
Started epoch 17
Trained 1 batches 	Training Loss: 0.109328
Trained 51 batches 	Training Loss: 0.100661
Trained 101 batches 	Training Loss: 0.052852
Trained 151 batches 	Training Loss: 0.060636
Trained 201 batches 	Training Loss: 0.039510
Trained 251 batches 	Training Loss: 0.083869
Trained 301 batches 	Training Loss: 0.040899
Trained 351 batches 	Training Loss: 0.063671
Trained 401 batches 	Training Loss: 0.094981
Trained 451 batches 	Training Loss: 0.074238
Trained 501 batches 	Training Loss: 0.087493
Trained 551 batches 	Training Loss: 0.081327
Trained 601 batches 	Training Loss: 0.054218
Trained 651 batches 	Training Loss: 0.046476
Trained 701 batches 	Training Loss: 0.112996
Trained 751 batches 	Training Loss: 0.081875
Trained 801 batches 	Training Loss: 0.073418
Trained 851 batches 	Training Loss: 0.050114
Trained 901 batches 	Training Loss: 0.056691
Trained 951 batches 	Training Loss: 0.069247
Trained 1001 batches 	Training Loss: 0.052162
Trained 1051 batches 	Training Loss: 0.088708
Trained 1101 batches 	Training Loss: 0.068219
Trained 1151 batches 	Training Loss: 0.045022
Trained 1201 batches 	Training Loss: 0.077715
Trained 1251 batches 	Training Loss: 0.108876
Trained 1301 batches 	Training Loss: 0.082477
Trained 1351 batches 	Training Loss: 0.094869
Trained 1401 batches 	Training Loss: 0.047706
Trained 1451 batches 	Training Loss: 0.080775
Trained 1501 batches 	Training Loss: 0.068336
Trained 1551 batches 	Training Loss: 0.082972
Trained 1601 batches 	Training Loss: 0.065756
Trained 1651 batches 	Training Loss: 0.082359
Trained 1701 batches 	Training Loss: 0.134479
Trained 1751 batches 	Training Loss: 0.131321
Trained 1801 batches 	Training Loss: 0.081268
Trained 1851 batches 	Training Loss: 0.085975
Trained 1901 batches 	Training Loss: 0.046728
Trained 1951 batches 	Training Loss: 0.077341
Trained 2001 batches 	Training Loss: 0.081809
Trained 2051 batches 	Training Loss: 0.110949
Trained 2101 batches 	Training Loss: 0.050617
Trained 2151 batches 	Training Loss: 0.108980
Trained 2201 batches 	Training Loss: 0.080866
Trained 2251 batches 	Training Loss: 0.039901
Trained 2301 batches 	Training Loss: 0.092447
Trained 2351 batches 	Training Loss: 0.093534
Trained 2401 batches 	Training Loss: 0.089426
Trained 2451 batches 	Training Loss: 0.097776
Trained 2501 batches 	Training Loss: 0.038275
Trained 2551 batches 	Training Loss: 0.067912
Trained 2601 batches 	Training Loss: 0.097776
Trained 2651 batches 	Training Loss: 0.072160
Trained 2701 batches 	Training Loss: 0.118753
Trained 2751 batches 	Training Loss: 0.025163
Trained 2801 batches 	Training Loss: 0.057929
Trained 2851 batches 	Training Loss: 0.071089
Trained 2901 batches 	Training Loss: 0.094485
Trained 2951 batches 	Training Loss: 0.052744
Trained 3001 batches 	Training Loss: 0.112667
Trained 3051 batches 	Training Loss: 0.041289
Trained 3101 batches 	Training Loss: 0.088137
Trained 3151 batches 	Training Loss: 0.054505
Trained 3201 batches 	Training Loss: 0.086330
Trained 3251 batches 	Training Loss: 0.084497
Trained 3301 batches 	Training Loss: 0.138099
Trained 3351 batches 	Training Loss: 0.053601
Trained 3401 batches 	Training Loss: 0.082215
Trained 3451 batches 	Training Loss: 0.040183
Trained 3501 batches 	Training Loss: 0.053489
Trained 3551 batches 	Training Loss: 0.074674
Trained 3601 batches 	Training Loss: 0.084010
Trained 3651 batches 	Training Loss: 0.046064
Trained 3701 batches 	Training Loss: 0.076471
Trained 3751 batches 	Training Loss: 0.108709
Trained 3801 batches 	Training Loss: 0.075817
Trained 3851 batches 	Training Loss: 0.056075
Trained 3901 batches 	Training Loss: 0.073229
Trained 3951 batches 	Training Loss: 0.055507
Trained 4001 batches 	Training Loss: 0.076745
Trained 4051 batches 	Training Loss: 0.086862
Trained 4101 batches 	Training Loss: 0.062824
Trained 4151 batches 	Training Loss: 0.059806
Trained 4201 batches 	Training Loss: 0.096424
Trained 4251 batches 	Training Loss: 0.056985
Trained 4301 batches 	Training Loss: 0.059086
Trained 4351 batches 	Training Loss: 0.093748
Trained 4401 batches 	Training Loss: 0.106371
Trained 4451 batches 	Training Loss: 0.085830
Trained 4501 batches 	Training Loss: 0.062875
Trained 4551 batches 	Training Loss: 0.091915
Trained 4601 batches 	Training Loss: 0.104423
Trained 4651 batches 	Training Loss: 0.085608
Trained 4701 batches 	Training Loss: 0.090759
Trained 4751 batches 	Training Loss: 0.055230
Trained 4801 batches 	Training Loss: 0.089691
Trained 4851 batches 	Training Loss: 0.069236
Trained 4901 batches 	Training Loss: 0.088261
Epoch: 17 	Training Loss: 0.075172
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1766931
The average AUROC is 0.802
The AUROC of Atelectasis is 0.7732555682510842
The AUROC of Cardiomegaly is 0.8654726481398209
The AUROC of Effusion is 0.8683956180216885
The AUROC of Infiltration is 0.6767269357381946
The AUROC of Mass is 0.8054961491400453
The AUROC of Nodule is 0.7517483575149088
The AUROC of Pneumonia is 0.7232568051507704
The AUROC of Pneumothorax is 0.8441352986729024
The AUROC of Consolidation is 0.7469282197765804
The AUROC of Edema is 0.8784012282328013
The AUROC of Emphysema is 0.8739757416763829
The AUROC of Fibrosis is 0.786992436652609
The AUROC of Pleural_Thickening is 0.7673581226717318
The AUROC of Hernia is 0.8662647697130457
Epoch: 17 	Learning Rate for first group: 0.0000000100
Started epoch 18
Trained 1 batches 	Training Loss: 0.059942
Trained 51 batches 	Training Loss: 0.050568
Trained 101 batches 	Training Loss: 0.050058
Trained 151 batches 	Training Loss: 0.070899
Trained 201 batches 	Training Loss: 0.070087
Trained 251 batches 	Training Loss: 0.070546
Trained 301 batches 	Training Loss: 0.067210
Trained 351 batches 	Training Loss: 0.055993
Trained 401 batches 	Training Loss: 0.069518
Trained 451 batches 	Training Loss: 0.082895
Trained 501 batches 	Training Loss: 0.082948
Trained 551 batches 	Training Loss: 0.069000
Trained 601 batches 	Training Loss: 0.063851
Trained 651 batches 	Training Loss: 0.110934
Trained 701 batches 	Training Loss: 0.072075
Trained 751 batches 	Training Loss: 0.051936
Trained 801 batches 	Training Loss: 0.067745
Trained 851 batches 	Training Loss: 0.046973
Trained 901 batches 	Training Loss: 0.078696
Trained 951 batches 	Training Loss: 0.063135
Trained 1001 batches 	Training Loss: 0.124517
Trained 1051 batches 	Training Loss: 0.075040
Trained 1101 batches 	Training Loss: 0.060502
Trained 1151 batches 	Training Loss: 0.078726
Trained 1201 batches 	Training Loss: 0.060463
Trained 1251 batches 	Training Loss: 0.090973
Trained 1301 batches 	Training Loss: 0.106122
Trained 1351 batches 	Training Loss: 0.059916
Trained 1401 batches 	Training Loss: 0.078184
Trained 1451 batches 	Training Loss: 0.043457
Trained 1501 batches 	Training Loss: 0.105346
Trained 1551 batches 	Training Loss: 0.099849
Trained 1601 batches 	Training Loss: 0.051513
Trained 1651 batches 	Training Loss: 0.062738
Trained 1701 batches 	Training Loss: 0.067114
Trained 1751 batches 	Training Loss: 0.065498
Trained 1801 batches 	Training Loss: 0.051592
Trained 1851 batches 	Training Loss: 0.070309
Trained 1901 batches 	Training Loss: 0.060744
Trained 1951 batches 	Training Loss: 0.085229
Trained 2001 batches 	Training Loss: 0.131683
Trained 2051 batches 	Training Loss: 0.064900
Trained 2101 batches 	Training Loss: 0.103260
Trained 2151 batches 	Training Loss: 0.071911
Trained 2201 batches 	Training Loss: 0.096739
Trained 2251 batches 	Training Loss: 0.059692
Trained 2301 batches 	Training Loss: 0.064243
Trained 2351 batches 	Training Loss: 0.063917
Trained 2401 batches 	Training Loss: 0.054100
Trained 2451 batches 	Training Loss: 0.064944
Trained 2501 batches 	Training Loss: 0.070959
Trained 2551 batches 	Training Loss: 0.079642
Trained 2601 batches 	Training Loss: 0.104503
Trained 2651 batches 	Training Loss: 0.074088
Trained 2701 batches 	Training Loss: 0.123897
Trained 2751 batches 	Training Loss: 0.059538
Trained 2801 batches 	Training Loss: 0.064509
Trained 2851 batches 	Training Loss: 0.040905
Trained 2901 batches 	Training Loss: 0.045964
Trained 2951 batches 	Training Loss: 0.074061
Trained 3001 batches 	Training Loss: 0.064397
Trained 3051 batches 	Training Loss: 0.108685
Trained 3101 batches 	Training Loss: 0.084247
Trained 3151 batches 	Training Loss: 0.075093
Trained 3201 batches 	Training Loss: 0.061418
Trained 3251 batches 	Training Loss: 0.065835
Trained 3301 batches 	Training Loss: 0.099245
Trained 3351 batches 	Training Loss: 0.077823
Trained 3401 batches 	Training Loss: 0.072917
Trained 3451 batches 	Training Loss: 0.053309
Trained 3501 batches 	Training Loss: 0.091124
Trained 3551 batches 	Training Loss: 0.046021
Trained 3601 batches 	Training Loss: 0.095538
Trained 3651 batches 	Training Loss: 0.071974
Trained 3701 batches 	Training Loss: 0.061822
Trained 3751 batches 	Training Loss: 0.068551
Trained 3801 batches 	Training Loss: 0.086821
Trained 3851 batches 	Training Loss: 0.080696
Trained 3901 batches 	Training Loss: 0.039815
Trained 3951 batches 	Training Loss: 0.079602
Trained 4001 batches 	Training Loss: 0.101520
Trained 4051 batches 	Training Loss: 0.112132
Trained 4101 batches 	Training Loss: 0.027328
Trained 4151 batches 	Training Loss: 0.083712
Trained 4201 batches 	Training Loss: 0.105939
Trained 4251 batches 	Training Loss: 0.065979
Trained 4301 batches 	Training Loss: 0.062322
Trained 4351 batches 	Training Loss: 0.074244
Trained 4401 batches 	Training Loss: 0.058846
Trained 4451 batches 	Training Loss: 0.067167
Trained 4501 batches 	Training Loss: 0.052220
Trained 4551 batches 	Training Loss: 0.073912
Trained 4601 batches 	Training Loss: 0.141750
Trained 4651 batches 	Training Loss: 0.080002
Trained 4701 batches 	Training Loss: 0.064556
Trained 4751 batches 	Training Loss: 0.051869
Trained 4801 batches 	Training Loss: 0.068703
Trained 4851 batches 	Training Loss: 0.065293
Trained 4901 batches 	Training Loss: 0.096936
Epoch: 18 	Training Loss: 0.075341
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1752667
The average AUROC is 0.799
The AUROC of Atelectasis is 0.7704699411105502
The AUROC of Cardiomegaly is 0.8645367112292847
The AUROC of Effusion is 0.8652651274017571
The AUROC of Infiltration is 0.6750901546377492
The AUROC of Mass is 0.8013782913729085
The AUROC of Nodule is 0.7483002981407405
The AUROC of Pneumonia is 0.718861911156828
The AUROC of Pneumothorax is 0.8410465825462865
The AUROC of Consolidation is 0.7421460029041995
The AUROC of Edema is 0.8738578428126863
The AUROC of Emphysema is 0.8735723686534996
The AUROC of Fibrosis is 0.7844411139381687
The AUROC of Pleural_Thickening is 0.7650707224107076
The AUROC of Hernia is 0.8648179406800096
Epoch: 18 	Learning Rate for first group: 0.0000000100
Started epoch 19
Trained 1 batches 	Training Loss: 0.074708
Trained 51 batches 	Training Loss: 0.077084
Trained 101 batches 	Training Loss: 0.100681
Trained 151 batches 	Training Loss: 0.063359
Trained 201 batches 	Training Loss: 0.059044
Trained 251 batches 	Training Loss: 0.054009
Trained 301 batches 	Training Loss: 0.078506
Trained 351 batches 	Training Loss: 0.071191
Trained 401 batches 	Training Loss: 0.107843
Trained 451 batches 	Training Loss: 0.056116
Trained 501 batches 	Training Loss: 0.060825
Trained 551 batches 	Training Loss: 0.066840
Trained 601 batches 	Training Loss: 0.068555
Trained 651 batches 	Training Loss: 0.109425
Trained 701 batches 	Training Loss: 0.098815
Trained 751 batches 	Training Loss: 0.070496
Trained 801 batches 	Training Loss: 0.092406
Trained 851 batches 	Training Loss: 0.055601
Trained 901 batches 	Training Loss: 0.053847
Trained 951 batches 	Training Loss: 0.039333
Trained 1001 batches 	Training Loss: 0.078812
Trained 1051 batches 	Training Loss: 0.065339
Trained 1101 batches 	Training Loss: 0.076352
Trained 1151 batches 	Training Loss: 0.083477
Trained 1201 batches 	Training Loss: 0.055277
Trained 1251 batches 	Training Loss: 0.060145
Trained 1301 batches 	Training Loss: 0.070779
Trained 1351 batches 	Training Loss: 0.039410
Trained 1401 batches 	Training Loss: 0.108395
Trained 1451 batches 	Training Loss: 0.087751
Trained 1501 batches 	Training Loss: 0.067392
Trained 1551 batches 	Training Loss: 0.076669
Trained 1601 batches 	Training Loss: 0.067296
Trained 1651 batches 	Training Loss: 0.087402
Trained 1701 batches 	Training Loss: 0.057633
Trained 1751 batches 	Training Loss: 0.086790
Trained 1801 batches 	Training Loss: 0.118515
Trained 1851 batches 	Training Loss: 0.048455
Trained 1901 batches 	Training Loss: 0.095729
Trained 1951 batches 	Training Loss: 0.061072
Trained 2001 batches 	Training Loss: 0.066100
Trained 2051 batches 	Training Loss: 0.052153
Trained 2101 batches 	Training Loss: 0.073070
Trained 2151 batches 	Training Loss: 0.064005
Trained 2201 batches 	Training Loss: 0.053293
Trained 2251 batches 	Training Loss: 0.088410
Trained 2301 batches 	Training Loss: 0.035667
Trained 2351 batches 	Training Loss: 0.067814
Trained 2401 batches 	Training Loss: 0.071240
Trained 2451 batches 	Training Loss: 0.069589
Trained 2501 batches 	Training Loss: 0.115880
Trained 2551 batches 	Training Loss: 0.055849
Trained 2601 batches 	Training Loss: 0.112533
Trained 2651 batches 	Training Loss: 0.064055
Trained 2701 batches 	Training Loss: 0.050300
Trained 2751 batches 	Training Loss: 0.058694
Trained 2801 batches 	Training Loss: 0.070889
Trained 2851 batches 	Training Loss: 0.024909
Trained 2901 batches 	Training Loss: 0.060571
Trained 2951 batches 	Training Loss: 0.058135
Trained 3001 batches 	Training Loss: 0.060103
Trained 3051 batches 	Training Loss: 0.071243
Trained 3101 batches 	Training Loss: 0.090720
Trained 3151 batches 	Training Loss: 0.057901
Trained 3201 batches 	Training Loss: 0.108295
Trained 3251 batches 	Training Loss: 0.043230
Trained 3301 batches 	Training Loss: 0.076193
Trained 3351 batches 	Training Loss: 0.087759
Trained 3401 batches 	Training Loss: 0.063690
Trained 3451 batches 	Training Loss: 0.045356
Trained 3501 batches 	Training Loss: 0.103541
Trained 3551 batches 	Training Loss: 0.046163
Trained 3601 batches 	Training Loss: 0.121268
Trained 3651 batches 	Training Loss: 0.055321
Trained 3701 batches 	Training Loss: 0.067647
Trained 3751 batches 	Training Loss: 0.041485
Trained 3801 batches 	Training Loss: 0.048170
Trained 3851 batches 	Training Loss: 0.118133
Trained 3901 batches 	Training Loss: 0.051188
Trained 3951 batches 	Training Loss: 0.177269
Trained 4001 batches 	Training Loss: 0.085629
Trained 4051 batches 	Training Loss: 0.073512
Trained 4101 batches 	Training Loss: 0.099276
Trained 4151 batches 	Training Loss: 0.083243
Trained 4201 batches 	Training Loss: 0.052114
Trained 4251 batches 	Training Loss: 0.046991
Trained 4301 batches 	Training Loss: 0.092519
Trained 4351 batches 	Training Loss: 0.034314
Trained 4401 batches 	Training Loss: 0.082047
Trained 4451 batches 	Training Loss: 0.051423
Trained 4501 batches 	Training Loss: 0.091248
Trained 4551 batches 	Training Loss: 0.052287
Trained 4601 batches 	Training Loss: 0.044762
Trained 4651 batches 	Training Loss: 0.093289
Trained 4701 batches 	Training Loss: 0.045396
Trained 4751 batches 	Training Loss: 0.058669
Trained 4801 batches 	Training Loss: 0.097828
Trained 4851 batches 	Training Loss: 0.075337
Trained 4901 batches 	Training Loss: 0.056983
Epoch: 19 	Training Loss: 0.075301
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1750720
The average AUROC is 0.799
The AUROC of Atelectasis is 0.7696894556159046
The AUROC of Cardiomegaly is 0.8658199756944612
The AUROC of Effusion is 0.8644075527680752
The AUROC of Infiltration is 0.6748930537497579
The AUROC of Mass is 0.8017048795215823
The AUROC of Nodule is 0.7489174116618047
The AUROC of Pneumonia is 0.718240447073166
The AUROC of Pneumothorax is 0.8418228739611486
The AUROC of Consolidation is 0.7408534231023984
The AUROC of Edema is 0.8730794378600278
The AUROC of Emphysema is 0.8722334352071041
The AUROC of Fibrosis is 0.7857387602115914
The AUROC of Pleural_Thickening is 0.7652333498132319
The AUROC of Hernia is 0.8596782527817011
Epoch: 19 	Learning Rate for first group: 0.0000000100
Started epoch 20
Trained 1 batches 	Training Loss: 0.075375
Trained 51 batches 	Training Loss: 0.071941
Trained 101 batches 	Training Loss: 0.116662
Trained 151 batches 	Training Loss: 0.059153
Trained 201 batches 	Training Loss: 0.082529
Trained 251 batches 	Training Loss: 0.070397
Trained 301 batches 	Training Loss: 0.067383
Trained 351 batches 	Training Loss: 0.079117
Trained 401 batches 	Training Loss: 0.104037
Trained 451 batches 	Training Loss: 0.110506
Trained 501 batches 	Training Loss: 0.126682
Trained 551 batches 	Training Loss: 0.059849
Trained 601 batches 	Training Loss: 0.074265
Trained 651 batches 	Training Loss: 0.098258
Trained 701 batches 	Training Loss: 0.076947
Trained 751 batches 	Training Loss: 0.050894
Trained 801 batches 	Training Loss: 0.112105
Trained 851 batches 	Training Loss: 0.058383
Trained 901 batches 	Training Loss: 0.111248
Trained 951 batches 	Training Loss: 0.067100
Trained 1001 batches 	Training Loss: 0.078692
Trained 1051 batches 	Training Loss: 0.055831
Trained 1101 batches 	Training Loss: 0.108653
Trained 1151 batches 	Training Loss: 0.069457
Trained 1201 batches 	Training Loss: 0.073485
Trained 1251 batches 	Training Loss: 0.032620
Trained 1301 batches 	Training Loss: 0.079016
Trained 1351 batches 	Training Loss: 0.096840
Trained 1401 batches 	Training Loss: 0.102639
Trained 1451 batches 	Training Loss: 0.122019
Trained 1501 batches 	Training Loss: 0.073962
Trained 1551 batches 	Training Loss: 0.166016
Trained 1601 batches 	Training Loss: 0.088034
Trained 1651 batches 	Training Loss: 0.077931
Trained 1701 batches 	Training Loss: 0.097276
Trained 1751 batches 	Training Loss: 0.099541
Trained 1801 batches 	Training Loss: 0.048988
Trained 1851 batches 	Training Loss: 0.121746
Trained 1901 batches 	Training Loss: 0.077150
Trained 1951 batches 	Training Loss: 0.104622
Trained 2001 batches 	Training Loss: 0.043380
Trained 2051 batches 	Training Loss: 0.049647
Trained 2101 batches 	Training Loss: 0.037479
Trained 2151 batches 	Training Loss: 0.080273
Trained 2201 batches 	Training Loss: 0.086543
Trained 2251 batches 	Training Loss: 0.069170
Trained 2301 batches 	Training Loss: 0.045234
Trained 2351 batches 	Training Loss: 0.037217
Trained 2401 batches 	Training Loss: 0.073322
Trained 2451 batches 	Training Loss: 0.115431
Trained 2501 batches 	Training Loss: 0.116166
Trained 2551 batches 	Training Loss: 0.060431
Trained 2601 batches 	Training Loss: 0.032463
Trained 2651 batches 	Training Loss: 0.111911
Trained 2701 batches 	Training Loss: 0.101029
Trained 2751 batches 	Training Loss: 0.078511
Trained 2801 batches 	Training Loss: 0.076935
Trained 2851 batches 	Training Loss: 0.076813
Trained 2901 batches 	Training Loss: 0.074605
Trained 2951 batches 	Training Loss: 0.154408
Trained 3001 batches 	Training Loss: 0.123412
Trained 3051 batches 	Training Loss: 0.079037
Trained 3101 batches 	Training Loss: 0.056343
Trained 3151 batches 	Training Loss: 0.063515
Trained 3201 batches 	Training Loss: 0.078020
Trained 3251 batches 	Training Loss: 0.048380
Trained 3301 batches 	Training Loss: 0.082275
Trained 3351 batches 	Training Loss: 0.044483
Trained 3401 batches 	Training Loss: 0.065580
Trained 3451 batches 	Training Loss: 0.057486
Trained 3501 batches 	Training Loss: 0.093661
Trained 3551 batches 	Training Loss: 0.073696
Trained 3601 batches 	Training Loss: 0.068443
Trained 3651 batches 	Training Loss: 0.061538
Trained 3701 batches 	Training Loss: 0.066611
Trained 3751 batches 	Training Loss: 0.060825
Trained 3801 batches 	Training Loss: 0.073470
Trained 3851 batches 	Training Loss: 0.122674
Trained 3901 batches 	Training Loss: 0.070220
Trained 3951 batches 	Training Loss: 0.061156
Trained 4001 batches 	Training Loss: 0.046001
Trained 4051 batches 	Training Loss: 0.075935
Trained 4101 batches 	Training Loss: 0.068183
Trained 4151 batches 	Training Loss: 0.089868
Trained 4201 batches 	Training Loss: 0.098056
Trained 4251 batches 	Training Loss: 0.064210
Trained 4301 batches 	Training Loss: 0.080670
Trained 4351 batches 	Training Loss: 0.105976
Trained 4401 batches 	Training Loss: 0.083070
Trained 4451 batches 	Training Loss: 0.075240
Trained 4501 batches 	Training Loss: 0.062658
Trained 4551 batches 	Training Loss: 0.068452
Trained 4601 batches 	Training Loss: 0.053366
Trained 4651 batches 	Training Loss: 0.091424
Trained 4701 batches 	Training Loss: 0.064854
Trained 4751 batches 	Training Loss: 0.117491
Trained 4801 batches 	Training Loss: 0.062178
Trained 4851 batches 	Training Loss: 0.060835
Trained 4901 batches 	Training Loss: 0.044042
Epoch: 20 	Training Loss: 0.075381
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1783886
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7703292445780198
The AUROC of Cardiomegaly is 0.8659252824530489
The AUROC of Effusion is 0.8656977896362559
The AUROC of Infiltration is 0.6764632216658829
The AUROC of Mass is 0.8028649047349158
The AUROC of Nodule is 0.7494172379425649
The AUROC of Pneumonia is 0.7200715040859478
The AUROC of Pneumothorax is 0.8421592985413059
The AUROC of Consolidation is 0.7435738214426739
The AUROC of Edema is 0.8755776997947428
The AUROC of Emphysema is 0.8707410286306516
The AUROC of Fibrosis is 0.7875774499105858
The AUROC of Pleural_Thickening is 0.7643715934807163
The AUROC of Hernia is 0.8652634721600239
Epoch: 20 	Learning Rate for first group: 0.0000000100
Started epoch 21
Trained 1 batches 	Training Loss: 0.071959
Trained 51 batches 	Training Loss: 0.063617
Trained 101 batches 	Training Loss: 0.076461
Trained 151 batches 	Training Loss: 0.088515
Trained 201 batches 	Training Loss: 0.087843
Trained 251 batches 	Training Loss: 0.045064
Trained 301 batches 	Training Loss: 0.077427
Trained 351 batches 	Training Loss: 0.049317
Trained 401 batches 	Training Loss: 0.102545
Trained 451 batches 	Training Loss: 0.053309
Trained 501 batches 	Training Loss: 0.077036
Trained 551 batches 	Training Loss: 0.079979
Trained 601 batches 	Training Loss: 0.081899
Trained 651 batches 	Training Loss: 0.041310
Trained 701 batches 	Training Loss: 0.083190
Trained 751 batches 	Training Loss: 0.059032
Trained 801 batches 	Training Loss: 0.027375
Trained 851 batches 	Training Loss: 0.066989
Trained 901 batches 	Training Loss: 0.078677
Trained 951 batches 	Training Loss: 0.056901
Trained 1001 batches 	Training Loss: 0.064642
Trained 1051 batches 	Training Loss: 0.105807
Trained 1101 batches 	Training Loss: 0.092355
Trained 1151 batches 	Training Loss: 0.046593
Trained 1201 batches 	Training Loss: 0.050152
Trained 1251 batches 	Training Loss: 0.086034
Trained 1301 batches 	Training Loss: 0.041697
Trained 1351 batches 	Training Loss: 0.057573
Trained 1401 batches 	Training Loss: 0.079432
Trained 1451 batches 	Training Loss: 0.073017
Trained 1501 batches 	Training Loss: 0.054453
Trained 1551 batches 	Training Loss: 0.068799
Trained 1601 batches 	Training Loss: 0.085420
Trained 1651 batches 	Training Loss: 0.064663
Trained 1701 batches 	Training Loss: 0.133738
Trained 1751 batches 	Training Loss: 0.075263
Trained 1801 batches 	Training Loss: 0.076477
Trained 1851 batches 	Training Loss: 0.078457
Trained 1901 batches 	Training Loss: 0.066727
Trained 1951 batches 	Training Loss: 0.057808
Trained 2001 batches 	Training Loss: 0.096768
Trained 2051 batches 	Training Loss: 0.099436
Trained 2101 batches 	Training Loss: 0.094406
Trained 2151 batches 	Training Loss: 0.081952
Trained 2201 batches 	Training Loss: 0.076635
Trained 2251 batches 	Training Loss: 0.054830
Trained 2301 batches 	Training Loss: 0.084189
Trained 2351 batches 	Training Loss: 0.047036
Trained 2401 batches 	Training Loss: 0.060613
Trained 2451 batches 	Training Loss: 0.043142
Trained 2501 batches 	Training Loss: 0.069073
Trained 2551 batches 	Training Loss: 0.064258
Trained 2601 batches 	Training Loss: 0.083119
Trained 2651 batches 	Training Loss: 0.049932
Trained 2701 batches 	Training Loss: 0.061110
Trained 2751 batches 	Training Loss: 0.127189
Trained 2801 batches 	Training Loss: 0.121727
Trained 2851 batches 	Training Loss: 0.050230
Trained 2901 batches 	Training Loss: 0.098808
Trained 2951 batches 	Training Loss: 0.045182
Trained 3001 batches 	Training Loss: 0.106421
Trained 3051 batches 	Training Loss: 0.056824
Trained 3101 batches 	Training Loss: 0.102513
Trained 3151 batches 	Training Loss: 0.065658
Trained 3201 batches 	Training Loss: 0.065158
Trained 3251 batches 	Training Loss: 0.084271
Trained 3301 batches 	Training Loss: 0.042329
Trained 3351 batches 	Training Loss: 0.093195
Trained 3401 batches 	Training Loss: 0.124536
Trained 3451 batches 	Training Loss: 0.057133
Trained 3501 batches 	Training Loss: 0.104737
Trained 3551 batches 	Training Loss: 0.045586
Trained 3601 batches 	Training Loss: 0.067923
Trained 3651 batches 	Training Loss: 0.117471
Trained 3701 batches 	Training Loss: 0.068945
Trained 3751 batches 	Training Loss: 0.106089
Trained 3801 batches 	Training Loss: 0.157238
Trained 3851 batches 	Training Loss: 0.114652
Trained 3901 batches 	Training Loss: 0.100861
Trained 3951 batches 	Training Loss: 0.054582
Trained 4001 batches 	Training Loss: 0.109446
Trained 4051 batches 	Training Loss: 0.058350
Trained 4101 batches 	Training Loss: 0.088670
Trained 4151 batches 	Training Loss: 0.083175
Trained 4201 batches 	Training Loss: 0.087169
Trained 4251 batches 	Training Loss: 0.071714
Trained 4301 batches 	Training Loss: 0.051290
Trained 4351 batches 	Training Loss: 0.118522
Trained 4401 batches 	Training Loss: 0.063565
Trained 4451 batches 	Training Loss: 0.067716
Trained 4501 batches 	Training Loss: 0.072190
Trained 4551 batches 	Training Loss: 0.090126
Trained 4601 batches 	Training Loss: 0.089880
Trained 4651 batches 	Training Loss: 0.070424
Trained 4701 batches 	Training Loss: 0.084811
Trained 4751 batches 	Training Loss: 0.066167
Trained 4801 batches 	Training Loss: 0.057089
Trained 4851 batches 	Training Loss: 0.052968
Trained 4901 batches 	Training Loss: 0.043763
Epoch: 21 	Training Loss: 0.075459
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1773052
The average AUROC is 0.799
The AUROC of Atelectasis is 0.7714981147269602
The AUROC of Cardiomegaly is 0.8660671694540935
The AUROC of Effusion is 0.8655247099808654
The AUROC of Infiltration is 0.6752513762483058
The AUROC of Mass is 0.8014308467389519
The AUROC of Nodule is 0.7508568603404326
The AUROC of Pneumonia is 0.718576775815761
The AUROC of Pneumothorax is 0.8428830779859698
The AUROC of Consolidation is 0.740971147477295
The AUROC of Edema is 0.8713492352512746
The AUROC of Emphysema is 0.8717877374601043
The AUROC of Fibrosis is 0.7850722418984243
The AUROC of Pleural_Thickening is 0.7656954143174366
The AUROC of Hernia is 0.8603396603396604
Epoch: 21 	Learning Rate for first group: 0.0000000100
Started epoch 22
Trained 1 batches 	Training Loss: 0.072773
Trained 51 batches 	Training Loss: 0.060559
Trained 101 batches 	Training Loss: 0.121562
Trained 151 batches 	Training Loss: 0.046349
Trained 201 batches 	Training Loss: 0.076167
Trained 251 batches 	Training Loss: 0.031832
Trained 301 batches 	Training Loss: 0.078083
Trained 351 batches 	Training Loss: 0.070794
Trained 401 batches 	Training Loss: 0.103752
Trained 451 batches 	Training Loss: 0.046640
Trained 501 batches 	Training Loss: 0.083857
Trained 551 batches 	Training Loss: 0.059717
Trained 601 batches 	Training Loss: 0.071222
Trained 651 batches 	Training Loss: 0.071334
Trained 701 batches 	Training Loss: 0.136599
Trained 751 batches 	Training Loss: 0.094649
Trained 801 batches 	Training Loss: 0.047678
Trained 851 batches 	Training Loss: 0.130015
Trained 901 batches 	Training Loss: 0.093948
Trained 951 batches 	Training Loss: 0.041350
Trained 1001 batches 	Training Loss: 0.057441
Trained 1051 batches 	Training Loss: 0.071444
Trained 1101 batches 	Training Loss: 0.049783
Trained 1151 batches 	Training Loss: 0.061866
Trained 1201 batches 	Training Loss: 0.050702
Trained 1251 batches 	Training Loss: 0.104062
Trained 1301 batches 	Training Loss: 0.075503
Trained 1351 batches 	Training Loss: 0.075488
Trained 1401 batches 	Training Loss: 0.118961
Trained 1451 batches 	Training Loss: 0.093919
Trained 1501 batches 	Training Loss: 0.077343
Trained 1551 batches 	Training Loss: 0.099306
Trained 1601 batches 	Training Loss: 0.082397
Trained 1651 batches 	Training Loss: 0.062589
Trained 1701 batches 	Training Loss: 0.073013
Trained 1751 batches 	Training Loss: 0.049761
Trained 1801 batches 	Training Loss: 0.063687
Trained 1851 batches 	Training Loss: 0.074517
Trained 1901 batches 	Training Loss: 0.069411
Trained 1951 batches 	Training Loss: 0.048727
Trained 2001 batches 	Training Loss: 0.061329
Trained 2051 batches 	Training Loss: 0.033710
Trained 2101 batches 	Training Loss: 0.138779
Trained 2151 batches 	Training Loss: 0.077340
Trained 2201 batches 	Training Loss: 0.044043
Trained 2251 batches 	Training Loss: 0.091696
Trained 2301 batches 	Training Loss: 0.096522
Trained 2351 batches 	Training Loss: 0.105842
Trained 2401 batches 	Training Loss: 0.060777
Trained 2451 batches 	Training Loss: 0.061265
Trained 2501 batches 	Training Loss: 0.049960
Trained 2551 batches 	Training Loss: 0.063169
Trained 2601 batches 	Training Loss: 0.077758
Trained 2651 batches 	Training Loss: 0.069803
Trained 2701 batches 	Training Loss: 0.065685
Trained 2751 batches 	Training Loss: 0.047354
Trained 2801 batches 	Training Loss: 0.045385
Trained 2851 batches 	Training Loss: 0.058041
Trained 2901 batches 	Training Loss: 0.096289
Trained 2951 batches 	Training Loss: 0.066008
Trained 3001 batches 	Training Loss: 0.066030
Trained 3051 batches 	Training Loss: 0.034352
Trained 3101 batches 	Training Loss: 0.082526
Trained 3151 batches 	Training Loss: 0.084426
Trained 3201 batches 	Training Loss: 0.043287
Trained 3251 batches 	Training Loss: 0.080481
Trained 3301 batches 	Training Loss: 0.107968
Trained 3351 batches 	Training Loss: 0.097062
Trained 3401 batches 	Training Loss: 0.126051
Trained 3451 batches 	Training Loss: 0.107858
Trained 3501 batches 	Training Loss: 0.069548
Trained 3551 batches 	Training Loss: 0.100028
Trained 3601 batches 	Training Loss: 0.050515
Trained 3651 batches 	Training Loss: 0.056740
Trained 3701 batches 	Training Loss: 0.056833
Trained 3751 batches 	Training Loss: 0.097308
Trained 3801 batches 	Training Loss: 0.109654
Trained 3851 batches 	Training Loss: 0.097772
Trained 3901 batches 	Training Loss: 0.081011
Trained 3951 batches 	Training Loss: 0.061858
Trained 4001 batches 	Training Loss: 0.048375
Trained 4051 batches 	Training Loss: 0.107030
Trained 4101 batches 	Training Loss: 0.097483
Trained 4151 batches 	Training Loss: 0.102560
Trained 4201 batches 	Training Loss: 0.059263
Trained 4251 batches 	Training Loss: 0.082230
Trained 4301 batches 	Training Loss: 0.084639
Trained 4351 batches 	Training Loss: 0.146263
Trained 4401 batches 	Training Loss: 0.089275
Trained 4451 batches 	Training Loss: 0.096135
Trained 4501 batches 	Training Loss: 0.073869
Trained 4551 batches 	Training Loss: 0.060888
Trained 4601 batches 	Training Loss: 0.066005
Trained 4651 batches 	Training Loss: 0.070098
Trained 4701 batches 	Training Loss: 0.073618
Trained 4751 batches 	Training Loss: 0.064729
Trained 4801 batches 	Training Loss: 0.078458
Trained 4851 batches 	Training Loss: 0.074915
Trained 4901 batches 	Training Loss: 0.096355
Epoch: 22 	Training Loss: 0.075208
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1772141
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7685297462977064
The AUROC of Cardiomegaly is 0.8665042848765824
The AUROC of Effusion is 0.8670397289771877
The AUROC of Infiltration is 0.6746393814490028
The AUROC of Mass is 0.8060602709903649
The AUROC of Nodule is 0.7529912883546754
The AUROC of Pneumonia is 0.7216477846353125
The AUROC of Pneumothorax is 0.8445601394920386
The AUROC of Consolidation is 0.7443868460774199
The AUROC of Edema is 0.8746496970800502
The AUROC of Emphysema is 0.8743286930714058
The AUROC of Fibrosis is 0.7875865656075478
The AUROC of Pleural_Thickening is 0.7688111526687663
The AUROC of Hernia is 0.8638419052212155
Epoch: 22 	Learning Rate for first group: 0.0000000100
Started epoch 23
Trained 1 batches 	Training Loss: 0.050148
Trained 51 batches 	Training Loss: 0.052008
Trained 101 batches 	Training Loss: 0.074096
Trained 151 batches 	Training Loss: 0.071049
Trained 201 batches 	Training Loss: 0.063038
Trained 251 batches 	Training Loss: 0.072969
Trained 301 batches 	Training Loss: 0.056029
Trained 351 batches 	Training Loss: 0.080903
Trained 401 batches 	Training Loss: 0.086203
Trained 451 batches 	Training Loss: 0.165518
Trained 501 batches 	Training Loss: 0.149149
Trained 551 batches 	Training Loss: 0.046654
Trained 601 batches 	Training Loss: 0.061237
Trained 651 batches 	Training Loss: 0.076634
Trained 701 batches 	Training Loss: 0.032772
Trained 751 batches 	Training Loss: 0.082642
Trained 801 batches 	Training Loss: 0.109883
Trained 851 batches 	Training Loss: 0.065092
Trained 901 batches 	Training Loss: 0.098294
Trained 951 batches 	Training Loss: 0.083009
Trained 1001 batches 	Training Loss: 0.073424
Trained 1051 batches 	Training Loss: 0.117750
Trained 1101 batches 	Training Loss: 0.105953
Trained 1151 batches 	Training Loss: 0.117075
Trained 1201 batches 	Training Loss: 0.122535
Trained 1251 batches 	Training Loss: 0.049293
Trained 1301 batches 	Training Loss: 0.077768
Trained 1351 batches 	Training Loss: 0.052044
Trained 1401 batches 	Training Loss: 0.073273
Trained 1451 batches 	Training Loss: 0.041053
Trained 1501 batches 	Training Loss: 0.085422
Trained 1551 batches 	Training Loss: 0.087092
Trained 1601 batches 	Training Loss: 0.050988
Trained 1651 batches 	Training Loss: 0.091840
Trained 1701 batches 	Training Loss: 0.078178
Trained 1751 batches 	Training Loss: 0.080862
Trained 1801 batches 	Training Loss: 0.066833
Trained 1851 batches 	Training Loss: 0.048649
Trained 1901 batches 	Training Loss: 0.045172
Trained 1951 batches 	Training Loss: 0.076942
Trained 2001 batches 	Training Loss: 0.100543
Trained 2051 batches 	Training Loss: 0.101261
Trained 2101 batches 	Training Loss: 0.053527
Trained 2151 batches 	Training Loss: 0.050792
Trained 2201 batches 	Training Loss: 0.092522
Trained 2251 batches 	Training Loss: 0.062245
Trained 2301 batches 	Training Loss: 0.075551
Trained 2351 batches 	Training Loss: 0.064506
Trained 2401 batches 	Training Loss: 0.133030
Trained 2451 batches 	Training Loss: 0.068263
Trained 2501 batches 	Training Loss: 0.112382
Trained 2551 batches 	Training Loss: 0.070873
Trained 2601 batches 	Training Loss: 0.060832
Trained 2651 batches 	Training Loss: 0.064848
Trained 2701 batches 	Training Loss: 0.081522
Trained 2751 batches 	Training Loss: 0.125933
Trained 2801 batches 	Training Loss: 0.113684
Trained 2851 batches 	Training Loss: 0.061080
Trained 2901 batches 	Training Loss: 0.064352
Trained 2951 batches 	Training Loss: 0.057928
Trained 3001 batches 	Training Loss: 0.121912
Trained 3051 batches 	Training Loss: 0.071145
Trained 3101 batches 	Training Loss: 0.055725
Trained 3151 batches 	Training Loss: 0.068975
Trained 3201 batches 	Training Loss: 0.114573
Trained 3251 batches 	Training Loss: 0.039266
Trained 3301 batches 	Training Loss: 0.138470
Trained 3351 batches 	Training Loss: 0.043495
Trained 3401 batches 	Training Loss: 0.091601
Trained 3451 batches 	Training Loss: 0.062320
Trained 3501 batches 	Training Loss: 0.060541
Trained 3551 batches 	Training Loss: 0.072325
Trained 3601 batches 	Training Loss: 0.071011
Trained 3651 batches 	Training Loss: 0.049142
Trained 3701 batches 	Training Loss: 0.057147
Trained 3751 batches 	Training Loss: 0.123214
Trained 3801 batches 	Training Loss: 0.067237
Trained 3851 batches 	Training Loss: 0.043787
Trained 3901 batches 	Training Loss: 0.065916
Trained 3951 batches 	Training Loss: 0.110471
Trained 4001 batches 	Training Loss: 0.047671
Trained 4051 batches 	Training Loss: 0.066065
Trained 4101 batches 	Training Loss: 0.104264
Trained 4151 batches 	Training Loss: 0.125011
Trained 4201 batches 	Training Loss: 0.086814
Trained 4251 batches 	Training Loss: 0.050511
Trained 4301 batches 	Training Loss: 0.077039
Trained 4351 batches 	Training Loss: 0.109364
Trained 4401 batches 	Training Loss: 0.044812
Trained 4451 batches 	Training Loss: 0.074941
Trained 4501 batches 	Training Loss: 0.057285
Trained 4551 batches 	Training Loss: 0.038391
Trained 4601 batches 	Training Loss: 0.118512
Trained 4651 batches 	Training Loss: 0.071190
Trained 4701 batches 	Training Loss: 0.082109
Trained 4751 batches 	Training Loss: 0.035095
Trained 4801 batches 	Training Loss: 0.083434
Trained 4851 batches 	Training Loss: 0.110244
Trained 4901 batches 	Training Loss: 0.052689
Epoch: 23 	Training Loss: 0.075374
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1753240
The average AUROC is 0.799
The AUROC of Atelectasis is 0.7691687574534376
The AUROC of Cardiomegaly is 0.8644339909524867
The AUROC of Effusion is 0.8654225597706049
The AUROC of Infiltration is 0.6764591551633516
The AUROC of Mass is 0.8008985891656824
The AUROC of Nodule is 0.7487097333427506
The AUROC of Pneumonia is 0.7176898203706878
The AUROC of Pneumothorax is 0.8401434399263062
The AUROC of Consolidation is 0.7414071157667469
The AUROC of Edema is 0.8754721744024365
The AUROC of Emphysema is 0.872450211402796
The AUROC of Fibrosis is 0.785715702860452
The AUROC of Pleural_Thickening is 0.7629153061098921
The AUROC of Hernia is 0.8626362143603522
Epoch: 23 	Learning Rate for first group: 0.0000000100
Started epoch 24
Trained 1 batches 	Training Loss: 0.051695
Trained 51 batches 	Training Loss: 0.067432
Trained 101 batches 	Training Loss: 0.053857
Trained 151 batches 	Training Loss: 0.059815
Trained 201 batches 	Training Loss: 0.068968
Trained 251 batches 	Training Loss: 0.054091
Trained 301 batches 	Training Loss: 0.097534
Trained 351 batches 	Training Loss: 0.093301
Trained 401 batches 	Training Loss: 0.057945
Trained 451 batches 	Training Loss: 0.079337
Trained 501 batches 	Training Loss: 0.123933
Trained 551 batches 	Training Loss: 0.065027
Trained 601 batches 	Training Loss: 0.068654
Trained 651 batches 	Training Loss: 0.134566
Trained 701 batches 	Training Loss: 0.072676
Trained 751 batches 	Training Loss: 0.064381
Trained 801 batches 	Training Loss: 0.081229
Trained 851 batches 	Training Loss: 0.091866
Trained 901 batches 	Training Loss: 0.043125
Trained 951 batches 	Training Loss: 0.065183
Trained 1001 batches 	Training Loss: 0.078480
Trained 1051 batches 	Training Loss: 0.060511
Trained 1101 batches 	Training Loss: 0.043936
Trained 1151 batches 	Training Loss: 0.063078
Trained 1201 batches 	Training Loss: 0.119881
Trained 1251 batches 	Training Loss: 0.094074
Trained 1301 batches 	Training Loss: 0.063981
Trained 1351 batches 	Training Loss: 0.048658
Trained 1401 batches 	Training Loss: 0.040563
Trained 1451 batches 	Training Loss: 0.065692
Trained 1501 batches 	Training Loss: 0.057630
Trained 1551 batches 	Training Loss: 0.048868
Trained 1601 batches 	Training Loss: 0.094088
Trained 1651 batches 	Training Loss: 0.130334
Trained 1701 batches 	Training Loss: 0.060865
Trained 1751 batches 	Training Loss: 0.099070
Trained 1801 batches 	Training Loss: 0.086274
Trained 1851 batches 	Training Loss: 0.095658
Trained 1901 batches 	Training Loss: 0.078962
Trained 1951 batches 	Training Loss: 0.089783
Trained 2001 batches 	Training Loss: 0.048570
Trained 2051 batches 	Training Loss: 0.092298
Trained 2101 batches 	Training Loss: 0.141276
Trained 2151 batches 	Training Loss: 0.089967
Trained 2201 batches 	Training Loss: 0.052521
Trained 2251 batches 	Training Loss: 0.090008
Trained 2301 batches 	Training Loss: 0.076594
Trained 2351 batches 	Training Loss: 0.052847
Trained 2401 batches 	Training Loss: 0.095816
Trained 2451 batches 	Training Loss: 0.069426
Trained 2501 batches 	Training Loss: 0.101191
Trained 2551 batches 	Training Loss: 0.053536
Trained 2601 batches 	Training Loss: 0.048704
Trained 2651 batches 	Training Loss: 0.072419
Trained 2701 batches 	Training Loss: 0.085337
Trained 2751 batches 	Training Loss: 0.064169
Trained 2801 batches 	Training Loss: 0.089413
Trained 2851 batches 	Training Loss: 0.055282
Trained 2901 batches 	Training Loss: 0.055734
Trained 2951 batches 	Training Loss: 0.075769
Trained 3001 batches 	Training Loss: 0.061895
Trained 3051 batches 	Training Loss: 0.091261
Trained 3101 batches 	Training Loss: 0.062158
Trained 3151 batches 	Training Loss: 0.121348
Trained 3201 batches 	Training Loss: 0.069788
Trained 3251 batches 	Training Loss: 0.077451
Trained 3301 batches 	Training Loss: 0.091223
Trained 3351 batches 	Training Loss: 0.069296
Trained 3401 batches 	Training Loss: 0.072096
Trained 3451 batches 	Training Loss: 0.071789
Trained 3501 batches 	Training Loss: 0.061881
Trained 3551 batches 	Training Loss: 0.050651
Trained 3601 batches 	Training Loss: 0.071594
Trained 3651 batches 	Training Loss: 0.049109
Trained 3701 batches 	Training Loss: 0.077087
Trained 3751 batches 	Training Loss: 0.079171
Trained 3801 batches 	Training Loss: 0.059350
Trained 3851 batches 	Training Loss: 0.154242
Trained 3901 batches 	Training Loss: 0.045853
Trained 3951 batches 	Training Loss: 0.053710
Trained 4001 batches 	Training Loss: 0.051770
Trained 4051 batches 	Training Loss: 0.070992
Trained 4101 batches 	Training Loss: 0.071027
Trained 4151 batches 	Training Loss: 0.055400
Trained 4201 batches 	Training Loss: 0.063216
Trained 4251 batches 	Training Loss: 0.076225
Trained 4301 batches 	Training Loss: 0.114307
Trained 4351 batches 	Training Loss: 0.070502
Trained 4401 batches 	Training Loss: 0.069621
Trained 4451 batches 	Training Loss: 0.042503
Trained 4501 batches 	Training Loss: 0.062008
Trained 4551 batches 	Training Loss: 0.051149
Trained 4601 batches 	Training Loss: 0.063037
Trained 4651 batches 	Training Loss: 0.107673
Trained 4701 batches 	Training Loss: 0.048041
Trained 4751 batches 	Training Loss: 0.064244
Trained 4801 batches 	Training Loss: 0.067679
Trained 4851 batches 	Training Loss: 0.092662
Trained 4901 batches 	Training Loss: 0.061914
Epoch: 24 	Training Loss: 0.075223
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1769668
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7717579884820702
The AUROC of Cardiomegaly is 0.8670659209223839
The AUROC of Effusion is 0.8663192526206436
The AUROC of Infiltration is 0.6769383938698167
The AUROC of Mass is 0.8025787055919571
The AUROC of Nodule is 0.7492644220856723
The AUROC of Pneumonia is 0.7213257424233765
The AUROC of Pneumothorax is 0.8419938542014637
The AUROC of Consolidation is 0.7440266074999682
The AUROC of Edema is 0.8753058167251537
The AUROC of Emphysema is 0.8732617419825932
The AUROC of Fibrosis is 0.7850191563690571
The AUROC of Pleural_Thickening is 0.7637981750503023
The AUROC of Hernia is 0.8668412047722392
Epoch: 24 	Learning Rate for first group: 0.0000000100
Started epoch 25
Trained 1 batches 	Training Loss: 0.061153
Trained 51 batches 	Training Loss: 0.065209
Trained 101 batches 	Training Loss: 0.061330
Trained 151 batches 	Training Loss: 0.069713
Trained 201 batches 	Training Loss: 0.065374
Trained 251 batches 	Training Loss: 0.063136
Trained 301 batches 	Training Loss: 0.060833
Trained 351 batches 	Training Loss: 0.064951
Trained 401 batches 	Training Loss: 0.077867
Trained 451 batches 	Training Loss: 0.090851
Trained 501 batches 	Training Loss: 0.085986
Trained 551 batches 	Training Loss: 0.055785
Trained 601 batches 	Training Loss: 0.065182
Trained 651 batches 	Training Loss: 0.068619
Trained 701 batches 	Training Loss: 0.084325
Trained 751 batches 	Training Loss: 0.049183
Trained 801 batches 	Training Loss: 0.127033
Trained 851 batches 	Training Loss: 0.070407
Trained 901 batches 	Training Loss: 0.044795
Trained 951 batches 	Training Loss: 0.056778
Trained 1001 batches 	Training Loss: 0.055397
Trained 1051 batches 	Training Loss: 0.063492
Trained 1101 batches 	Training Loss: 0.048392
Trained 1151 batches 	Training Loss: 0.058050
Trained 1201 batches 	Training Loss: 0.088148
Trained 1251 batches 	Training Loss: 0.074944
Trained 1301 batches 	Training Loss: 0.081409
Trained 1351 batches 	Training Loss: 0.092857
Trained 1401 batches 	Training Loss: 0.049569
Trained 1451 batches 	Training Loss: 0.102037
Trained 1501 batches 	Training Loss: 0.073931
Trained 1551 batches 	Training Loss: 0.072917
Trained 1601 batches 	Training Loss: 0.056832
Trained 1651 batches 	Training Loss: 0.068104
Trained 1701 batches 	Training Loss: 0.091936
Trained 1751 batches 	Training Loss: 0.090604
Trained 1801 batches 	Training Loss: 0.093310
Trained 1851 batches 	Training Loss: 0.081451
Trained 1901 batches 	Training Loss: 0.094271
Trained 1951 batches 	Training Loss: 0.125481
Trained 2001 batches 	Training Loss: 0.088743
Trained 2051 batches 	Training Loss: 0.123943
Trained 2101 batches 	Training Loss: 0.103714
Trained 2151 batches 	Training Loss: 0.065120
Trained 2201 batches 	Training Loss: 0.059271
Trained 2251 batches 	Training Loss: 0.100424
Trained 2301 batches 	Training Loss: 0.072280
Trained 2351 batches 	Training Loss: 0.075795
Trained 2401 batches 	Training Loss: 0.037091
Trained 2451 batches 	Training Loss: 0.107249
Trained 2501 batches 	Training Loss: 0.098912
Trained 2551 batches 	Training Loss: 0.039028
Trained 2601 batches 	Training Loss: 0.076591
Trained 2651 batches 	Training Loss: 0.054794
Trained 2701 batches 	Training Loss: 0.057050
Trained 2751 batches 	Training Loss: 0.038148
Trained 2801 batches 	Training Loss: 0.069319
Trained 2851 batches 	Training Loss: 0.047490
Trained 2901 batches 	Training Loss: 0.053724
Trained 2951 batches 	Training Loss: 0.076756
Trained 3001 batches 	Training Loss: 0.058004
Trained 3051 batches 	Training Loss: 0.080009
Trained 3101 batches 	Training Loss: 0.070278
Trained 3151 batches 	Training Loss: 0.075312
Trained 3201 batches 	Training Loss: 0.091419
Trained 3251 batches 	Training Loss: 0.065928
Trained 3301 batches 	Training Loss: 0.157135
Trained 3351 batches 	Training Loss: 0.118470
Trained 3401 batches 	Training Loss: 0.045290
Trained 3451 batches 	Training Loss: 0.044943
Trained 3501 batches 	Training Loss: 0.101198
Trained 3551 batches 	Training Loss: 0.095464
Trained 3601 batches 	Training Loss: 0.101467
Trained 3651 batches 	Training Loss: 0.107525
Trained 3701 batches 	Training Loss: 0.059688
Trained 3751 batches 	Training Loss: 0.067217
Trained 3801 batches 	Training Loss: 0.126865
Trained 3851 batches 	Training Loss: 0.070368
Trained 3901 batches 	Training Loss: 0.074550
Trained 3951 batches 	Training Loss: 0.070422
Trained 4001 batches 	Training Loss: 0.077562
Trained 4051 batches 	Training Loss: 0.061469
Trained 4101 batches 	Training Loss: 0.093617
Trained 4151 batches 	Training Loss: 0.127398
Trained 4201 batches 	Training Loss: 0.084328
Trained 4251 batches 	Training Loss: 0.048419
Trained 4301 batches 	Training Loss: 0.063769
Trained 4351 batches 	Training Loss: 0.112016
Trained 4401 batches 	Training Loss: 0.091220
Trained 4451 batches 	Training Loss: 0.066105
Trained 4501 batches 	Training Loss: 0.074078
Trained 4551 batches 	Training Loss: 0.076153
Trained 4601 batches 	Training Loss: 0.055782
Trained 4651 batches 	Training Loss: 0.126675
Trained 4701 batches 	Training Loss: 0.068814
Trained 4751 batches 	Training Loss: 0.065251
Trained 4801 batches 	Training Loss: 0.073791
Trained 4851 batches 	Training Loss: 0.077158
Trained 4901 batches 	Training Loss: 0.083487
Epoch: 25 	Training Loss: 0.075355
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1760394
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7712158574325838
The AUROC of Cardiomegaly is 0.8667777129515118
The AUROC of Effusion is 0.866290393710086
The AUROC of Infiltration is 0.6758366206534067
The AUROC of Mass is 0.803500700517509
The AUROC of Nodule is 0.7500683105429387
The AUROC of Pneumonia is 0.720605463169324
The AUROC of Pneumothorax is 0.8430244807009019
The AUROC of Consolidation is 0.7443640575044264
The AUROC of Edema is 0.874385469774217
The AUROC of Emphysema is 0.8741752199413491
The AUROC of Fibrosis is 0.7857473396910851
The AUROC of Pleural_Thickening is 0.7670468625096304
The AUROC of Hernia is 0.8642759539311263
Epoch: 25 	Learning Rate for first group: 0.0000000100
Started epoch 26
Trained 1 batches 	Training Loss: 0.069615
Trained 51 batches 	Training Loss: 0.069890
Trained 101 batches 	Training Loss: 0.076525
Trained 151 batches 	Training Loss: 0.057840
Trained 201 batches 	Training Loss: 0.111150
Trained 251 batches 	Training Loss: 0.067197
Trained 301 batches 	Training Loss: 0.041298
Trained 351 batches 	Training Loss: 0.076692
Trained 401 batches 	Training Loss: 0.040403
Trained 451 batches 	Training Loss: 0.050504
Trained 501 batches 	Training Loss: 0.086558
Trained 551 batches 	Training Loss: 0.111833
Trained 601 batches 	Training Loss: 0.035286
Trained 651 batches 	Training Loss: 0.045687
Trained 701 batches 	Training Loss: 0.159690
Trained 751 batches 	Training Loss: 0.051996
Trained 801 batches 	Training Loss: 0.053529
Trained 851 batches 	Training Loss: 0.109190
Trained 901 batches 	Training Loss: 0.111620
Trained 951 batches 	Training Loss: 0.085515
Trained 1001 batches 	Training Loss: 0.057725
Trained 1051 batches 	Training Loss: 0.060523
Trained 1101 batches 	Training Loss: 0.066136
Trained 1151 batches 	Training Loss: 0.082425
Trained 1201 batches 	Training Loss: 0.070500
Trained 1251 batches 	Training Loss: 0.108984
Trained 1301 batches 	Training Loss: 0.072470
Trained 1351 batches 	Training Loss: 0.093959
Trained 1401 batches 	Training Loss: 0.046988
Trained 1451 batches 	Training Loss: 0.059504
Trained 1501 batches 	Training Loss: 0.062616
Trained 1551 batches 	Training Loss: 0.076825
Trained 1601 batches 	Training Loss: 0.068511
Trained 1651 batches 	Training Loss: 0.081785
Trained 1701 batches 	Training Loss: 0.080093
Trained 1751 batches 	Training Loss: 0.054631
Trained 1801 batches 	Training Loss: 0.128336
Trained 1851 batches 	Training Loss: 0.112090
Trained 1901 batches 	Training Loss: 0.089611
Trained 1951 batches 	Training Loss: 0.072077
Trained 2001 batches 	Training Loss: 0.076343
Trained 2051 batches 	Training Loss: 0.127271
Trained 2101 batches 	Training Loss: 0.091803
Trained 2151 batches 	Training Loss: 0.044202
Trained 2201 batches 	Training Loss: 0.041924
Trained 2251 batches 	Training Loss: 0.060861
Trained 2301 batches 	Training Loss: 0.052789
Trained 2351 batches 	Training Loss: 0.064608
Trained 2401 batches 	Training Loss: 0.063668
Trained 2451 batches 	Training Loss: 0.098564
Trained 2501 batches 	Training Loss: 0.071180
Trained 2551 batches 	Training Loss: 0.114128
Trained 2601 batches 	Training Loss: 0.074477
Trained 2651 batches 	Training Loss: 0.129104
Trained 2701 batches 	Training Loss: 0.092928
Trained 2751 batches 	Training Loss: 0.085709
Trained 2801 batches 	Training Loss: 0.067721
Trained 2851 batches 	Training Loss: 0.062259
Trained 2901 batches 	Training Loss: 0.051063
Trained 2951 batches 	Training Loss: 0.081292
Trained 3001 batches 	Training Loss: 0.070391
Trained 3051 batches 	Training Loss: 0.059109
Trained 3101 batches 	Training Loss: 0.049646
Trained 3151 batches 	Training Loss: 0.062045
Trained 3201 batches 	Training Loss: 0.038019
Trained 3251 batches 	Training Loss: 0.054926
Trained 3301 batches 	Training Loss: 0.071097
Trained 3351 batches 	Training Loss: 0.114606
Trained 3401 batches 	Training Loss: 0.068041
Trained 3451 batches 	Training Loss: 0.098233
Trained 3501 batches 	Training Loss: 0.046164
Trained 3551 batches 	Training Loss: 0.049770
Trained 3601 batches 	Training Loss: 0.054616
Trained 3651 batches 	Training Loss: 0.082884
Trained 3701 batches 	Training Loss: 0.141396
Trained 3751 batches 	Training Loss: 0.040210
Trained 3801 batches 	Training Loss: 0.086675
Trained 3851 batches 	Training Loss: 0.101462
Trained 3901 batches 	Training Loss: 0.095673
Trained 3951 batches 	Training Loss: 0.109467
Trained 4001 batches 	Training Loss: 0.065312
Trained 4051 batches 	Training Loss: 0.095440
Trained 4101 batches 	Training Loss: 0.062683
Trained 4151 batches 	Training Loss: 0.073901
Trained 4201 batches 	Training Loss: 0.053664
Trained 4251 batches 	Training Loss: 0.042241
Trained 4301 batches 	Training Loss: 0.075785
Trained 4351 batches 	Training Loss: 0.091169
Trained 4401 batches 	Training Loss: 0.056378
Trained 4451 batches 	Training Loss: 0.081853
Trained 4501 batches 	Training Loss: 0.066176
Trained 4551 batches 	Training Loss: 0.073431
Trained 4601 batches 	Training Loss: 0.102133
Trained 4651 batches 	Training Loss: 0.056843
Trained 4701 batches 	Training Loss: 0.086662
Trained 4751 batches 	Training Loss: 0.069865
Trained 4801 batches 	Training Loss: 0.035155
Trained 4851 batches 	Training Loss: 0.064678
Trained 4901 batches 	Training Loss: 0.117005
Epoch: 26 	Training Loss: 0.075106
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1762430
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7720695863617009
The AUROC of Cardiomegaly is 0.866274457494682
The AUROC of Effusion is 0.8665871385947336
The AUROC of Infiltration is 0.6777336302525658
The AUROC of Mass is 0.8029366614315453
The AUROC of Nodule is 0.7495709099107005
The AUROC of Pneumonia is 0.720432238985085
The AUROC of Pneumothorax is 0.8417475857147195
The AUROC of Consolidation is 0.7443080314494248
The AUROC of Edema is 0.8783077037674635
The AUROC of Emphysema is 0.8729253524361374
The AUROC of Fibrosis is 0.7863135853376696
The AUROC of Pleural_Thickening is 0.7634627861934642
The AUROC of Hernia is 0.8626545867925178
Epoch: 26 	Learning Rate for first group: 0.0000000100
Started epoch 27
Trained 1 batches 	Training Loss: 0.081636
Trained 51 batches 	Training Loss: 0.139454
Trained 101 batches 	Training Loss: 0.052876
Trained 151 batches 	Training Loss: 0.068525
Trained 201 batches 	Training Loss: 0.083549
Trained 251 batches 	Training Loss: 0.063889
Trained 301 batches 	Training Loss: 0.061335
Trained 351 batches 	Training Loss: 0.054468
Trained 401 batches 	Training Loss: 0.049606
Trained 451 batches 	Training Loss: 0.076844
Trained 501 batches 	Training Loss: 0.087947
Trained 551 batches 	Training Loss: 0.055087
Trained 601 batches 	Training Loss: 0.066230
Trained 651 batches 	Training Loss: 0.088176
Trained 701 batches 	Training Loss: 0.059312
Trained 751 batches 	Training Loss: 0.115779
Trained 801 batches 	Training Loss: 0.054853
Trained 851 batches 	Training Loss: 0.034623
Trained 901 batches 	Training Loss: 0.052407
Trained 951 batches 	Training Loss: 0.040710
Trained 1001 batches 	Training Loss: 0.073573
Trained 1051 batches 	Training Loss: 0.033969
Trained 1101 batches 	Training Loss: 0.096925
Trained 1151 batches 	Training Loss: 0.089255
Trained 1201 batches 	Training Loss: 0.056390
Trained 1251 batches 	Training Loss: 0.066568
Trained 1301 batches 	Training Loss: 0.048198
Trained 1351 batches 	Training Loss: 0.085481
Trained 1401 batches 	Training Loss: 0.085719
Trained 1451 batches 	Training Loss: 0.085417
Trained 1501 batches 	Training Loss: 0.101107
Trained 1551 batches 	Training Loss: 0.047980
Trained 1601 batches 	Training Loss: 0.101777
Trained 1651 batches 	Training Loss: 0.054730
Trained 1701 batches 	Training Loss: 0.069194
Trained 1751 batches 	Training Loss: 0.055083
Trained 1801 batches 	Training Loss: 0.049807
Trained 1851 batches 	Training Loss: 0.070992
Trained 1901 batches 	Training Loss: 0.079262
Trained 1951 batches 	Training Loss: 0.054068
Trained 2001 batches 	Training Loss: 0.071066
Trained 2051 batches 	Training Loss: 0.084526
Trained 2101 batches 	Training Loss: 0.092749
Trained 2151 batches 	Training Loss: 0.093337
Trained 2201 batches 	Training Loss: 0.066395
Trained 2251 batches 	Training Loss: 0.052940
Trained 2301 batches 	Training Loss: 0.052596
Trained 2351 batches 	Training Loss: 0.105443
Trained 2401 batches 	Training Loss: 0.067374
Trained 2451 batches 	Training Loss: 0.060043
Trained 2501 batches 	Training Loss: 0.061837
Trained 2551 batches 	Training Loss: 0.054831
Trained 2601 batches 	Training Loss: 0.033312
Trained 2651 batches 	Training Loss: 0.071795
Trained 2701 batches 	Training Loss: 0.071121
Trained 2751 batches 	Training Loss: 0.075230
Trained 2801 batches 	Training Loss: 0.060134
Trained 2851 batches 	Training Loss: 0.078925
Trained 2901 batches 	Training Loss: 0.071738
Trained 2951 batches 	Training Loss: 0.051255
Trained 3001 batches 	Training Loss: 0.061064
Trained 3051 batches 	Training Loss: 0.063998
Trained 3101 batches 	Training Loss: 0.054874
Trained 3151 batches 	Training Loss: 0.108308
Trained 3201 batches 	Training Loss: 0.049854
Trained 3251 batches 	Training Loss: 0.042633
Trained 3301 batches 	Training Loss: 0.088799
Trained 3351 batches 	Training Loss: 0.035270
Trained 3401 batches 	Training Loss: 0.088464
Trained 3451 batches 	Training Loss: 0.103550
Trained 3501 batches 	Training Loss: 0.050093
Trained 3551 batches 	Training Loss: 0.080298
Trained 3601 batches 	Training Loss: 0.083057
Trained 3651 batches 	Training Loss: 0.148616
Trained 3701 batches 	Training Loss: 0.071177
Trained 3751 batches 	Training Loss: 0.053859
Trained 3801 batches 	Training Loss: 0.082242
Trained 3851 batches 	Training Loss: 0.057803
Trained 3901 batches 	Training Loss: 0.073779
Trained 3951 batches 	Training Loss: 0.095276
Trained 4001 batches 	Training Loss: 0.059929
Trained 4051 batches 	Training Loss: 0.099733
Trained 4101 batches 	Training Loss: 0.110246
Trained 4151 batches 	Training Loss: 0.082620
Trained 4201 batches 	Training Loss: 0.102310
Trained 4251 batches 	Training Loss: 0.083464
Trained 4301 batches 	Training Loss: 0.081951
Trained 4351 batches 	Training Loss: 0.068170
Trained 4401 batches 	Training Loss: 0.073324
Trained 4451 batches 	Training Loss: 0.081047
Trained 4501 batches 	Training Loss: 0.114598
Trained 4551 batches 	Training Loss: 0.055012
Trained 4601 batches 	Training Loss: 0.067208
Trained 4651 batches 	Training Loss: 0.082119
Trained 4701 batches 	Training Loss: 0.061402
Trained 4751 batches 	Training Loss: 0.058836
Trained 4801 batches 	Training Loss: 0.068150
Trained 4851 batches 	Training Loss: 0.055498
Trained 4901 batches 	Training Loss: 0.060028
Epoch: 27 	Training Loss: 0.075145
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1777403
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7717979590879027
The AUROC of Cardiomegaly is 0.8666110696247643
The AUROC of Effusion is 0.8671902603022067
The AUROC of Infiltration is 0.6755861019668593
The AUROC of Mass is 0.8050829894750224
The AUROC of Nodule is 0.7528267009681904
The AUROC of Pneumonia is 0.7199191144255863
The AUROC of Pneumothorax is 0.8446656379381902
The AUROC of Consolidation is 0.7457573448864432
The AUROC of Edema is 0.875424170694564
The AUROC of Emphysema is 0.8747959212215431
The AUROC of Fibrosis is 0.7859052557355162
The AUROC of Pleural_Thickening is 0.768312412548562
The AUROC of Hernia is 0.865203761755486
Epoch: 27 	Learning Rate for first group: 0.0000000100
Started epoch 28
Trained 1 batches 	Training Loss: 0.059131
Trained 51 batches 	Training Loss: 0.104376
Trained 101 batches 	Training Loss: 0.067589
Trained 151 batches 	Training Loss: 0.052087
Trained 201 batches 	Training Loss: 0.083198
Trained 251 batches 	Training Loss: 0.095682
Trained 301 batches 	Training Loss: 0.079219
Trained 351 batches 	Training Loss: 0.096697
Trained 401 batches 	Training Loss: 0.048983
Trained 451 batches 	Training Loss: 0.060640
Trained 501 batches 	Training Loss: 0.053044
Trained 551 batches 	Training Loss: 0.066483
Trained 601 batches 	Training Loss: 0.057721
Trained 651 batches 	Training Loss: 0.059278
Trained 701 batches 	Training Loss: 0.047291
Trained 751 batches 	Training Loss: 0.083236
Trained 801 batches 	Training Loss: 0.117149
Trained 851 batches 	Training Loss: 0.055365
Trained 901 batches 	Training Loss: 0.063348
Trained 951 batches 	Training Loss: 0.082594
Trained 1001 batches 	Training Loss: 0.127147
Trained 1051 batches 	Training Loss: 0.053739
Trained 1101 batches 	Training Loss: 0.066478
Trained 1151 batches 	Training Loss: 0.058103
Trained 1201 batches 	Training Loss: 0.051847
Trained 1251 batches 	Training Loss: 0.105242
Trained 1301 batches 	Training Loss: 0.085705
Trained 1351 batches 	Training Loss: 0.074318
Trained 1401 batches 	Training Loss: 0.116700
Trained 1451 batches 	Training Loss: 0.072374
Trained 1501 batches 	Training Loss: 0.064312
Trained 1551 batches 	Training Loss: 0.092281
Trained 1601 batches 	Training Loss: 0.056022
Trained 1651 batches 	Training Loss: 0.069775
Trained 1701 batches 	Training Loss: 0.092759
Trained 1751 batches 	Training Loss: 0.101954
Trained 1801 batches 	Training Loss: 0.050658
Trained 1851 batches 	Training Loss: 0.058064
Trained 1901 batches 	Training Loss: 0.063362
Trained 1951 batches 	Training Loss: 0.080513
Trained 2001 batches 	Training Loss: 0.099608
Trained 2051 batches 	Training Loss: 0.076447
Trained 2101 batches 	Training Loss: 0.050200
Trained 2151 batches 	Training Loss: 0.056320
Trained 2201 batches 	Training Loss: 0.056820
Trained 2251 batches 	Training Loss: 0.069213
Trained 2301 batches 	Training Loss: 0.057411
Trained 2351 batches 	Training Loss: 0.069884
Trained 2401 batches 	Training Loss: 0.063328
Trained 2451 batches 	Training Loss: 0.063974
Trained 2501 batches 	Training Loss: 0.041637
Trained 2551 batches 	Training Loss: 0.096646
Trained 2601 batches 	Training Loss: 0.163926
Trained 2651 batches 	Training Loss: 0.059975
Trained 2701 batches 	Training Loss: 0.056300
Trained 2751 batches 	Training Loss: 0.059215
Trained 2801 batches 	Training Loss: 0.058067
Trained 2851 batches 	Training Loss: 0.096767
Trained 2901 batches 	Training Loss: 0.076672
Trained 2951 batches 	Training Loss: 0.056978
Trained 3001 batches 	Training Loss: 0.128278
Trained 3051 batches 	Training Loss: 0.043292
Trained 3101 batches 	Training Loss: 0.066272
Trained 3151 batches 	Training Loss: 0.063679
Trained 3201 batches 	Training Loss: 0.049600
Trained 3251 batches 	Training Loss: 0.051016
Trained 3301 batches 	Training Loss: 0.085881
Trained 3351 batches 	Training Loss: 0.080420
Trained 3401 batches 	Training Loss: 0.076642
Trained 3451 batches 	Training Loss: 0.101170
Trained 3501 batches 	Training Loss: 0.096054
Trained 3551 batches 	Training Loss: 0.074411
Trained 3601 batches 	Training Loss: 0.058207
Trained 3651 batches 	Training Loss: 0.094810
Trained 3701 batches 	Training Loss: 0.094285
Trained 3751 batches 	Training Loss: 0.111323
Trained 3801 batches 	Training Loss: 0.079009
Trained 3851 batches 	Training Loss: 0.164960
Trained 3901 batches 	Training Loss: 0.084452
Trained 3951 batches 	Training Loss: 0.108201
Trained 4001 batches 	Training Loss: 0.055101
Trained 4051 batches 	Training Loss: 0.112878
Trained 4101 batches 	Training Loss: 0.063744
Trained 4151 batches 	Training Loss: 0.077104
Trained 4201 batches 	Training Loss: 0.114851
Trained 4251 batches 	Training Loss: 0.092477
Trained 4301 batches 	Training Loss: 0.092437
Trained 4351 batches 	Training Loss: 0.071434
Trained 4401 batches 	Training Loss: 0.046768
Trained 4451 batches 	Training Loss: 0.107496
Trained 4501 batches 	Training Loss: 0.067284
Trained 4551 batches 	Training Loss: 0.100072
Trained 4601 batches 	Training Loss: 0.095761
Trained 4651 batches 	Training Loss: 0.067498
Trained 4701 batches 	Training Loss: 0.102179
Trained 4751 batches 	Training Loss: 0.071854
Trained 4801 batches 	Training Loss: 0.063541
Trained 4851 batches 	Training Loss: 0.053537
Trained 4901 batches 	Training Loss: 0.100301
Epoch: 28 	Training Loss: 0.075216
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1778955
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7697012955575242
The AUROC of Cardiomegaly is 0.8670077250821115
The AUROC of Effusion is 0.8664594139279302
The AUROC of Infiltration is 0.6788728318902321
The AUROC of Mass is 0.8037350891736277
The AUROC of Nodule is 0.7504395774520274
The AUROC of Pneumonia is 0.7212674057565196
The AUROC of Pneumothorax is 0.8429686471904198
The AUROC of Consolidation is 0.7443739093329258
The AUROC of Edema is 0.8770753327153546
The AUROC of Emphysema is 0.8734498109741017
The AUROC of Fibrosis is 0.7857031017499458
The AUROC of Pleural_Thickening is 0.7658849452145922
The AUROC of Hernia is 0.865665369113645
Epoch: 28 	Learning Rate for first group: 0.0000000100
Started epoch 29
Trained 1 batches 	Training Loss: 0.045841
Trained 51 batches 	Training Loss: 0.067905
Trained 101 batches 	Training Loss: 0.076579
Trained 151 batches 	Training Loss: 0.082687
Trained 201 batches 	Training Loss: 0.062178
Trained 251 batches 	Training Loss: 0.055976
Trained 301 batches 	Training Loss: 0.095900
Trained 351 batches 	Training Loss: 0.042270
Trained 401 batches 	Training Loss: 0.070049
Trained 451 batches 	Training Loss: 0.077289
Trained 501 batches 	Training Loss: 0.071497
Trained 551 batches 	Training Loss: 0.064856
Trained 601 batches 	Training Loss: 0.044077
Trained 651 batches 	Training Loss: 0.042229
Trained 701 batches 	Training Loss: 0.052912
Trained 751 batches 	Training Loss: 0.071534
Trained 801 batches 	Training Loss: 0.041870
Trained 851 batches 	Training Loss: 0.066375
Trained 901 batches 	Training Loss: 0.028077
Trained 951 batches 	Training Loss: 0.054512
Trained 1001 batches 	Training Loss: 0.091685
Trained 1051 batches 	Training Loss: 0.062208
Trained 1101 batches 	Training Loss: 0.052036
Trained 1151 batches 	Training Loss: 0.068489
Trained 1201 batches 	Training Loss: 0.069826
Trained 1251 batches 	Training Loss: 0.097843
Trained 1301 batches 	Training Loss: 0.074105
Trained 1351 batches 	Training Loss: 0.065105
Trained 1401 batches 	Training Loss: 0.051685
Trained 1451 batches 	Training Loss: 0.124917
Trained 1501 batches 	Training Loss: 0.069921
Trained 1551 batches 	Training Loss: 0.070700
Trained 1601 batches 	Training Loss: 0.095308
Trained 1651 batches 	Training Loss: 0.116018
Trained 1701 batches 	Training Loss: 0.061141
Trained 1751 batches 	Training Loss: 0.058956
Trained 1801 batches 	Training Loss: 0.126480
Trained 1851 batches 	Training Loss: 0.079323
Trained 1901 batches 	Training Loss: 0.062548
Trained 1951 batches 	Training Loss: 0.058232
Trained 2001 batches 	Training Loss: 0.089068
Trained 2051 batches 	Training Loss: 0.069071
Trained 2101 batches 	Training Loss: 0.086304
Trained 2151 batches 	Training Loss: 0.057386
Trained 2201 batches 	Training Loss: 0.059452
Trained 2251 batches 	Training Loss: 0.065244
Trained 2301 batches 	Training Loss: 0.063244
Trained 2351 batches 	Training Loss: 0.095086
Trained 2401 batches 	Training Loss: 0.065676
Trained 2451 batches 	Training Loss: 0.072397
Trained 2501 batches 	Training Loss: 0.053126
Trained 2551 batches 	Training Loss: 0.066833
Trained 2601 batches 	Training Loss: 0.077643
Trained 2651 batches 	Training Loss: 0.041516
Trained 2701 batches 	Training Loss: 0.132199
Trained 2751 batches 	Training Loss: 0.067794
Trained 2801 batches 	Training Loss: 0.072156
Trained 2851 batches 	Training Loss: 0.036852
Trained 2901 batches 	Training Loss: 0.107612
Trained 2951 batches 	Training Loss: 0.038018
Trained 3001 batches 	Training Loss: 0.085295
Trained 3051 batches 	Training Loss: 0.043746
Trained 3101 batches 	Training Loss: 0.084361
Trained 3151 batches 	Training Loss: 0.104031
Trained 3201 batches 	Training Loss: 0.127845
Trained 3251 batches 	Training Loss: 0.071138
Trained 3301 batches 	Training Loss: 0.126089
Trained 3351 batches 	Training Loss: 0.068215
Trained 3401 batches 	Training Loss: 0.073276
Trained 3451 batches 	Training Loss: 0.068938
Trained 3501 batches 	Training Loss: 0.081995
Trained 3551 batches 	Training Loss: 0.101025
Trained 3601 batches 	Training Loss: 0.084771
Trained 3651 batches 	Training Loss: 0.041373
Trained 3701 batches 	Training Loss: 0.091986
Trained 3751 batches 	Training Loss: 0.093183
Trained 3801 batches 	Training Loss: 0.086875
Trained 3851 batches 	Training Loss: 0.121983
Trained 3901 batches 	Training Loss: 0.060302
Trained 3951 batches 	Training Loss: 0.092578
Trained 4001 batches 	Training Loss: 0.048236
Trained 4051 batches 	Training Loss: 0.050437
Trained 4101 batches 	Training Loss: 0.067200
Trained 4151 batches 	Training Loss: 0.137625
Trained 4201 batches 	Training Loss: 0.048860
Trained 4251 batches 	Training Loss: 0.057446
Trained 4301 batches 	Training Loss: 0.097878
Trained 4351 batches 	Training Loss: 0.040482
Trained 4401 batches 	Training Loss: 0.070134
Trained 4451 batches 	Training Loss: 0.054726
Trained 4501 batches 	Training Loss: 0.133699
Trained 4551 batches 	Training Loss: 0.057929
Trained 4601 batches 	Training Loss: 0.068994
Trained 4651 batches 	Training Loss: 0.095065
Trained 4701 batches 	Training Loss: 0.027332
Trained 4751 batches 	Training Loss: 0.037246
Trained 4801 batches 	Training Loss: 0.073311
Trained 4851 batches 	Training Loss: 0.091748
Trained 4901 batches 	Training Loss: 0.050580
Epoch: 29 	Training Loss: 0.075219
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1771634
The average AUROC is 0.802
The AUROC of Atelectasis is 0.7731036799489206
The AUROC of Cardiomegaly is 0.8644260467584177
The AUROC of Effusion is 0.8686110634434733
The AUROC of Infiltration is 0.6767734100528369
The AUROC of Mass is 0.8048417313773577
The AUROC of Nodule is 0.7509578101245142
The AUROC of Pneumonia is 0.7207001114349392
The AUROC of Pneumothorax is 0.8452694465111805
The AUROC of Consolidation is 0.7473712535187945
The AUROC of Edema is 0.8773662517380654
The AUROC of Emphysema is 0.8754228792000849
The AUROC of Fibrosis is 0.7857403688639965
The AUROC of Pleural_Thickening is 0.7690415817035003
The AUROC of Hernia is 0.8628038628038628
Epoch: 29 	Learning Rate for first group: 0.0000000100
Started epoch 30
Trained 1 batches 	Training Loss: 0.074047
Trained 51 batches 	Training Loss: 0.043886
Trained 101 batches 	Training Loss: 0.065633
Trained 151 batches 	Training Loss: 0.047280
Trained 201 batches 	Training Loss: 0.086747
Trained 251 batches 	Training Loss: 0.055750
Trained 301 batches 	Training Loss: 0.070991
Trained 351 batches 	Training Loss: 0.146216
Trained 401 batches 	Training Loss: 0.063519
Trained 451 batches 	Training Loss: 0.090987
Trained 501 batches 	Training Loss: 0.090829
Trained 551 batches 	Training Loss: 0.089326
Trained 601 batches 	Training Loss: 0.041170
Trained 651 batches 	Training Loss: 0.062248
Trained 701 batches 	Training Loss: 0.053446
Trained 751 batches 	Training Loss: 0.090431
Trained 801 batches 	Training Loss: 0.080027
Trained 851 batches 	Training Loss: 0.090465
Trained 901 batches 	Training Loss: 0.095657
Trained 951 batches 	Training Loss: 0.057645
Trained 1001 batches 	Training Loss: 0.130293
Trained 1051 batches 	Training Loss: 0.089750
Trained 1101 batches 	Training Loss: 0.063941
Trained 1151 batches 	Training Loss: 0.054742
Trained 1201 batches 	Training Loss: 0.054416
Trained 1251 batches 	Training Loss: 0.056490
Trained 1301 batches 	Training Loss: 0.081224
Trained 1351 batches 	Training Loss: 0.102250
Trained 1401 batches 	Training Loss: 0.061957
Trained 1451 batches 	Training Loss: 0.085174
Trained 1501 batches 	Training Loss: 0.127194
Trained 1551 batches 	Training Loss: 0.044819
Trained 1601 batches 	Training Loss: 0.098003
Trained 1651 batches 	Training Loss: 0.082270
Trained 1701 batches 	Training Loss: 0.122754
Trained 1751 batches 	Training Loss: 0.074085
Trained 1801 batches 	Training Loss: 0.055007
Trained 1851 batches 	Training Loss: 0.046843
Trained 1901 batches 	Training Loss: 0.076298
Trained 1951 batches 	Training Loss: 0.080429
Trained 2001 batches 	Training Loss: 0.082806
Trained 2051 batches 	Training Loss: 0.081776
Trained 2101 batches 	Training Loss: 0.068718
Trained 2151 batches 	Training Loss: 0.113823
Trained 2201 batches 	Training Loss: 0.039605
Trained 2251 batches 	Training Loss: 0.127209
Trained 2301 batches 	Training Loss: 0.123766
Trained 2351 batches 	Training Loss: 0.114913
Trained 2401 batches 	Training Loss: 0.072410
Trained 2451 batches 	Training Loss: 0.100766
Trained 2501 batches 	Training Loss: 0.075094
Trained 2551 batches 	Training Loss: 0.119057
Trained 2601 batches 	Training Loss: 0.081128
Trained 2651 batches 	Training Loss: 0.060258
Trained 2701 batches 	Training Loss: 0.070238
Trained 2751 batches 	Training Loss: 0.061541
Trained 2801 batches 	Training Loss: 0.058853
Trained 2851 batches 	Training Loss: 0.077077
Trained 2901 batches 	Training Loss: 0.066375
Trained 2951 batches 	Training Loss: 0.062701
Trained 3001 batches 	Training Loss: 0.072309
Trained 3051 batches 	Training Loss: 0.087695
Trained 3101 batches 	Training Loss: 0.100553
Trained 3151 batches 	Training Loss: 0.066883
Trained 3201 batches 	Training Loss: 0.087974
Trained 3251 batches 	Training Loss: 0.069928
Trained 3301 batches 	Training Loss: 0.067296
Trained 3351 batches 	Training Loss: 0.057806
Trained 3401 batches 	Training Loss: 0.057531
Trained 3451 batches 	Training Loss: 0.099996
Trained 3501 batches 	Training Loss: 0.093688
Trained 3551 batches 	Training Loss: 0.098284
Trained 3601 batches 	Training Loss: 0.093121
Trained 3651 batches 	Training Loss: 0.083037
Trained 3701 batches 	Training Loss: 0.055112
Trained 3751 batches 	Training Loss: 0.058267
Trained 3801 batches 	Training Loss: 0.049055
Trained 3851 batches 	Training Loss: 0.060408
Trained 3901 batches 	Training Loss: 0.083817
Trained 3951 batches 	Training Loss: 0.089434
Trained 4001 batches 	Training Loss: 0.041187
Trained 4051 batches 	Training Loss: 0.063621
Trained 4101 batches 	Training Loss: 0.056613
Trained 4151 batches 	Training Loss: 0.095843
Trained 4201 batches 	Training Loss: 0.039991
Trained 4251 batches 	Training Loss: 0.081035
Trained 4301 batches 	Training Loss: 0.094440
Trained 4351 batches 	Training Loss: 0.065566
Trained 4401 batches 	Training Loss: 0.067944
Trained 4451 batches 	Training Loss: 0.075661
Trained 4501 batches 	Training Loss: 0.097316
Trained 4551 batches 	Training Loss: 0.080527
Trained 4601 batches 	Training Loss: 0.107423
Trained 4651 batches 	Training Loss: 0.044135
Trained 4701 batches 	Training Loss: 0.064351
Trained 4751 batches 	Training Loss: 0.054265
Trained 4801 batches 	Training Loss: 0.047309
Trained 4851 batches 	Training Loss: 0.053769
Trained 4901 batches 	Training Loss: 0.066414
Epoch: 30 	Training Loss: 0.075167
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The total val loss is 0.1765570
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7705094796017251
The AUROC of Cardiomegaly is 0.8650030169462591
The AUROC of Effusion is 0.8654662540801961
The AUROC of Infiltration is 0.6748988353758058
The AUROC of Mass is 0.8005913678763064
The AUROC of Nodule is 0.7488666157280477
The AUROC of Pneumonia is 0.7189601310551079
The AUROC of Pneumothorax is 0.8433740332736088
The AUROC of Consolidation is 0.7429137489332162
The AUROC of Edema is 0.8759621432827914
The AUROC of Emphysema is 0.8735031769305963
The AUROC of Fibrosis is 0.7852449039232351
The AUROC of Pleural_Thickening is 0.7660153608096432
The AUROC of Hernia is 0.8642208366346298
Epoch: 30 	Learning Rate for first group: 0.0000000100
Training time lapse: 338.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The total val loss is 0.1750135
The average AUROC is 0.804
The AUROC of Atelectasis is 0.7850217908297474
The AUROC of Cardiomegaly is 0.8701567085105537
The AUROC of Effusion is 0.8534020570877543
The AUROC of Infiltration is 0.6862451503480442
The AUROC of Mass is 0.8169076723775842
The AUROC of Nodule is 0.7414960516654243
The AUROC of Pneumonia is 0.6986585385454613
The AUROC of Pneumothorax is 0.8396823382742634
The AUROC of Consolidation is 0.78269471266059
The AUROC of Edema is 0.8489193219171278
The AUROC of Emphysema is 0.8907863159224322
The AUROC of Fibrosis is 0.7686195602034936
The AUROC of Pleural_Thickening is 0.7675768563331424
The AUROC of Hernia is 0.911823382660068
