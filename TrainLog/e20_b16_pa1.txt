Started training, total epoch : 20
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.683528
Trained 51 batches 	Training Loss: 0.188223
Trained 101 batches 	Training Loss: 0.219586
Trained 151 batches 	Training Loss: 0.131479
Trained 201 batches 	Training Loss: 0.249823
Trained 251 batches 	Training Loss: 0.213811
Trained 301 batches 	Training Loss: 0.239368
Trained 351 batches 	Training Loss: 0.157055
Trained 401 batches 	Training Loss: 0.144824
Trained 451 batches 	Training Loss: 0.165537
Trained 501 batches 	Training Loss: 0.250536
Trained 551 batches 	Training Loss: 0.237868
Trained 601 batches 	Training Loss: 0.158300
Trained 651 batches 	Training Loss: 0.306324
Trained 701 batches 	Training Loss: 0.182408
Trained 751 batches 	Training Loss: 0.151180
Trained 801 batches 	Training Loss: 0.234981
Trained 851 batches 	Training Loss: 0.284134
Trained 901 batches 	Training Loss: 0.107899
Trained 951 batches 	Training Loss: 0.153757
Trained 1001 batches 	Training Loss: 0.153405
Trained 1051 batches 	Training Loss: 0.136971
Trained 1101 batches 	Training Loss: 0.149119
Trained 1151 batches 	Training Loss: 0.122541
Trained 1201 batches 	Training Loss: 0.138644
Trained 1251 batches 	Training Loss: 0.159919
Trained 1301 batches 	Training Loss: 0.195188
Trained 1351 batches 	Training Loss: 0.168898
Trained 1401 batches 	Training Loss: 0.174499
Trained 1451 batches 	Training Loss: 0.184041
Trained 1501 batches 	Training Loss: 0.225435
Trained 1551 batches 	Training Loss: 0.185043
Trained 1601 batches 	Training Loss: 0.170120
Trained 1651 batches 	Training Loss: 0.113725
Trained 1701 batches 	Training Loss: 0.170351
Trained 1751 batches 	Training Loss: 0.103559
Trained 1801 batches 	Training Loss: 0.142700
Trained 1851 batches 	Training Loss: 0.145656
Trained 1901 batches 	Training Loss: 0.273118
Trained 1951 batches 	Training Loss: 0.148124
Trained 2001 batches 	Training Loss: 0.202665
Trained 2051 batches 	Training Loss: 0.111621
Trained 2101 batches 	Training Loss: 0.178108
Trained 2151 batches 	Training Loss: 0.096927
Trained 2201 batches 	Training Loss: 0.260708
Trained 2251 batches 	Training Loss: 0.071035
Trained 2301 batches 	Training Loss: 0.154612
Trained 2351 batches 	Training Loss: 0.127185
Trained 2401 batches 	Training Loss: 0.106828
Trained 2451 batches 	Training Loss: 0.102921
Trained 2501 batches 	Training Loss: 0.154184
Trained 2551 batches 	Training Loss: 0.175414
Trained 2601 batches 	Training Loss: 0.229843
Trained 2651 batches 	Training Loss: 0.173935
Trained 2701 batches 	Training Loss: 0.134938
Trained 2751 batches 	Training Loss: 0.177023
Trained 2801 batches 	Training Loss: 0.244316
Trained 2851 batches 	Training Loss: 0.161468
Trained 2901 batches 	Training Loss: 0.240794
Trained 2951 batches 	Training Loss: 0.160589
Trained 3001 batches 	Training Loss: 0.201094
Trained 3051 batches 	Training Loss: 0.212831
Trained 3101 batches 	Training Loss: 0.170682
Trained 3151 batches 	Training Loss: 0.098150
Trained 3201 batches 	Training Loss: 0.129583
Trained 3251 batches 	Training Loss: 0.149593
Trained 3301 batches 	Training Loss: 0.151636
Trained 3351 batches 	Training Loss: 0.209577
Trained 3401 batches 	Training Loss: 0.167231
Trained 3451 batches 	Training Loss: 0.157703
Trained 3501 batches 	Training Loss: 0.220227
Trained 3551 batches 	Training Loss: 0.141961
Trained 3601 batches 	Training Loss: 0.132024
Trained 3651 batches 	Training Loss: 0.124029
Trained 3701 batches 	Training Loss: 0.242740
Trained 3751 batches 	Training Loss: 0.191613
Trained 3801 batches 	Training Loss: 0.132977
Trained 3851 batches 	Training Loss: 0.193968
Trained 3901 batches 	Training Loss: 0.213060
Trained 3951 batches 	Training Loss: 0.151774
Trained 4001 batches 	Training Loss: 0.327662
Trained 4051 batches 	Training Loss: 0.204058
Trained 4101 batches 	Training Loss: 0.202082
Trained 4151 batches 	Training Loss: 0.230367
Trained 4201 batches 	Training Loss: 0.159053
Trained 4251 batches 	Training Loss: 0.284478
Trained 4301 batches 	Training Loss: 0.211751
Trained 4351 batches 	Training Loss: 0.140597
Trained 4401 batches 	Training Loss: 0.178086
Trained 4451 batches 	Training Loss: 0.131066
Trained 4501 batches 	Training Loss: 0.172167
Trained 4551 batches 	Training Loss: 0.163707
Trained 4601 batches 	Training Loss: 0.126897
Trained 4651 batches 	Training Loss: 0.174445
Trained 4701 batches 	Training Loss: 0.213364
Trained 4751 batches 	Training Loss: 0.180543
Trained 4801 batches 	Training Loss: 0.206493
Trained 4851 batches 	Training Loss: 0.177810
Trained 4901 batches 	Training Loss: 0.120508
Epoch: 1 	Training Loss: 0.171622
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.735
The AUROC of Atelectasis is 0.744944496168569
The AUROC of Cardiomegaly is 0.8178109939516971
The AUROC of Effusion is 0.8445651327528337
The AUROC of Infiltration is 0.6685442197571163
The AUROC of Mass is 0.7144269693944032
The AUROC of Nodule is 0.6691618028846669
The AUROC of Pneumonia is 0.6718172586052537
The AUROC of Pneumothorax is 0.7828427576502981
The AUROC of Consolidation is 0.7290002802297885
The AUROC of Edema is 0.8342365755147984
The AUROC of Emphysema is 0.6836650119539742
The AUROC of Fibrosis is 0.6720006005635646
The AUROC of Pleural_Thickening is 0.7130283852377751
The AUROC of Hernia is 0.7386624869383489
Started epoch 2
Trained 1 batches 	Training Loss: 0.161174
Trained 51 batches 	Training Loss: 0.164535
Trained 101 batches 	Training Loss: 0.127194
Trained 151 batches 	Training Loss: 0.157627
Trained 201 batches 	Training Loss: 0.266799
Trained 251 batches 	Training Loss: 0.176936
Trained 301 batches 	Training Loss: 0.219477
Trained 351 batches 	Training Loss: 0.194235
Trained 401 batches 	Training Loss: 0.195127
Trained 451 batches 	Training Loss: 0.167681
Trained 501 batches 	Training Loss: 0.114305
Trained 551 batches 	Training Loss: 0.102912
Trained 601 batches 	Training Loss: 0.169766
Trained 651 batches 	Training Loss: 0.158134
Trained 701 batches 	Training Loss: 0.181191
Trained 751 batches 	Training Loss: 0.104868
Trained 801 batches 	Training Loss: 0.165387
Trained 851 batches 	Training Loss: 0.143852
Trained 901 batches 	Training Loss: 0.142893
Trained 951 batches 	Training Loss: 0.201721
Trained 1001 batches 	Training Loss: 0.186492
Trained 1051 batches 	Training Loss: 0.222753
Trained 1101 batches 	Training Loss: 0.245858
Trained 1151 batches 	Training Loss: 0.177274
Trained 1201 batches 	Training Loss: 0.140576
Trained 1251 batches 	Training Loss: 0.153566
Trained 1301 batches 	Training Loss: 0.234338
Trained 1351 batches 	Training Loss: 0.123759
Trained 1401 batches 	Training Loss: 0.113012
Trained 1451 batches 	Training Loss: 0.168739
Trained 1501 batches 	Training Loss: 0.141893
Trained 1551 batches 	Training Loss: 0.217917
Trained 1601 batches 	Training Loss: 0.148320
Trained 1651 batches 	Training Loss: 0.103414
Trained 1701 batches 	Training Loss: 0.190974
Trained 1751 batches 	Training Loss: 0.184775
Trained 1801 batches 	Training Loss: 0.180255
Trained 1851 batches 	Training Loss: 0.091928
Trained 1901 batches 	Training Loss: 0.165771
Trained 1951 batches 	Training Loss: 0.241167
Trained 2001 batches 	Training Loss: 0.092624
Trained 2051 batches 	Training Loss: 0.264215
Trained 2101 batches 	Training Loss: 0.126473
Trained 2151 batches 	Training Loss: 0.179884
Trained 2201 batches 	Training Loss: 0.261298
Trained 2251 batches 	Training Loss: 0.174543
Trained 2301 batches 	Training Loss: 0.195318
Trained 2351 batches 	Training Loss: 0.139147
Trained 2401 batches 	Training Loss: 0.112459
Trained 2451 batches 	Training Loss: 0.131649
Trained 2501 batches 	Training Loss: 0.085445
Trained 2551 batches 	Training Loss: 0.223833
Trained 2601 batches 	Training Loss: 0.089839
Trained 2651 batches 	Training Loss: 0.146473
Trained 2701 batches 	Training Loss: 0.211002
Trained 2751 batches 	Training Loss: 0.161243
Trained 2801 batches 	Training Loss: 0.176122
Trained 2851 batches 	Training Loss: 0.180665
Trained 2901 batches 	Training Loss: 0.196587
Trained 2951 batches 	Training Loss: 0.159986
Trained 3001 batches 	Training Loss: 0.187010
Trained 3051 batches 	Training Loss: 0.160108
Trained 3101 batches 	Training Loss: 0.122484
Trained 3151 batches 	Training Loss: 0.183326
Trained 3201 batches 	Training Loss: 0.195206
Trained 3251 batches 	Training Loss: 0.206530
Trained 3301 batches 	Training Loss: 0.145048
Trained 3351 batches 	Training Loss: 0.135217
Trained 3401 batches 	Training Loss: 0.143546
Trained 3451 batches 	Training Loss: 0.148014
Trained 3501 batches 	Training Loss: 0.176466
Trained 3551 batches 	Training Loss: 0.158131
Trained 3601 batches 	Training Loss: 0.085705
Trained 3651 batches 	Training Loss: 0.138493
Trained 3701 batches 	Training Loss: 0.186267
Trained 3751 batches 	Training Loss: 0.149243
Trained 3801 batches 	Training Loss: 0.155140
Trained 3851 batches 	Training Loss: 0.114684
Trained 3901 batches 	Training Loss: 0.197544
Trained 3951 batches 	Training Loss: 0.129686
Trained 4001 batches 	Training Loss: 0.168488
Trained 4051 batches 	Training Loss: 0.151236
Trained 4101 batches 	Training Loss: 0.192031
Trained 4151 batches 	Training Loss: 0.167766
Trained 4201 batches 	Training Loss: 0.137087
Trained 4251 batches 	Training Loss: 0.084701
Trained 4301 batches 	Training Loss: 0.139185
Trained 4351 batches 	Training Loss: 0.144838
Trained 4401 batches 	Training Loss: 0.199215
Trained 4451 batches 	Training Loss: 0.178567
Trained 4501 batches 	Training Loss: 0.186162
Trained 4551 batches 	Training Loss: 0.093142
Trained 4601 batches 	Training Loss: 0.134865
Trained 4651 batches 	Training Loss: 0.070262
Trained 4701 batches 	Training Loss: 0.144251
Trained 4751 batches 	Training Loss: 0.166278
Trained 4801 batches 	Training Loss: 0.201298
Trained 4851 batches 	Training Loss: 0.191413
Trained 4901 batches 	Training Loss: 0.131704
Epoch: 2 	Training Loss: 0.162595
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.766
The AUROC of Atelectasis is 0.7517891923457283
The AUROC of Cardiomegaly is 0.8677044124270842
The AUROC of Effusion is 0.8512822471570097
The AUROC of Infiltration is 0.6817872693573819
The AUROC of Mass is 0.7466417534919938
The AUROC of Nodule is 0.6829177983387161
The AUROC of Pneumonia is 0.7153611158732879
The AUROC of Pneumothorax is 0.8168722541933656
The AUROC of Consolidation is 0.7444455589947393
The AUROC of Edema is 0.881843011322254
The AUROC of Emphysema is 0.7438272150184314
The AUROC of Fibrosis is 0.7183952083607028
The AUROC of Pleural_Thickening is 0.7417505802347867
The AUROC of Hernia is 0.7767060525681214
Started epoch 3
Trained 1 batches 	Training Loss: 0.174256
Trained 51 batches 	Training Loss: 0.180229
Trained 101 batches 	Training Loss: 0.225167
Trained 151 batches 	Training Loss: 0.307235
Trained 201 batches 	Training Loss: 0.147134
Trained 251 batches 	Training Loss: 0.138864
Trained 301 batches 	Training Loss: 0.120308
Trained 351 batches 	Training Loss: 0.249790
Trained 401 batches 	Training Loss: 0.208054
Trained 451 batches 	Training Loss: 0.177051
Trained 501 batches 	Training Loss: 0.114424
Trained 551 batches 	Training Loss: 0.157626
Trained 601 batches 	Training Loss: 0.179703
Trained 651 batches 	Training Loss: 0.143587
Trained 701 batches 	Training Loss: 0.228433
Trained 751 batches 	Training Loss: 0.147718
Trained 801 batches 	Training Loss: 0.109436
Trained 851 batches 	Training Loss: 0.112863
Trained 901 batches 	Training Loss: 0.141329
Trained 951 batches 	Training Loss: 0.087436
Trained 1001 batches 	Training Loss: 0.148884
Trained 1051 batches 	Training Loss: 0.101565
Trained 1101 batches 	Training Loss: 0.185847
Trained 1151 batches 	Training Loss: 0.169554
Trained 1201 batches 	Training Loss: 0.149964
Trained 1251 batches 	Training Loss: 0.157345
Trained 1301 batches 	Training Loss: 0.120652
Trained 1351 batches 	Training Loss: 0.089739
Trained 1401 batches 	Training Loss: 0.109607
Trained 1451 batches 	Training Loss: 0.198704
Trained 1501 batches 	Training Loss: 0.155685
Trained 1551 batches 	Training Loss: 0.107146
Trained 1601 batches 	Training Loss: 0.201889
Trained 1651 batches 	Training Loss: 0.161867
Trained 1701 batches 	Training Loss: 0.195816
Trained 1751 batches 	Training Loss: 0.102706
Trained 1801 batches 	Training Loss: 0.187189
Trained 1851 batches 	Training Loss: 0.139285
Trained 1901 batches 	Training Loss: 0.092485
Trained 1951 batches 	Training Loss: 0.192265
Trained 2001 batches 	Training Loss: 0.180376
Trained 2051 batches 	Training Loss: 0.162576
Trained 2101 batches 	Training Loss: 0.172122
Trained 2151 batches 	Training Loss: 0.167911
Trained 2201 batches 	Training Loss: 0.170002
Trained 2251 batches 	Training Loss: 0.189083
Trained 2301 batches 	Training Loss: 0.088226
Trained 2351 batches 	Training Loss: 0.174294
Trained 2401 batches 	Training Loss: 0.158734
Trained 2451 batches 	Training Loss: 0.224385
Trained 2501 batches 	Training Loss: 0.154736
Trained 2551 batches 	Training Loss: 0.114212
Trained 2601 batches 	Training Loss: 0.163346
Trained 2651 batches 	Training Loss: 0.137057
Trained 2701 batches 	Training Loss: 0.163005
Trained 2751 batches 	Training Loss: 0.180677
Trained 2801 batches 	Training Loss: 0.194070
Trained 2851 batches 	Training Loss: 0.107910
Trained 2901 batches 	Training Loss: 0.195375
Trained 2951 batches 	Training Loss: 0.208954
Trained 3001 batches 	Training Loss: 0.170634
Trained 3051 batches 	Training Loss: 0.102209
Trained 3101 batches 	Training Loss: 0.169335
Trained 3151 batches 	Training Loss: 0.137059
Trained 3201 batches 	Training Loss: 0.137240
Trained 3251 batches 	Training Loss: 0.223748
Trained 3301 batches 	Training Loss: 0.141550
Trained 3351 batches 	Training Loss: 0.115995
Trained 3401 batches 	Training Loss: 0.224122
Trained 3451 batches 	Training Loss: 0.162657
Trained 3501 batches 	Training Loss: 0.156207
Trained 3551 batches 	Training Loss: 0.179780
Trained 3601 batches 	Training Loss: 0.147078
Trained 3651 batches 	Training Loss: 0.176853
Trained 3701 batches 	Training Loss: 0.137609
Trained 3751 batches 	Training Loss: 0.163828
Trained 3801 batches 	Training Loss: 0.217350
Trained 3851 batches 	Training Loss: 0.194024
Trained 3901 batches 	Training Loss: 0.140927
Trained 3951 batches 	Training Loss: 0.095077
Trained 4001 batches 	Training Loss: 0.111317
Trained 4051 batches 	Training Loss: 0.149438
Trained 4101 batches 	Training Loss: 0.173076
Trained 4151 batches 	Training Loss: 0.159033
Trained 4201 batches 	Training Loss: 0.199534
Trained 4251 batches 	Training Loss: 0.171730
Trained 4301 batches 	Training Loss: 0.186003
Trained 4351 batches 	Training Loss: 0.172731
Trained 4401 batches 	Training Loss: 0.142810
Trained 4451 batches 	Training Loss: 0.149970
Trained 4501 batches 	Training Loss: 0.139732
Trained 4551 batches 	Training Loss: 0.193296
Trained 4601 batches 	Training Loss: 0.149913
Trained 4651 batches 	Training Loss: 0.145257
Trained 4701 batches 	Training Loss: 0.148445
Trained 4751 batches 	Training Loss: 0.225177
Trained 4801 batches 	Training Loss: 0.156780
Trained 4851 batches 	Training Loss: 0.172255
Trained 4901 batches 	Training Loss: 0.155094
Epoch: 3 	Training Loss: 0.158394
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.776
The AUROC of Atelectasis is 0.7651590791221815
The AUROC of Cardiomegaly is 0.8799440137541711
The AUROC of Effusion is 0.8669007855011651
The AUROC of Infiltration is 0.6818366206534067
The AUROC of Mass is 0.7716566179703267
The AUROC of Nodule is 0.674067462992808
The AUROC of Pneumonia is 0.7208423814694174
The AUROC of Pneumothorax is 0.8151126286859607
The AUROC of Consolidation is 0.7471634694995349
The AUROC of Edema is 0.8753687181354698
The AUROC of Emphysema is 0.7904870950075964
The AUROC of Fibrosis is 0.7438333650595336
The AUROC of Pleural_Thickening is 0.7376113026527328
The AUROC of Hernia is 0.7970741901776385
Started epoch 4
Trained 1 batches 	Training Loss: 0.122233
Trained 51 batches 	Training Loss: 0.184839
Trained 101 batches 	Training Loss: 0.168789
Trained 151 batches 	Training Loss: 0.108400
Trained 201 batches 	Training Loss: 0.139855
Trained 251 batches 	Training Loss: 0.177190
Trained 301 batches 	Training Loss: 0.137407
Trained 351 batches 	Training Loss: 0.109727
Trained 401 batches 	Training Loss: 0.142462
Trained 451 batches 	Training Loss: 0.336340
Trained 501 batches 	Training Loss: 0.169614
Trained 551 batches 	Training Loss: 0.132257
Trained 601 batches 	Training Loss: 0.155260
Trained 651 batches 	Training Loss: 0.130857
Trained 701 batches 	Training Loss: 0.172411
Trained 751 batches 	Training Loss: 0.165762
Trained 801 batches 	Training Loss: 0.113226
Trained 851 batches 	Training Loss: 0.088958
Trained 901 batches 	Training Loss: 0.171718
Trained 951 batches 	Training Loss: 0.219126
Trained 1001 batches 	Training Loss: 0.174865
Trained 1051 batches 	Training Loss: 0.198667
Trained 1101 batches 	Training Loss: 0.186632
Trained 1151 batches 	Training Loss: 0.148735
Trained 1201 batches 	Training Loss: 0.129508
Trained 1251 batches 	Training Loss: 0.127514
Trained 1301 batches 	Training Loss: 0.193896
Trained 1351 batches 	Training Loss: 0.089248
Trained 1401 batches 	Training Loss: 0.112828
Trained 1451 batches 	Training Loss: 0.106912
Trained 1501 batches 	Training Loss: 0.116132
Trained 1551 batches 	Training Loss: 0.122348
Trained 1601 batches 	Training Loss: 0.134833
Trained 1651 batches 	Training Loss: 0.125121
Trained 1701 batches 	Training Loss: 0.129940
Trained 1751 batches 	Training Loss: 0.166345
Trained 1801 batches 	Training Loss: 0.119553
Trained 1851 batches 	Training Loss: 0.160633
Trained 1901 batches 	Training Loss: 0.102897
Trained 1951 batches 	Training Loss: 0.158888
Trained 2001 batches 	Training Loss: 0.109261
Trained 2051 batches 	Training Loss: 0.210029
Trained 2101 batches 	Training Loss: 0.105644
Trained 2151 batches 	Training Loss: 0.181356
Trained 2201 batches 	Training Loss: 0.153190
Trained 2251 batches 	Training Loss: 0.118486
Trained 2301 batches 	Training Loss: 0.122263
Trained 2351 batches 	Training Loss: 0.135860
Trained 2401 batches 	Training Loss: 0.091995
Trained 2451 batches 	Training Loss: 0.244160
Trained 2501 batches 	Training Loss: 0.154994
Trained 2551 batches 	Training Loss: 0.190609
Trained 2601 batches 	Training Loss: 0.176313
Trained 2651 batches 	Training Loss: 0.182650
Trained 2701 batches 	Training Loss: 0.154335
Trained 2751 batches 	Training Loss: 0.185783
Trained 2801 batches 	Training Loss: 0.131080
Trained 2851 batches 	Training Loss: 0.163441
Trained 2901 batches 	Training Loss: 0.186114
Trained 2951 batches 	Training Loss: 0.131233
Trained 3001 batches 	Training Loss: 0.231136
Trained 3051 batches 	Training Loss: 0.100640
Trained 3101 batches 	Training Loss: 0.151931
Trained 3151 batches 	Training Loss: 0.280502
Trained 3201 batches 	Training Loss: 0.131806
Trained 3251 batches 	Training Loss: 0.121612
Trained 3301 batches 	Training Loss: 0.135483
Trained 3351 batches 	Training Loss: 0.142849
Trained 3401 batches 	Training Loss: 0.172468
Trained 3451 batches 	Training Loss: 0.133854
Trained 3501 batches 	Training Loss: 0.074955
Trained 3551 batches 	Training Loss: 0.112580
Trained 3601 batches 	Training Loss: 0.147183
Trained 3651 batches 	Training Loss: 0.119656
Trained 3701 batches 	Training Loss: 0.153906
Trained 3751 batches 	Training Loss: 0.103139
Trained 3801 batches 	Training Loss: 0.204668
Trained 3851 batches 	Training Loss: 0.145737
Trained 3901 batches 	Training Loss: 0.116034
Trained 3951 batches 	Training Loss: 0.315852
Trained 4001 batches 	Training Loss: 0.211436
Trained 4051 batches 	Training Loss: 0.131537
Trained 4101 batches 	Training Loss: 0.169868
Trained 4151 batches 	Training Loss: 0.221144
Trained 4201 batches 	Training Loss: 0.128181
Trained 4251 batches 	Training Loss: 0.161542
Trained 4301 batches 	Training Loss: 0.261930
Trained 4351 batches 	Training Loss: 0.107277
Trained 4401 batches 	Training Loss: 0.165035
Trained 4451 batches 	Training Loss: 0.158154
Trained 4501 batches 	Training Loss: 0.242915
Trained 4551 batches 	Training Loss: 0.174307
Trained 4601 batches 	Training Loss: 0.208958
Trained 4651 batches 	Training Loss: 0.178054
Trained 4701 batches 	Training Loss: 0.133958
Trained 4751 batches 	Training Loss: 0.162273
Trained 4801 batches 	Training Loss: 0.167573
Trained 4851 batches 	Training Loss: 0.216913
Trained 4901 batches 	Training Loss: 0.156035
Epoch: 4 	Training Loss: 0.155640
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.784
The AUROC of Atelectasis is 0.7707094190538193
The AUROC of Cardiomegaly is 0.8783610868987677
The AUROC of Effusion is 0.8685339341300904
The AUROC of Infiltration is 0.6817079310630998
The AUROC of Mass is 0.7825177844048117
The AUROC of Nodule is 0.6970932169592784
The AUROC of Pneumonia is 0.7237270701182925
The AUROC of Pneumothorax is 0.8327955601445535
The AUROC of Consolidation is 0.7447199174914338
The AUROC of Edema is 0.8857575895517446
The AUROC of Emphysema is 0.7973569497933082
The AUROC of Fibrosis is 0.7543909507940041
The AUROC of Pleural_Thickening is 0.7542487615344209
The AUROC of Hernia is 0.8013986013986014
Started epoch 5
Trained 1 batches 	Training Loss: 0.179546
Trained 51 batches 	Training Loss: 0.135529
Trained 101 batches 	Training Loss: 0.151472
Trained 151 batches 	Training Loss: 0.190385
Trained 201 batches 	Training Loss: 0.172405
Trained 251 batches 	Training Loss: 0.188908
Trained 301 batches 	Training Loss: 0.140641
Trained 351 batches 	Training Loss: 0.166037
Trained 401 batches 	Training Loss: 0.128801
Trained 451 batches 	Training Loss: 0.101316
Trained 501 batches 	Training Loss: 0.138747
Trained 551 batches 	Training Loss: 0.108897
Trained 601 batches 	Training Loss: 0.121750
Trained 651 batches 	Training Loss: 0.093169
Trained 701 batches 	Training Loss: 0.119068
Trained 751 batches 	Training Loss: 0.142695
Trained 801 batches 	Training Loss: 0.206371
Trained 851 batches 	Training Loss: 0.160918
Trained 901 batches 	Training Loss: 0.188916
Trained 951 batches 	Training Loss: 0.193513
Trained 1001 batches 	Training Loss: 0.223739
Trained 1051 batches 	Training Loss: 0.134213
Trained 1101 batches 	Training Loss: 0.296721
Trained 1151 batches 	Training Loss: 0.181870
Trained 1201 batches 	Training Loss: 0.130561
Trained 1251 batches 	Training Loss: 0.116228
Trained 1301 batches 	Training Loss: 0.113223
Trained 1351 batches 	Training Loss: 0.163425
Trained 1401 batches 	Training Loss: 0.176993
Trained 1451 batches 	Training Loss: 0.205563
Trained 1501 batches 	Training Loss: 0.164880
Trained 1551 batches 	Training Loss: 0.081391
Trained 1601 batches 	Training Loss: 0.132267
Trained 1651 batches 	Training Loss: 0.101672
Trained 1701 batches 	Training Loss: 0.189109
Trained 1751 batches 	Training Loss: 0.268350
Trained 1801 batches 	Training Loss: 0.161188
Trained 1851 batches 	Training Loss: 0.234040
Trained 1901 batches 	Training Loss: 0.150612
Trained 1951 batches 	Training Loss: 0.195857
Trained 2001 batches 	Training Loss: 0.138822
Trained 2051 batches 	Training Loss: 0.166017
Trained 2101 batches 	Training Loss: 0.167166
Trained 2151 batches 	Training Loss: 0.143476
Trained 2201 batches 	Training Loss: 0.238474
Trained 2251 batches 	Training Loss: 0.130866
Trained 2301 batches 	Training Loss: 0.140492
Trained 2351 batches 	Training Loss: 0.116536
Trained 2401 batches 	Training Loss: 0.126155
Trained 2451 batches 	Training Loss: 0.136331
Trained 2501 batches 	Training Loss: 0.157415
Trained 2551 batches 	Training Loss: 0.123553
Trained 2601 batches 	Training Loss: 0.200067
Trained 2651 batches 	Training Loss: 0.177039
Trained 2701 batches 	Training Loss: 0.162448
Trained 2751 batches 	Training Loss: 0.134911
Trained 2801 batches 	Training Loss: 0.148581
Trained 2851 batches 	Training Loss: 0.149977
Trained 2901 batches 	Training Loss: 0.210724
Trained 2951 batches 	Training Loss: 0.129981
Trained 3001 batches 	Training Loss: 0.130928
Trained 3051 batches 	Training Loss: 0.128290
Trained 3101 batches 	Training Loss: 0.162758
Trained 3151 batches 	Training Loss: 0.200082
Trained 3201 batches 	Training Loss: 0.120342
Trained 3251 batches 	Training Loss: 0.230680
Trained 3301 batches 	Training Loss: 0.130868
Trained 3351 batches 	Training Loss: 0.147299
Trained 3401 batches 	Training Loss: 0.141671
Trained 3451 batches 	Training Loss: 0.166328
Trained 3501 batches 	Training Loss: 0.153720
Trained 3551 batches 	Training Loss: 0.135269
Trained 3601 batches 	Training Loss: 0.116683
Trained 3651 batches 	Training Loss: 0.236845
Trained 3701 batches 	Training Loss: 0.131591
Trained 3751 batches 	Training Loss: 0.147001
Trained 3801 batches 	Training Loss: 0.127491
Trained 3851 batches 	Training Loss: 0.184620
Trained 3901 batches 	Training Loss: 0.258106
Trained 3951 batches 	Training Loss: 0.192079
Trained 4001 batches 	Training Loss: 0.189784
Trained 4051 batches 	Training Loss: 0.094487
Trained 4101 batches 	Training Loss: 0.164930
Trained 4151 batches 	Training Loss: 0.163020
Trained 4201 batches 	Training Loss: 0.136141
Trained 4251 batches 	Training Loss: 0.097013
Trained 4301 batches 	Training Loss: 0.119117
Trained 4351 batches 	Training Loss: 0.247544
Trained 4401 batches 	Training Loss: 0.146183
Trained 4451 batches 	Training Loss: 0.094072
Trained 4501 batches 	Training Loss: 0.113732
Trained 4551 batches 	Training Loss: 0.144791
Trained 4601 batches 	Training Loss: 0.165864
Trained 4651 batches 	Training Loss: 0.131152
Trained 4701 batches 	Training Loss: 0.164614
Trained 4751 batches 	Training Loss: 0.115659
Trained 4801 batches 	Training Loss: 0.117103
Trained 4851 batches 	Training Loss: 0.114103
Trained 4901 batches 	Training Loss: 0.126242
Epoch: 5 	Training Loss: 0.153600
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.787
The AUROC of Atelectasis is 0.7691754120191655
The AUROC of Cardiomegaly is 0.8752170335344754
The AUROC of Effusion is 0.8727685179547077
The AUROC of Infiltration is 0.6769630141912639
The AUROC of Mass is 0.8003765116081974
The AUROC of Nodule is 0.7138072907860314
The AUROC of Pneumonia is 0.7214822989885136
The AUROC of Pneumothorax is 0.8387978206899693
The AUROC of Consolidation is 0.7451949945546257
The AUROC of Edema is 0.8900098490366153
The AUROC of Emphysema is 0.8148749690845494
The AUROC of Fibrosis is 0.7375381719810287
The AUROC of Pleural_Thickening is 0.7401168263141753
The AUROC of Hernia is 0.8163399818572232
Started epoch 6
Trained 1 batches 	Training Loss: 0.192738
Trained 51 batches 	Training Loss: 0.168627
Trained 101 batches 	Training Loss: 0.130270
Trained 151 batches 	Training Loss: 0.118433
Trained 201 batches 	Training Loss: 0.219989
Trained 251 batches 	Training Loss: 0.131374
Trained 301 batches 	Training Loss: 0.248346
Trained 351 batches 	Training Loss: 0.093073
Trained 401 batches 	Training Loss: 0.129743
Trained 451 batches 	Training Loss: 0.132811
Trained 501 batches 	Training Loss: 0.184884
Trained 551 batches 	Training Loss: 0.117566
Trained 601 batches 	Training Loss: 0.116684
Trained 651 batches 	Training Loss: 0.109892
Trained 701 batches 	Training Loss: 0.135875
Trained 751 batches 	Training Loss: 0.196058
Trained 801 batches 	Training Loss: 0.239820
Trained 851 batches 	Training Loss: 0.186487
Trained 901 batches 	Training Loss: 0.174539
Trained 951 batches 	Training Loss: 0.138431
Trained 1001 batches 	Training Loss: 0.113413
Trained 1051 batches 	Training Loss: 0.078390
Trained 1101 batches 	Training Loss: 0.074494
Trained 1151 batches 	Training Loss: 0.123784
Trained 1201 batches 	Training Loss: 0.154756
Trained 1251 batches 	Training Loss: 0.139655
Trained 1301 batches 	Training Loss: 0.196530
Trained 1351 batches 	Training Loss: 0.106155
Trained 1401 batches 	Training Loss: 0.171532
Trained 1451 batches 	Training Loss: 0.134658
Trained 1501 batches 	Training Loss: 0.207163
Trained 1551 batches 	Training Loss: 0.165866
Trained 1601 batches 	Training Loss: 0.120901
Trained 1651 batches 	Training Loss: 0.106405
Trained 1701 batches 	Training Loss: 0.179494
Trained 1751 batches 	Training Loss: 0.174316
Trained 1801 batches 	Training Loss: 0.159774
Trained 1851 batches 	Training Loss: 0.138980
Trained 1901 batches 	Training Loss: 0.170941
Trained 1951 batches 	Training Loss: 0.102699
Trained 2001 batches 	Training Loss: 0.210558
Trained 2051 batches 	Training Loss: 0.169426
Trained 2101 batches 	Training Loss: 0.137871
Trained 2151 batches 	Training Loss: 0.194328
Trained 2201 batches 	Training Loss: 0.085887
Trained 2251 batches 	Training Loss: 0.162417
Trained 2301 batches 	Training Loss: 0.155714
Trained 2351 batches 	Training Loss: 0.175578
Trained 2401 batches 	Training Loss: 0.193725
Trained 2451 batches 	Training Loss: 0.146500
Trained 2501 batches 	Training Loss: 0.170118
Trained 2551 batches 	Training Loss: 0.169633
Trained 2601 batches 	Training Loss: 0.206908
Trained 2651 batches 	Training Loss: 0.142521
Trained 2701 batches 	Training Loss: 0.098728
Trained 2751 batches 	Training Loss: 0.207941
Trained 2801 batches 	Training Loss: 0.122052
Trained 2851 batches 	Training Loss: 0.071390
Trained 2901 batches 	Training Loss: 0.162701
Trained 2951 batches 	Training Loss: 0.171999
Trained 3001 batches 	Training Loss: 0.173841
Trained 3051 batches 	Training Loss: 0.126928
Trained 3101 batches 	Training Loss: 0.115748
Trained 3151 batches 	Training Loss: 0.148939
Trained 3201 batches 	Training Loss: 0.140541
Trained 3251 batches 	Training Loss: 0.214748
Trained 3301 batches 	Training Loss: 0.174419
Trained 3351 batches 	Training Loss: 0.159223
Trained 3401 batches 	Training Loss: 0.128388
Trained 3451 batches 	Training Loss: 0.112522
Trained 3501 batches 	Training Loss: 0.108266
Trained 3551 batches 	Training Loss: 0.240080
Trained 3601 batches 	Training Loss: 0.196484
Trained 3651 batches 	Training Loss: 0.162352
Trained 3701 batches 	Training Loss: 0.127664
Trained 3751 batches 	Training Loss: 0.143004
Trained 3801 batches 	Training Loss: 0.148767
Trained 3851 batches 	Training Loss: 0.103915
Trained 3901 batches 	Training Loss: 0.197219
Trained 3951 batches 	Training Loss: 0.173866
Trained 4001 batches 	Training Loss: 0.184205
Trained 4051 batches 	Training Loss: 0.184416
Trained 4101 batches 	Training Loss: 0.158149
Trained 4151 batches 	Training Loss: 0.165493
Trained 4201 batches 	Training Loss: 0.159042
Trained 4251 batches 	Training Loss: 0.199713
Trained 4301 batches 	Training Loss: 0.129024
Trained 4351 batches 	Training Loss: 0.151746
Trained 4401 batches 	Training Loss: 0.103569
Trained 4451 batches 	Training Loss: 0.172158
Trained 4501 batches 	Training Loss: 0.215954
Trained 4551 batches 	Training Loss: 0.209154
Trained 4601 batches 	Training Loss: 0.169581
Trained 4651 batches 	Training Loss: 0.135629
Trained 4701 batches 	Training Loss: 0.151870
Trained 4751 batches 	Training Loss: 0.163493
Trained 4801 batches 	Training Loss: 0.100921
Trained 4851 batches 	Training Loss: 0.150484
Trained 4901 batches 	Training Loss: 0.130817
Epoch: 6 	Training Loss: 0.151765
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.793
The AUROC of Atelectasis is 0.769033246296799
The AUROC of Cardiomegaly is 0.8975956434778721
The AUROC of Effusion is 0.8755212963629285
The AUROC of Infiltration is 0.6845795734321835
The AUROC of Mass is 0.7913907027157302
The AUROC of Nodule is 0.7220109767729886
The AUROC of Pneumonia is 0.7280832714250338
The AUROC of Pneumothorax is 0.8495957527306225
The AUROC of Consolidation is 0.747946441081687
The AUROC of Edema is 0.8917561908230154
The AUROC of Emphysema is 0.8180541079273104
The AUROC of Fibrosis is 0.7661233890016435
The AUROC of Pleural_Thickening is 0.7525879834757047
The AUROC of Hernia is 0.8110785765958182
Started epoch 7
Trained 1 batches 	Training Loss: 0.123913
Trained 51 batches 	Training Loss: 0.134861
Trained 101 batches 	Training Loss: 0.126141
Trained 151 batches 	Training Loss: 0.138941
Trained 201 batches 	Training Loss: 0.126463
Trained 251 batches 	Training Loss: 0.174819
Trained 301 batches 	Training Loss: 0.246769
Trained 351 batches 	Training Loss: 0.169192
Trained 401 batches 	Training Loss: 0.129761
Trained 451 batches 	Training Loss: 0.159367
Trained 501 batches 	Training Loss: 0.119446
Trained 551 batches 	Training Loss: 0.178747
Trained 601 batches 	Training Loss: 0.182148
Trained 651 batches 	Training Loss: 0.201915
Trained 701 batches 	Training Loss: 0.174676
Trained 751 batches 	Training Loss: 0.089358
Trained 801 batches 	Training Loss: 0.118006
Trained 851 batches 	Training Loss: 0.226155
Trained 901 batches 	Training Loss: 0.160946
Trained 951 batches 	Training Loss: 0.162849
Trained 1001 batches 	Training Loss: 0.173183
Trained 1051 batches 	Training Loss: 0.123380
Trained 1101 batches 	Training Loss: 0.164720
Trained 1151 batches 	Training Loss: 0.140120
Trained 1201 batches 	Training Loss: 0.163044
Trained 1251 batches 	Training Loss: 0.121047
Trained 1301 batches 	Training Loss: 0.203621
Trained 1351 batches 	Training Loss: 0.054717
Trained 1401 batches 	Training Loss: 0.146404
Trained 1451 batches 	Training Loss: 0.160725
Trained 1501 batches 	Training Loss: 0.209564
Trained 1551 batches 	Training Loss: 0.165736
Trained 1601 batches 	Training Loss: 0.118592
Trained 1651 batches 	Training Loss: 0.142794
Trained 1701 batches 	Training Loss: 0.253483
Trained 1751 batches 	Training Loss: 0.169398
Trained 1801 batches 	Training Loss: 0.082544
Trained 1851 batches 	Training Loss: 0.276561
Trained 1901 batches 	Training Loss: 0.127721
Trained 1951 batches 	Training Loss: 0.157341
Trained 2001 batches 	Training Loss: 0.191780
Trained 2051 batches 	Training Loss: 0.114380
Trained 2101 batches 	Training Loss: 0.158994
Trained 2151 batches 	Training Loss: 0.172452
Trained 2201 batches 	Training Loss: 0.168161
Trained 2251 batches 	Training Loss: 0.105926
Trained 2301 batches 	Training Loss: 0.096678
Trained 2351 batches 	Training Loss: 0.157535
Trained 2401 batches 	Training Loss: 0.167802
Trained 2451 batches 	Training Loss: 0.155297
Trained 2501 batches 	Training Loss: 0.246253
Trained 2551 batches 	Training Loss: 0.125758
Trained 2601 batches 	Training Loss: 0.149846
Trained 2651 batches 	Training Loss: 0.128560
Trained 2701 batches 	Training Loss: 0.134405
Trained 2751 batches 	Training Loss: 0.124354
Trained 2801 batches 	Training Loss: 0.173459
Trained 2851 batches 	Training Loss: 0.152668
Trained 2901 batches 	Training Loss: 0.153485
Trained 2951 batches 	Training Loss: 0.101829
Trained 3001 batches 	Training Loss: 0.206588
Trained 3051 batches 	Training Loss: 0.193678
Trained 3101 batches 	Training Loss: 0.156224
Trained 3151 batches 	Training Loss: 0.168622
Trained 3201 batches 	Training Loss: 0.148736
Trained 3251 batches 	Training Loss: 0.163621
Trained 3301 batches 	Training Loss: 0.093205
Trained 3351 batches 	Training Loss: 0.150193
Trained 3401 batches 	Training Loss: 0.146304
Trained 3451 batches 	Training Loss: 0.123382
Trained 3501 batches 	Training Loss: 0.127475
Trained 3551 batches 	Training Loss: 0.124581
Trained 3601 batches 	Training Loss: 0.168506
Trained 3651 batches 	Training Loss: 0.091756
Trained 3701 batches 	Training Loss: 0.192040
Trained 3751 batches 	Training Loss: 0.105541
Trained 3801 batches 	Training Loss: 0.137302
Trained 3851 batches 	Training Loss: 0.193873
Trained 3901 batches 	Training Loss: 0.121007
Trained 3951 batches 	Training Loss: 0.104984
Trained 4001 batches 	Training Loss: 0.156964
Trained 4051 batches 	Training Loss: 0.151950
Trained 4101 batches 	Training Loss: 0.188043
Trained 4151 batches 	Training Loss: 0.108833
Trained 4201 batches 	Training Loss: 0.116983
Trained 4251 batches 	Training Loss: 0.238455
Trained 4301 batches 	Training Loss: 0.108408
Trained 4351 batches 	Training Loss: 0.193618
Trained 4401 batches 	Training Loss: 0.181646
Trained 4451 batches 	Training Loss: 0.162949
Trained 4501 batches 	Training Loss: 0.159712
Trained 4551 batches 	Training Loss: 0.127284
Trained 4601 batches 	Training Loss: 0.076227
Trained 4651 batches 	Training Loss: 0.050682
Trained 4701 batches 	Training Loss: 0.081358
Trained 4751 batches 	Training Loss: 0.160656
Trained 4801 batches 	Training Loss: 0.143158
Trained 4851 batches 	Training Loss: 0.158677
Trained 4901 batches 	Training Loss: 0.094365
Epoch: 7 	Training Loss: 0.150237
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.797
The AUROC of Atelectasis is 0.7731740282151859
The AUROC of Cardiomegaly is 0.889820679217508
The AUROC of Effusion is 0.8761527603252718
The AUROC of Infiltration is 0.6921693822789011
The AUROC of Mass is 0.793439203290446
The AUROC of Nodule is 0.7139206541831379
The AUROC of Pneumonia is 0.7170891908109035
The AUROC of Pneumothorax is 0.8480528972688715
The AUROC of Consolidation is 0.7586926961926963
The AUROC of Edema is 0.8961092167119116
The AUROC of Emphysema is 0.8390475391301276
The AUROC of Fibrosis is 0.7649201170026516
The AUROC of Pleural_Thickening is 0.7640348775456678
The AUROC of Hernia is 0.834510317268938
Started epoch 8
Trained 1 batches 	Training Loss: 0.137068
Trained 51 batches 	Training Loss: 0.163507
Trained 101 batches 	Training Loss: 0.136608
Trained 151 batches 	Training Loss: 0.154656
Trained 201 batches 	Training Loss: 0.124871
Trained 251 batches 	Training Loss: 0.092289
Trained 301 batches 	Training Loss: 0.194099
Trained 351 batches 	Training Loss: 0.111748
Trained 401 batches 	Training Loss: 0.138149
Trained 451 batches 	Training Loss: 0.210637
Trained 501 batches 	Training Loss: 0.107306
Trained 551 batches 	Training Loss: 0.106770
Trained 601 batches 	Training Loss: 0.160060
Trained 651 batches 	Training Loss: 0.122673
Trained 701 batches 	Training Loss: 0.143129
Trained 751 batches 	Training Loss: 0.191075
Trained 801 batches 	Training Loss: 0.183890
Trained 851 batches 	Training Loss: 0.221380
Trained 901 batches 	Training Loss: 0.155043
Trained 951 batches 	Training Loss: 0.171209
Trained 1001 batches 	Training Loss: 0.149085
Trained 1051 batches 	Training Loss: 0.146485
Trained 1101 batches 	Training Loss: 0.122666
Trained 1151 batches 	Training Loss: 0.176993
Trained 1201 batches 	Training Loss: 0.163142
Trained 1251 batches 	Training Loss: 0.139799
Trained 1301 batches 	Training Loss: 0.150533
Trained 1351 batches 	Training Loss: 0.141760
Trained 1401 batches 	Training Loss: 0.068944
Trained 1451 batches 	Training Loss: 0.142820
Trained 1501 batches 	Training Loss: 0.150248
Trained 1551 batches 	Training Loss: 0.091200
Trained 1601 batches 	Training Loss: 0.128453
Trained 1651 batches 	Training Loss: 0.186030
Trained 1701 batches 	Training Loss: 0.127801
Trained 1751 batches 	Training Loss: 0.193414
Trained 1801 batches 	Training Loss: 0.139829
Trained 1851 batches 	Training Loss: 0.089304
Trained 1901 batches 	Training Loss: 0.165754
Trained 1951 batches 	Training Loss: 0.168424
Trained 2001 batches 	Training Loss: 0.143102
Trained 2051 batches 	Training Loss: 0.151740
Trained 2101 batches 	Training Loss: 0.142290
Trained 2151 batches 	Training Loss: 0.121150
Trained 2201 batches 	Training Loss: 0.136678
Trained 2251 batches 	Training Loss: 0.240435
Trained 2301 batches 	Training Loss: 0.128814
Trained 2351 batches 	Training Loss: 0.151595
Trained 2401 batches 	Training Loss: 0.088059
Trained 2451 batches 	Training Loss: 0.078973
Trained 2501 batches 	Training Loss: 0.172092
Trained 2551 batches 	Training Loss: 0.199494
Trained 2601 batches 	Training Loss: 0.114351
Trained 2651 batches 	Training Loss: 0.155184
Trained 2701 batches 	Training Loss: 0.088075
Trained 2751 batches 	Training Loss: 0.101285
Trained 2801 batches 	Training Loss: 0.128619
Trained 2851 batches 	Training Loss: 0.153209
Trained 2901 batches 	Training Loss: 0.151832
Trained 2951 batches 	Training Loss: 0.218122
Trained 3001 batches 	Training Loss: 0.082154
Trained 3051 batches 	Training Loss: 0.111972
Trained 3101 batches 	Training Loss: 0.124093
Trained 3151 batches 	Training Loss: 0.168663
Trained 3201 batches 	Training Loss: 0.141101
Trained 3251 batches 	Training Loss: 0.176965
Trained 3301 batches 	Training Loss: 0.124096
Trained 3351 batches 	Training Loss: 0.141126
Trained 3401 batches 	Training Loss: 0.098101
Trained 3451 batches 	Training Loss: 0.179988
Trained 3501 batches 	Training Loss: 0.154137
Trained 3551 batches 	Training Loss: 0.113196
Trained 3601 batches 	Training Loss: 0.209869
Trained 3651 batches 	Training Loss: 0.194168
Trained 3701 batches 	Training Loss: 0.108459
Trained 3751 batches 	Training Loss: 0.117235
Trained 3801 batches 	Training Loss: 0.112286
Trained 3851 batches 	Training Loss: 0.118855
Trained 3901 batches 	Training Loss: 0.164652
Trained 3951 batches 	Training Loss: 0.076883
Trained 4001 batches 	Training Loss: 0.179648
Trained 4051 batches 	Training Loss: 0.151147
Trained 4101 batches 	Training Loss: 0.182074
Trained 4151 batches 	Training Loss: 0.177446
Trained 4201 batches 	Training Loss: 0.089170
Trained 4251 batches 	Training Loss: 0.186225
Trained 4301 batches 	Training Loss: 0.113420
Trained 4351 batches 	Training Loss: 0.143698
Trained 4401 batches 	Training Loss: 0.146589
Trained 4451 batches 	Training Loss: 0.143957
Trained 4501 batches 	Training Loss: 0.162085
Trained 4551 batches 	Training Loss: 0.160618
Trained 4601 batches 	Training Loss: 0.165474
Trained 4651 batches 	Training Loss: 0.163609
Trained 4701 batches 	Training Loss: 0.092365
Trained 4751 batches 	Training Loss: 0.095424
Trained 4801 batches 	Training Loss: 0.177357
Trained 4851 batches 	Training Loss: 0.127586
Trained 4901 batches 	Training Loss: 0.186050
Epoch: 8 	Training Loss: 0.148949
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.805
The AUROC of Atelectasis is 0.7690500987684473
The AUROC of Cardiomegaly is 0.8992997654800012
The AUROC of Effusion is 0.8802375140004467
The AUROC of Infiltration is 0.6899021273064262
The AUROC of Mass is 0.8088109443611651
The AUROC of Nodule is 0.7258387928260731
The AUROC of Pneumonia is 0.7476201021010724
The AUROC of Pneumothorax is 0.8528850266229362
The AUROC of Consolidation is 0.7566574477435133
The AUROC of Edema is 0.8988528769118719
The AUROC of Emphysema is 0.8434772815602587
The AUROC of Fibrosis is 0.7515398825147527
The AUROC of Pleural_Thickening is 0.7671804148349971
The AUROC of Hernia is 0.8743922743922745
Started epoch 9
Trained 1 batches 	Training Loss: 0.212493
Trained 51 batches 	Training Loss: 0.176043
Trained 101 batches 	Training Loss: 0.234891
Trained 151 batches 	Training Loss: 0.126819
Trained 201 batches 	Training Loss: 0.135239
Trained 251 batches 	Training Loss: 0.165084
Trained 301 batches 	Training Loss: 0.228818
Trained 351 batches 	Training Loss: 0.127046
Trained 401 batches 	Training Loss: 0.112970
Trained 451 batches 	Training Loss: 0.129165
Trained 501 batches 	Training Loss: 0.157041
Trained 551 batches 	Training Loss: 0.152120
Trained 601 batches 	Training Loss: 0.233118
Trained 651 batches 	Training Loss: 0.164571
Trained 701 batches 	Training Loss: 0.151798
Trained 751 batches 	Training Loss: 0.136948
Trained 801 batches 	Training Loss: 0.152752
Trained 851 batches 	Training Loss: 0.207469
Trained 901 batches 	Training Loss: 0.097690
Trained 951 batches 	Training Loss: 0.100065
Trained 1001 batches 	Training Loss: 0.110413
Trained 1051 batches 	Training Loss: 0.103714
Trained 1101 batches 	Training Loss: 0.078029
Trained 1151 batches 	Training Loss: 0.127417
Trained 1201 batches 	Training Loss: 0.173699
Trained 1251 batches 	Training Loss: 0.118619
Trained 1301 batches 	Training Loss: 0.130613
Trained 1351 batches 	Training Loss: 0.159854
Trained 1401 batches 	Training Loss: 0.176803
Trained 1451 batches 	Training Loss: 0.162387
Trained 1501 batches 	Training Loss: 0.091160
Trained 1551 batches 	Training Loss: 0.179944
Trained 1601 batches 	Training Loss: 0.205843
Trained 1651 batches 	Training Loss: 0.198531
Trained 1701 batches 	Training Loss: 0.179868
Trained 1751 batches 	Training Loss: 0.127301
Trained 1801 batches 	Training Loss: 0.184328
Trained 1851 batches 	Training Loss: 0.183916
Trained 1901 batches 	Training Loss: 0.207724
Trained 1951 batches 	Training Loss: 0.184167
Trained 2001 batches 	Training Loss: 0.237664
Trained 2051 batches 	Training Loss: 0.088716
Trained 2101 batches 	Training Loss: 0.125721
Trained 2151 batches 	Training Loss: 0.060966
Trained 2201 batches 	Training Loss: 0.127944
Trained 2251 batches 	Training Loss: 0.196250
Trained 2301 batches 	Training Loss: 0.111423
Trained 2351 batches 	Training Loss: 0.070818
Trained 2401 batches 	Training Loss: 0.141882
Trained 2451 batches 	Training Loss: 0.142765
Trained 2501 batches 	Training Loss: 0.105814
Trained 2551 batches 	Training Loss: 0.127342
Trained 2601 batches 	Training Loss: 0.110959
Trained 2651 batches 	Training Loss: 0.198861
Trained 2701 batches 	Training Loss: 0.130451
Trained 2751 batches 	Training Loss: 0.099978
Trained 2801 batches 	Training Loss: 0.180556
Trained 2851 batches 	Training Loss: 0.146713
Trained 2901 batches 	Training Loss: 0.085390
Trained 2951 batches 	Training Loss: 0.102724
Trained 3001 batches 	Training Loss: 0.272687
Trained 3051 batches 	Training Loss: 0.097076
Trained 3101 batches 	Training Loss: 0.083336
Trained 3151 batches 	Training Loss: 0.113716
Trained 3201 batches 	Training Loss: 0.165616
Trained 3251 batches 	Training Loss: 0.141797
Trained 3301 batches 	Training Loss: 0.138163
Trained 3351 batches 	Training Loss: 0.194756
Trained 3401 batches 	Training Loss: 0.174112
Trained 3451 batches 	Training Loss: 0.095902
Trained 3501 batches 	Training Loss: 0.123816
Trained 3551 batches 	Training Loss: 0.165796
Trained 3601 batches 	Training Loss: 0.166992
Trained 3651 batches 	Training Loss: 0.148727
Trained 3701 batches 	Training Loss: 0.179315
Trained 3751 batches 	Training Loss: 0.118354
Trained 3801 batches 	Training Loss: 0.121034
Trained 3851 batches 	Training Loss: 0.102945
Trained 3901 batches 	Training Loss: 0.130331
Trained 3951 batches 	Training Loss: 0.088601
Trained 4001 batches 	Training Loss: 0.161870
Trained 4051 batches 	Training Loss: 0.187968
Trained 4101 batches 	Training Loss: 0.146899
Trained 4151 batches 	Training Loss: 0.083123
Trained 4201 batches 	Training Loss: 0.142386
Trained 4251 batches 	Training Loss: 0.155771
Trained 4301 batches 	Training Loss: 0.119400
Trained 4351 batches 	Training Loss: 0.140020
Trained 4401 batches 	Training Loss: 0.147931
Trained 4451 batches 	Training Loss: 0.136876
Trained 4501 batches 	Training Loss: 0.096746
Trained 4551 batches 	Training Loss: 0.065313
Trained 4601 batches 	Training Loss: 0.151478
Trained 4651 batches 	Training Loss: 0.156016
Trained 4701 batches 	Training Loss: 0.139488
Trained 4751 batches 	Training Loss: 0.224842
Trained 4801 batches 	Training Loss: 0.069051
Trained 4851 batches 	Training Loss: 0.152442
Trained 4901 batches 	Training Loss: 0.200413
Epoch: 9 	Training Loss: 0.147686
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.7823038763363902
The AUROC of Cardiomegaly is 0.9089772718455175
The AUROC of Effusion is 0.8817075470479585
The AUROC of Infiltration is 0.695976818169244
The AUROC of Mass is 0.8070183511751214
The AUROC of Nodule is 0.722897551307817
The AUROC of Pneumonia is 0.7483481198925652
The AUROC of Pneumothorax is 0.8562899217508375
The AUROC of Consolidation is 0.7612244170901958
The AUROC of Edema is 0.9038593325829305
The AUROC of Emphysema is 0.8566770748683884
The AUROC of Fibrosis is 0.7717051447385002
The AUROC of Pleural_Thickening is 0.7593952914782517
The AUROC of Hernia is 0.8654816447919896
Started epoch 10
Trained 1 batches 	Training Loss: 0.195176
Trained 51 batches 	Training Loss: 0.131457
Trained 101 batches 	Training Loss: 0.158245
Trained 151 batches 	Training Loss: 0.151334
Trained 201 batches 	Training Loss: 0.143072
Trained 251 batches 	Training Loss: 0.147561
Trained 301 batches 	Training Loss: 0.131457
Trained 351 batches 	Training Loss: 0.141409
Trained 401 batches 	Training Loss: 0.176005
Trained 451 batches 	Training Loss: 0.113779
Trained 501 batches 	Training Loss: 0.146467
Trained 551 batches 	Training Loss: 0.178607
Trained 601 batches 	Training Loss: 0.169489
Trained 651 batches 	Training Loss: 0.169262
Trained 701 batches 	Training Loss: 0.135689
Trained 751 batches 	Training Loss: 0.074152
Trained 801 batches 	Training Loss: 0.134383
Trained 851 batches 	Training Loss: 0.138089
Trained 901 batches 	Training Loss: 0.095321
Trained 951 batches 	Training Loss: 0.178455
Trained 1001 batches 	Training Loss: 0.169084
Trained 1051 batches 	Training Loss: 0.142252
Trained 1101 batches 	Training Loss: 0.092857
Trained 1151 batches 	Training Loss: 0.143718
Trained 1201 batches 	Training Loss: 0.213371
Trained 1251 batches 	Training Loss: 0.179585
Trained 1301 batches 	Training Loss: 0.110855
Trained 1351 batches 	Training Loss: 0.156562
Trained 1401 batches 	Training Loss: 0.210795
Trained 1451 batches 	Training Loss: 0.162022
Trained 1501 batches 	Training Loss: 0.160820
Trained 1551 batches 	Training Loss: 0.190065
Trained 1601 batches 	Training Loss: 0.170440
Trained 1651 batches 	Training Loss: 0.218230
Trained 1701 batches 	Training Loss: 0.175548
Trained 1751 batches 	Training Loss: 0.131276
Trained 1801 batches 	Training Loss: 0.175217
Trained 1851 batches 	Training Loss: 0.190501
Trained 1901 batches 	Training Loss: 0.083500
Trained 1951 batches 	Training Loss: 0.126334
Trained 2001 batches 	Training Loss: 0.175038
Trained 2051 batches 	Training Loss: 0.166538
Trained 2101 batches 	Training Loss: 0.152369
Trained 2151 batches 	Training Loss: 0.144803
Trained 2201 batches 	Training Loss: 0.126785
Trained 2251 batches 	Training Loss: 0.155624
Trained 2301 batches 	Training Loss: 0.149103
Trained 2351 batches 	Training Loss: 0.139085
Trained 2401 batches 	Training Loss: 0.136452
Trained 2451 batches 	Training Loss: 0.151596
Trained 2501 batches 	Training Loss: 0.112611
Trained 2551 batches 	Training Loss: 0.102175
Trained 2601 batches 	Training Loss: 0.111510
Trained 2651 batches 	Training Loss: 0.166538
Trained 2701 batches 	Training Loss: 0.134920
Trained 2751 batches 	Training Loss: 0.170100
Trained 2801 batches 	Training Loss: 0.209590
Trained 2851 batches 	Training Loss: 0.143428
Trained 2901 batches 	Training Loss: 0.066748
Trained 2951 batches 	Training Loss: 0.176342
Trained 3001 batches 	Training Loss: 0.144469
Trained 3051 batches 	Training Loss: 0.155424
Trained 3101 batches 	Training Loss: 0.094086
Trained 3151 batches 	Training Loss: 0.165953
Trained 3201 batches 	Training Loss: 0.110755
Trained 3251 batches 	Training Loss: 0.134426
Trained 3301 batches 	Training Loss: 0.133694
Trained 3351 batches 	Training Loss: 0.134674
Trained 3401 batches 	Training Loss: 0.114960
Trained 3451 batches 	Training Loss: 0.128092
Trained 3501 batches 	Training Loss: 0.123021
Trained 3551 batches 	Training Loss: 0.134191
Trained 3601 batches 	Training Loss: 0.146985
Trained 3651 batches 	Training Loss: 0.060240
Trained 3701 batches 	Training Loss: 0.172610
Trained 3751 batches 	Training Loss: 0.219699
Trained 3801 batches 	Training Loss: 0.150817
Trained 3851 batches 	Training Loss: 0.124071
Trained 3901 batches 	Training Loss: 0.149568
Trained 3951 batches 	Training Loss: 0.193207
Trained 4001 batches 	Training Loss: 0.058441
Trained 4051 batches 	Training Loss: 0.171760
Trained 4101 batches 	Training Loss: 0.206088
Trained 4151 batches 	Training Loss: 0.122039
Trained 4201 batches 	Training Loss: 0.127969
Trained 4251 batches 	Training Loss: 0.116138
Trained 4301 batches 	Training Loss: 0.149800
Trained 4351 batches 	Training Loss: 0.129007
Trained 4401 batches 	Training Loss: 0.106132
Trained 4451 batches 	Training Loss: 0.115239
Trained 4501 batches 	Training Loss: 0.134966
Trained 4551 batches 	Training Loss: 0.119832
Trained 4601 batches 	Training Loss: 0.116408
Trained 4651 batches 	Training Loss: 0.149664
Trained 4701 batches 	Training Loss: 0.125059
Trained 4751 batches 	Training Loss: 0.174303
Trained 4801 batches 	Training Loss: 0.117017
Trained 4851 batches 	Training Loss: 0.156843
Trained 4901 batches 	Training Loss: 0.250714
Epoch: 10 	Training Loss: 0.146434
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.807
The AUROC of Atelectasis is 0.7779838532580104
The AUROC of Cardiomegaly is 0.8963663256328842
The AUROC of Effusion is 0.883682205086475
The AUROC of Infiltration is 0.7012900218539933
The AUROC of Mass is 0.808616282595914
The AUROC of Nodule is 0.7334590390008611
The AUROC of Pneumonia is 0.7272567360992057
The AUROC of Pneumothorax is 0.859947649361758
The AUROC of Consolidation is 0.7556521631829008
The AUROC of Edema is 0.9030668575779648
The AUROC of Emphysema is 0.8622039477558326
The AUROC of Fibrosis is 0.7588453093036414
The AUROC of Pleural_Thickening is 0.7588013636573118
The AUROC of Hernia is 0.8753384546487994
Started epoch 11
Trained 1 batches 	Training Loss: 0.111943
Trained 51 batches 	Training Loss: 0.093164
Trained 101 batches 	Training Loss: 0.102028
Trained 151 batches 	Training Loss: 0.178039
Trained 201 batches 	Training Loss: 0.187607
Trained 251 batches 	Training Loss: 0.222763
Trained 301 batches 	Training Loss: 0.133409
Trained 351 batches 	Training Loss: 0.125058
Trained 401 batches 	Training Loss: 0.158526
Trained 451 batches 	Training Loss: 0.110911
Trained 501 batches 	Training Loss: 0.184625
Trained 551 batches 	Training Loss: 0.153315
Trained 601 batches 	Training Loss: 0.126133
Trained 651 batches 	Training Loss: 0.109901
Trained 701 batches 	Training Loss: 0.143713
Trained 751 batches 	Training Loss: 0.133927
Trained 801 batches 	Training Loss: 0.171931
Trained 851 batches 	Training Loss: 0.261260
Trained 901 batches 	Training Loss: 0.161518
Trained 951 batches 	Training Loss: 0.086370
Trained 1001 batches 	Training Loss: 0.112774
Trained 1051 batches 	Training Loss: 0.119709
Trained 1101 batches 	Training Loss: 0.110619
Trained 1151 batches 	Training Loss: 0.122813
Trained 1201 batches 	Training Loss: 0.176615
Trained 1251 batches 	Training Loss: 0.168148
Trained 1301 batches 	Training Loss: 0.129779
Trained 1351 batches 	Training Loss: 0.135022
Trained 1401 batches 	Training Loss: 0.127524
Trained 1451 batches 	Training Loss: 0.179658
Trained 1501 batches 	Training Loss: 0.130533
Trained 1551 batches 	Training Loss: 0.103966
Trained 1601 batches 	Training Loss: 0.191073
Trained 1651 batches 	Training Loss: 0.162565
Trained 1701 batches 	Training Loss: 0.127855
Trained 1751 batches 	Training Loss: 0.099836
Trained 1801 batches 	Training Loss: 0.122356
Trained 1851 batches 	Training Loss: 0.138036
Trained 1901 batches 	Training Loss: 0.141629
Trained 1951 batches 	Training Loss: 0.136027
Trained 2001 batches 	Training Loss: 0.187845
Trained 2051 batches 	Training Loss: 0.150357
Trained 2101 batches 	Training Loss: 0.118761
Trained 2151 batches 	Training Loss: 0.141628
Trained 2201 batches 	Training Loss: 0.116450
Trained 2251 batches 	Training Loss: 0.101039
Trained 2301 batches 	Training Loss: 0.154120
Trained 2351 batches 	Training Loss: 0.126635
Trained 2401 batches 	Training Loss: 0.153499
Trained 2451 batches 	Training Loss: 0.152223
Trained 2501 batches 	Training Loss: 0.203149
Trained 2551 batches 	Training Loss: 0.126045
Trained 2601 batches 	Training Loss: 0.128621
Trained 2651 batches 	Training Loss: 0.206425
Trained 2701 batches 	Training Loss: 0.066167
Trained 2751 batches 	Training Loss: 0.117177
Trained 2801 batches 	Training Loss: 0.162920
Trained 2851 batches 	Training Loss: 0.173956
Trained 2901 batches 	Training Loss: 0.182355
Trained 2951 batches 	Training Loss: 0.175675
Trained 3001 batches 	Training Loss: 0.147705
Trained 3051 batches 	Training Loss: 0.123785
Trained 3101 batches 	Training Loss: 0.103724
Trained 3151 batches 	Training Loss: 0.136892
Trained 3201 batches 	Training Loss: 0.122930
Trained 3251 batches 	Training Loss: 0.163084
Trained 3301 batches 	Training Loss: 0.119419
Trained 3351 batches 	Training Loss: 0.149640
Trained 3401 batches 	Training Loss: 0.175577
Trained 3451 batches 	Training Loss: 0.111230
Trained 3501 batches 	Training Loss: 0.084166
Trained 3551 batches 	Training Loss: 0.156291
Trained 3601 batches 	Training Loss: 0.150498
Trained 3651 batches 	Training Loss: 0.112191
Trained 3701 batches 	Training Loss: 0.123579
Trained 3751 batches 	Training Loss: 0.147659
Trained 3801 batches 	Training Loss: 0.139211
Trained 3851 batches 	Training Loss: 0.192372
Trained 3901 batches 	Training Loss: 0.180341
Trained 3951 batches 	Training Loss: 0.114003
Trained 4001 batches 	Training Loss: 0.170729
Trained 4051 batches 	Training Loss: 0.154204
Trained 4101 batches 	Training Loss: 0.127929
Trained 4151 batches 	Training Loss: 0.127801
Trained 4201 batches 	Training Loss: 0.209765
Trained 4251 batches 	Training Loss: 0.134608
Trained 4301 batches 	Training Loss: 0.174751
Trained 4351 batches 	Training Loss: 0.161885
Trained 4401 batches 	Training Loss: 0.152384
Trained 4451 batches 	Training Loss: 0.190764
Trained 4501 batches 	Training Loss: 0.161438
Trained 4551 batches 	Training Loss: 0.168918
Trained 4601 batches 	Training Loss: 0.182479
Trained 4651 batches 	Training Loss: 0.164899
Trained 4701 batches 	Training Loss: 0.154243
Trained 4751 batches 	Training Loss: 0.145486
Trained 4801 batches 	Training Loss: 0.151568
Trained 4851 batches 	Training Loss: 0.114130
Trained 4901 batches 	Training Loss: 0.097690
Epoch: 11 	Training Loss: 0.145120
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.7786313338610319
The AUROC of Cardiomegaly is 0.9002959304665015
The AUROC of Effusion is 0.8808101899263212
The AUROC of Infiltration is 0.6955863509364022
The AUROC of Mass is 0.8073772174226252
The AUROC of Nodule is 0.7290812428166915
The AUROC of Pneumonia is 0.7420638322189839
The AUROC of Pneumothorax is 0.85696814864304
The AUROC of Consolidation is 0.7575184378463066
The AUROC of Edema is 0.9068293550950143
The AUROC of Emphysema is 0.8719154477146122
The AUROC of Fibrosis is 0.7606437826925089
The AUROC of Pleural_Thickening is 0.7675704551854132
The AUROC of Hernia is 0.8682030613065096
Started epoch 12
Trained 1 batches 	Training Loss: 0.125329
Trained 51 batches 	Training Loss: 0.106502
Trained 101 batches 	Training Loss: 0.166812
Trained 151 batches 	Training Loss: 0.132678
Trained 201 batches 	Training Loss: 0.101939
Trained 251 batches 	Training Loss: 0.290071
Trained 301 batches 	Training Loss: 0.127283
Trained 351 batches 	Training Loss: 0.134723
Trained 401 batches 	Training Loss: 0.122273
Trained 451 batches 	Training Loss: 0.117327
Trained 501 batches 	Training Loss: 0.165524
Trained 551 batches 	Training Loss: 0.138305
Trained 601 batches 	Training Loss: 0.166307
Trained 651 batches 	Training Loss: 0.091434
Trained 701 batches 	Training Loss: 0.107179
Trained 751 batches 	Training Loss: 0.107134
Trained 801 batches 	Training Loss: 0.113210
Trained 851 batches 	Training Loss: 0.139593
Trained 901 batches 	Training Loss: 0.148435
Trained 951 batches 	Training Loss: 0.166295
Trained 1001 batches 	Training Loss: 0.159504
Trained 1051 batches 	Training Loss: 0.138624
Trained 1101 batches 	Training Loss: 0.189944
Trained 1151 batches 	Training Loss: 0.150206
Trained 1201 batches 	Training Loss: 0.114113
Trained 1251 batches 	Training Loss: 0.107636
Trained 1301 batches 	Training Loss: 0.148171
Trained 1351 batches 	Training Loss: 0.097514
Trained 1401 batches 	Training Loss: 0.173127
Trained 1451 batches 	Training Loss: 0.125376
Trained 1501 batches 	Training Loss: 0.106180
Trained 1551 batches 	Training Loss: 0.138226
Trained 1601 batches 	Training Loss: 0.115622
Trained 1651 batches 	Training Loss: 0.164285
Trained 1701 batches 	Training Loss: 0.168277
Trained 1751 batches 	Training Loss: 0.129366
Trained 1801 batches 	Training Loss: 0.151914
Trained 1851 batches 	Training Loss: 0.129486
Trained 1901 batches 	Training Loss: 0.120383
Trained 1951 batches 	Training Loss: 0.170743
Trained 2001 batches 	Training Loss: 0.135061
Trained 2051 batches 	Training Loss: 0.126284
Trained 2101 batches 	Training Loss: 0.148659
Trained 2151 batches 	Training Loss: 0.136667
Trained 2201 batches 	Training Loss: 0.098243
Trained 2251 batches 	Training Loss: 0.205645
Trained 2301 batches 	Training Loss: 0.069007
Trained 2351 batches 	Training Loss: 0.121122
Trained 2401 batches 	Training Loss: 0.134903
Trained 2451 batches 	Training Loss: 0.135153
Trained 2501 batches 	Training Loss: 0.168742
Trained 2551 batches 	Training Loss: 0.224367
Trained 2601 batches 	Training Loss: 0.111526
Trained 2651 batches 	Training Loss: 0.170346
Trained 2701 batches 	Training Loss: 0.187038
Trained 2751 batches 	Training Loss: 0.122049
Trained 2801 batches 	Training Loss: 0.144649
Trained 2851 batches 	Training Loss: 0.161354
Trained 2901 batches 	Training Loss: 0.120877
Trained 2951 batches 	Training Loss: 0.119157
Trained 3001 batches 	Training Loss: 0.155430
Trained 3051 batches 	Training Loss: 0.111419
Trained 3101 batches 	Training Loss: 0.097337
Trained 3151 batches 	Training Loss: 0.173030
Trained 3201 batches 	Training Loss: 0.143685
Trained 3251 batches 	Training Loss: 0.152493
Trained 3301 batches 	Training Loss: 0.176734
Trained 3351 batches 	Training Loss: 0.146415
Trained 3401 batches 	Training Loss: 0.156081
Trained 3451 batches 	Training Loss: 0.160411
Trained 3501 batches 	Training Loss: 0.122142
Trained 3551 batches 	Training Loss: 0.097967
Trained 3601 batches 	Training Loss: 0.148801
Trained 3651 batches 	Training Loss: 0.165462
Trained 3701 batches 	Training Loss: 0.110090
Trained 3751 batches 	Training Loss: 0.158399
Trained 3801 batches 	Training Loss: 0.196046
Trained 3851 batches 	Training Loss: 0.181562
Trained 3901 batches 	Training Loss: 0.172890
Trained 3951 batches 	Training Loss: 0.120567
Trained 4001 batches 	Training Loss: 0.199948
Trained 4051 batches 	Training Loss: 0.195643
Trained 4101 batches 	Training Loss: 0.121151
Trained 4151 batches 	Training Loss: 0.134025
Trained 4201 batches 	Training Loss: 0.148460
Trained 4251 batches 	Training Loss: 0.131249
Trained 4301 batches 	Training Loss: 0.131194
Trained 4351 batches 	Training Loss: 0.216196
Trained 4401 batches 	Training Loss: 0.150103
Trained 4451 batches 	Training Loss: 0.156739
Trained 4501 batches 	Training Loss: 0.204627
Trained 4551 batches 	Training Loss: 0.142072
Trained 4601 batches 	Training Loss: 0.125347
Trained 4651 batches 	Training Loss: 0.086558
Trained 4701 batches 	Training Loss: 0.127205
Trained 4751 batches 	Training Loss: 0.118548
Trained 4801 batches 	Training Loss: 0.186790
Trained 4851 batches 	Training Loss: 0.122973
Trained 4901 batches 	Training Loss: 0.119544
Epoch: 12 	Training Loss: 0.143786
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.807
The AUROC of Atelectasis is 0.7830250324885405
The AUROC of Cardiomegaly is 0.8947039568367919
The AUROC of Effusion is 0.8845707052534657
The AUROC of Infiltration is 0.6950350493789592
The AUROC of Mass is 0.8048962730879448
The AUROC of Nodule is 0.7320023657207351
The AUROC of Pneumonia is 0.7361974255671752
The AUROC of Pneumothorax is 0.860821768046403
The AUROC of Consolidation is 0.7601002139936566
The AUROC of Edema is 0.9042172912666357
The AUROC of Emphysema is 0.8496064536739334
The AUROC of Fibrosis is 0.7582753101347781
The AUROC of Pleural_Thickening is 0.7617760697879064
The AUROC of Hernia is 0.8775982638051604
Started epoch 13
Trained 1 batches 	Training Loss: 0.121055
Trained 51 batches 	Training Loss: 0.129326
Trained 101 batches 	Training Loss: 0.158407
Trained 151 batches 	Training Loss: 0.090426
Trained 201 batches 	Training Loss: 0.132822
Trained 251 batches 	Training Loss: 0.096975
Trained 301 batches 	Training Loss: 0.090122
Trained 351 batches 	Training Loss: 0.159791
Trained 401 batches 	Training Loss: 0.144432
Trained 451 batches 	Training Loss: 0.144164
Trained 501 batches 	Training Loss: 0.221923
Trained 551 batches 	Training Loss: 0.162088
Trained 601 batches 	Training Loss: 0.103273
Trained 651 batches 	Training Loss: 0.144071
Trained 701 batches 	Training Loss: 0.149136
Trained 751 batches 	Training Loss: 0.062817
Trained 801 batches 	Training Loss: 0.135344
Trained 851 batches 	Training Loss: 0.114918
Trained 901 batches 	Training Loss: 0.191285
Trained 951 batches 	Training Loss: 0.114247
Trained 1001 batches 	Training Loss: 0.190945
Trained 1051 batches 	Training Loss: 0.145943
Trained 1101 batches 	Training Loss: 0.173137
Trained 1151 batches 	Training Loss: 0.115801
Trained 1201 batches 	Training Loss: 0.263719
Trained 1251 batches 	Training Loss: 0.085890
Trained 1301 batches 	Training Loss: 0.101934
Trained 1351 batches 	Training Loss: 0.118194
Trained 1401 batches 	Training Loss: 0.139425
Trained 1451 batches 	Training Loss: 0.136599
Trained 1501 batches 	Training Loss: 0.112328
Trained 1551 batches 	Training Loss: 0.219536
Trained 1601 batches 	Training Loss: 0.110768
Trained 1651 batches 	Training Loss: 0.201353
Trained 1701 batches 	Training Loss: 0.150188
Trained 1751 batches 	Training Loss: 0.109687
Trained 1801 batches 	Training Loss: 0.101508
Trained 1851 batches 	Training Loss: 0.118480
Trained 1901 batches 	Training Loss: 0.214539
Trained 1951 batches 	Training Loss: 0.118671
Trained 2001 batches 	Training Loss: 0.114931
Trained 2051 batches 	Training Loss: 0.132833
Trained 2101 batches 	Training Loss: 0.104503
Trained 2151 batches 	Training Loss: 0.118986
Trained 2201 batches 	Training Loss: 0.168887
Trained 2251 batches 	Training Loss: 0.173147
Trained 2301 batches 	Training Loss: 0.127787
Trained 2351 batches 	Training Loss: 0.096927
Trained 2401 batches 	Training Loss: 0.164087
Trained 2451 batches 	Training Loss: 0.154105
Trained 2501 batches 	Training Loss: 0.160246
Trained 2551 batches 	Training Loss: 0.138473
Trained 2601 batches 	Training Loss: 0.183963
Trained 2651 batches 	Training Loss: 0.127348
Trained 2701 batches 	Training Loss: 0.151838
Trained 2751 batches 	Training Loss: 0.104216
Trained 2801 batches 	Training Loss: 0.197431
Trained 2851 batches 	Training Loss: 0.109021
Trained 2901 batches 	Training Loss: 0.135397
Trained 2951 batches 	Training Loss: 0.248078
Trained 3001 batches 	Training Loss: 0.123532
Trained 3051 batches 	Training Loss: 0.169739
Trained 3101 batches 	Training Loss: 0.095352
Trained 3151 batches 	Training Loss: 0.199564
Trained 3201 batches 	Training Loss: 0.148558
Trained 3251 batches 	Training Loss: 0.111424
Trained 3301 batches 	Training Loss: 0.151343
Trained 3351 batches 	Training Loss: 0.133799
Trained 3401 batches 	Training Loss: 0.138349
Trained 3451 batches 	Training Loss: 0.082608
Trained 3501 batches 	Training Loss: 0.099958
Trained 3551 batches 	Training Loss: 0.154335
Trained 3601 batches 	Training Loss: 0.183665
Trained 3651 batches 	Training Loss: 0.144517
Trained 3701 batches 	Training Loss: 0.165830
Trained 3751 batches 	Training Loss: 0.172914
Trained 3801 batches 	Training Loss: 0.103782
Trained 3851 batches 	Training Loss: 0.173309
Trained 3901 batches 	Training Loss: 0.241697
Trained 3951 batches 	Training Loss: 0.173102
Trained 4001 batches 	Training Loss: 0.236959
Trained 4051 batches 	Training Loss: 0.108327
Trained 4101 batches 	Training Loss: 0.157282
Trained 4151 batches 	Training Loss: 0.133544
Trained 4201 batches 	Training Loss: 0.190035
Trained 4251 batches 	Training Loss: 0.112215
Trained 4301 batches 	Training Loss: 0.209122
Trained 4351 batches 	Training Loss: 0.101343
Trained 4401 batches 	Training Loss: 0.135858
Trained 4451 batches 	Training Loss: 0.139366
Trained 4501 batches 	Training Loss: 0.187429
Trained 4551 batches 	Training Loss: 0.117300
Trained 4601 batches 	Training Loss: 0.158451
Trained 4651 batches 	Training Loss: 0.176332
Trained 4701 batches 	Training Loss: 0.180430
Trained 4751 batches 	Training Loss: 0.248464
Trained 4801 batches 	Training Loss: 0.134796
Trained 4851 batches 	Training Loss: 0.126934
Trained 4901 batches 	Training Loss: 0.316185
Epoch: 13 	Training Loss: 0.142736
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.7835111614783956
The AUROC of Cardiomegaly is 0.898233026490377
The AUROC of Effusion is 0.880370663551971
The AUROC of Infiltration is 0.7015489778417108
The AUROC of Mass is 0.8232937962487221
The AUROC of Nodule is 0.735412827542811
The AUROC of Pneumonia is 0.7358283568584871
The AUROC of Pneumothorax is 0.8499112990575683
The AUROC of Consolidation is 0.7621838259645636
The AUROC of Edema is 0.9003732702112163
The AUROC of Emphysema is 0.8541033636010317
The AUROC of Fibrosis is 0.742523922001807
The AUROC of Pleural_Thickening is 0.770859196278004
The AUROC of Hernia is 0.8822051511706683
Started epoch 14
Trained 1 batches 	Training Loss: 0.135826
Trained 51 batches 	Training Loss: 0.164242
Trained 101 batches 	Training Loss: 0.108681
Trained 151 batches 	Training Loss: 0.176676
Trained 201 batches 	Training Loss: 0.141446
Trained 251 batches 	Training Loss: 0.159474
Trained 301 batches 	Training Loss: 0.108405
Trained 351 batches 	Training Loss: 0.116104
Trained 401 batches 	Training Loss: 0.134637
Trained 451 batches 	Training Loss: 0.191300
Trained 501 batches 	Training Loss: 0.085882
Trained 551 batches 	Training Loss: 0.128358
Trained 601 batches 	Training Loss: 0.186647
Trained 651 batches 	Training Loss: 0.130132
Trained 701 batches 	Training Loss: 0.141207
Trained 751 batches 	Training Loss: 0.118487
Trained 801 batches 	Training Loss: 0.121391
Trained 851 batches 	Training Loss: 0.141096
Trained 901 batches 	Training Loss: 0.187278
Trained 951 batches 	Training Loss: 0.058644
Trained 1001 batches 	Training Loss: 0.154244
Trained 1051 batches 	Training Loss: 0.174937
Trained 1101 batches 	Training Loss: 0.077120
Trained 1151 batches 	Training Loss: 0.164876
Trained 1201 batches 	Training Loss: 0.147715
Trained 1251 batches 	Training Loss: 0.137654
Trained 1301 batches 	Training Loss: 0.151567
Trained 1351 batches 	Training Loss: 0.248936
Trained 1401 batches 	Training Loss: 0.221367
Trained 1451 batches 	Training Loss: 0.166713
Trained 1501 batches 	Training Loss: 0.120357
Trained 1551 batches 	Training Loss: 0.184123
Trained 1601 batches 	Training Loss: 0.236705
Trained 1651 batches 	Training Loss: 0.118638
Trained 1701 batches 	Training Loss: 0.093411
Trained 1751 batches 	Training Loss: 0.117945
Trained 1801 batches 	Training Loss: 0.125009
Trained 1851 batches 	Training Loss: 0.239151
Trained 1901 batches 	Training Loss: 0.144163
Trained 1951 batches 	Training Loss: 0.142380
Trained 2001 batches 	Training Loss: 0.163703
Trained 2051 batches 	Training Loss: 0.194410
Trained 2101 batches 	Training Loss: 0.119270
Trained 2151 batches 	Training Loss: 0.187197
Trained 2201 batches 	Training Loss: 0.210473
Trained 2251 batches 	Training Loss: 0.115756
Trained 2301 batches 	Training Loss: 0.179156
Trained 2351 batches 	Training Loss: 0.163177
Trained 2401 batches 	Training Loss: 0.170785
Trained 2451 batches 	Training Loss: 0.127767
Trained 2501 batches 	Training Loss: 0.116395
Trained 2551 batches 	Training Loss: 0.071985
Trained 2601 batches 	Training Loss: 0.143582
Trained 2651 batches 	Training Loss: 0.148458
Trained 2701 batches 	Training Loss: 0.150031
Trained 2751 batches 	Training Loss: 0.141230
Trained 2801 batches 	Training Loss: 0.159951
Trained 2851 batches 	Training Loss: 0.183592
Trained 2901 batches 	Training Loss: 0.132434
Trained 2951 batches 	Training Loss: 0.127723
Trained 3001 batches 	Training Loss: 0.168637
Trained 3051 batches 	Training Loss: 0.088646
Trained 3101 batches 	Training Loss: 0.110774
Trained 3151 batches 	Training Loss: 0.152388
Trained 3201 batches 	Training Loss: 0.211540
Trained 3251 batches 	Training Loss: 0.134000
Trained 3301 batches 	Training Loss: 0.190621
Trained 3351 batches 	Training Loss: 0.165292
Trained 3401 batches 	Training Loss: 0.101496
Trained 3451 batches 	Training Loss: 0.189403
Trained 3501 batches 	Training Loss: 0.094261
Trained 3551 batches 	Training Loss: 0.150275
Trained 3601 batches 	Training Loss: 0.161454
Trained 3651 batches 	Training Loss: 0.135044
Trained 3701 batches 	Training Loss: 0.101026
Trained 3751 batches 	Training Loss: 0.142691
Trained 3801 batches 	Training Loss: 0.147968
Trained 3851 batches 	Training Loss: 0.109928
Trained 3901 batches 	Training Loss: 0.112313
Trained 3951 batches 	Training Loss: 0.134331
Trained 4001 batches 	Training Loss: 0.134071
Trained 4051 batches 	Training Loss: 0.111927
Trained 4101 batches 	Training Loss: 0.162349
Trained 4151 batches 	Training Loss: 0.120332
Trained 4201 batches 	Training Loss: 0.099965
Trained 4251 batches 	Training Loss: 0.107862
Trained 4301 batches 	Training Loss: 0.197814
Trained 4351 batches 	Training Loss: 0.137813
Trained 4401 batches 	Training Loss: 0.108541
Trained 4451 batches 	Training Loss: 0.229874
Trained 4501 batches 	Training Loss: 0.138650
Trained 4551 batches 	Training Loss: 0.125898
Trained 4601 batches 	Training Loss: 0.141056
Trained 4651 batches 	Training Loss: 0.185097
Trained 4701 batches 	Training Loss: 0.207094
Trained 4751 batches 	Training Loss: 0.150836
Trained 4801 batches 	Training Loss: 0.117652
Trained 4851 batches 	Training Loss: 0.105915
Trained 4901 batches 	Training Loss: 0.252265
Epoch: 14 	Training Loss: 0.141374
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.810
The AUROC of Atelectasis is 0.7766986146317653
The AUROC of Cardiomegaly is 0.8956764739897849
The AUROC of Effusion is 0.8812710467997793
The AUROC of Infiltration is 0.7001046501977924
The AUROC of Mass is 0.8142528662951761
The AUROC of Nodule is 0.7392065418313789
The AUROC of Pneumonia is 0.7434662933119986
The AUROC of Pneumothorax is 0.8528934095579379
The AUROC of Consolidation is 0.7573441898493127
The AUROC of Edema is 0.8979995696219295
The AUROC of Emphysema is 0.8500839869742901
The AUROC of Fibrosis is 0.7698688680181135
The AUROC of Pleural_Thickening is 0.7655441274014385
The AUROC of Hernia is 0.8961773858325582
Started epoch 15
Trained 1 batches 	Training Loss: 0.168229
Trained 51 batches 	Training Loss: 0.087445
Trained 101 batches 	Training Loss: 0.110893
Trained 151 batches 	Training Loss: 0.208718
Trained 201 batches 	Training Loss: 0.094361
Trained 251 batches 	Training Loss: 0.174576
Trained 301 batches 	Training Loss: 0.116785
Trained 351 batches 	Training Loss: 0.137710
Trained 401 batches 	Training Loss: 0.123712
Trained 451 batches 	Training Loss: 0.163819
Trained 501 batches 	Training Loss: 0.144150
Trained 551 batches 	Training Loss: 0.120073
Trained 601 batches 	Training Loss: 0.138850
Trained 651 batches 	Training Loss: 0.103734
Trained 701 batches 	Training Loss: 0.167190
Trained 751 batches 	Training Loss: 0.183248
Trained 801 batches 	Training Loss: 0.135114
Trained 851 batches 	Training Loss: 0.103528
Trained 901 batches 	Training Loss: 0.121584
Trained 951 batches 	Training Loss: 0.153838
Trained 1001 batches 	Training Loss: 0.174574
Trained 1051 batches 	Training Loss: 0.080521
Trained 1101 batches 	Training Loss: 0.161257
Trained 1151 batches 	Training Loss: 0.098161
Trained 1201 batches 	Training Loss: 0.106162
Trained 1251 batches 	Training Loss: 0.131340
Trained 1301 batches 	Training Loss: 0.105027
Trained 1351 batches 	Training Loss: 0.169606
Trained 1401 batches 	Training Loss: 0.169387
Trained 1451 batches 	Training Loss: 0.147042
Trained 1501 batches 	Training Loss: 0.124446
Trained 1551 batches 	Training Loss: 0.146692
Trained 1601 batches 	Training Loss: 0.150915
Trained 1651 batches 	Training Loss: 0.155888
Trained 1701 batches 	Training Loss: 0.188378
Trained 1751 batches 	Training Loss: 0.099350
Trained 1801 batches 	Training Loss: 0.069787
Trained 1851 batches 	Training Loss: 0.109988
Trained 1901 batches 	Training Loss: 0.137250
Trained 1951 batches 	Training Loss: 0.116174
Trained 2001 batches 	Training Loss: 0.145692
Trained 2051 batches 	Training Loss: 0.111229
Trained 2101 batches 	Training Loss: 0.110425
Trained 2151 batches 	Training Loss: 0.102378
Trained 2201 batches 	Training Loss: 0.226459
Trained 2251 batches 	Training Loss: 0.103051
Trained 2301 batches 	Training Loss: 0.167532
Trained 2351 batches 	Training Loss: 0.080250
Trained 2401 batches 	Training Loss: 0.158400
Trained 2451 batches 	Training Loss: 0.136329
Trained 2501 batches 	Training Loss: 0.081867
Trained 2551 batches 	Training Loss: 0.127900
Trained 2601 batches 	Training Loss: 0.105215
Trained 2651 batches 	Training Loss: 0.171296
Trained 2701 batches 	Training Loss: 0.157935
Trained 2751 batches 	Training Loss: 0.118995
Trained 2801 batches 	Training Loss: 0.119791
Trained 2851 batches 	Training Loss: 0.222542
Trained 2901 batches 	Training Loss: 0.099629
Trained 2951 batches 	Training Loss: 0.168615
Trained 3001 batches 	Training Loss: 0.104729
Trained 3051 batches 	Training Loss: 0.099166
Trained 3101 batches 	Training Loss: 0.101701
Trained 3151 batches 	Training Loss: 0.136054
Trained 3201 batches 	Training Loss: 0.142705
Trained 3251 batches 	Training Loss: 0.160738
Trained 3301 batches 	Training Loss: 0.145425
Trained 3351 batches 	Training Loss: 0.135846
Trained 3401 batches 	Training Loss: 0.137984
Trained 3451 batches 	Training Loss: 0.137804
Trained 3501 batches 	Training Loss: 0.157477
Trained 3551 batches 	Training Loss: 0.120992
Trained 3601 batches 	Training Loss: 0.108130
Trained 3651 batches 	Training Loss: 0.129333
Trained 3701 batches 	Training Loss: 0.159604
Trained 3751 batches 	Training Loss: 0.108498
Trained 3801 batches 	Training Loss: 0.162348
Trained 3851 batches 	Training Loss: 0.120517
Trained 3901 batches 	Training Loss: 0.122649
Trained 3951 batches 	Training Loss: 0.130805
Trained 4001 batches 	Training Loss: 0.103064
Trained 4051 batches 	Training Loss: 0.141292
Trained 4101 batches 	Training Loss: 0.165512
Trained 4151 batches 	Training Loss: 0.142515
Trained 4201 batches 	Training Loss: 0.157755
Trained 4251 batches 	Training Loss: 0.181708
Trained 4301 batches 	Training Loss: 0.145332
Trained 4351 batches 	Training Loss: 0.155308
Trained 4401 batches 	Training Loss: 0.132591
Trained 4451 batches 	Training Loss: 0.114491
Trained 4501 batches 	Training Loss: 0.120342
Trained 4551 batches 	Training Loss: 0.200209
Trained 4601 batches 	Training Loss: 0.142238
Trained 4651 batches 	Training Loss: 0.139906
Trained 4701 batches 	Training Loss: 0.116621
Trained 4751 batches 	Training Loss: 0.111061
Trained 4801 batches 	Training Loss: 0.110497
Trained 4851 batches 	Training Loss: 0.129072
Trained 4901 batches 	Training Loss: 0.132458
Epoch: 15 	Training Loss: 0.140104
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.810
The AUROC of Atelectasis is 0.7838964349071553
The AUROC of Cardiomegaly is 0.8961594070896942
The AUROC of Effusion is 0.8830253142835
The AUROC of Infiltration is 0.7048400785637223
The AUROC of Mass is 0.8116462856681268
The AUROC of Nodule is 0.7293569819795718
The AUROC of Pneumonia is 0.7115228013029316
The AUROC of Pneumothorax is 0.8517411514166845
The AUROC of Consolidation is 0.7613296027742749
The AUROC of Edema is 0.9014703204661325
The AUROC of Emphysema is 0.8703564257028111
The AUROC of Fibrosis is 0.7749892086234492
The AUROC of Pleural_Thickening is 0.7624076383726355
The AUROC of Hernia is 0.896053371915441
Started epoch 16
Trained 1 batches 	Training Loss: 0.152847
Trained 51 batches 	Training Loss: 0.113194
Trained 101 batches 	Training Loss: 0.106341
Trained 151 batches 	Training Loss: 0.118620
Trained 201 batches 	Training Loss: 0.158503
Trained 251 batches 	Training Loss: 0.125321
Trained 301 batches 	Training Loss: 0.103865
Trained 351 batches 	Training Loss: 0.109727
Trained 401 batches 	Training Loss: 0.112041
Trained 451 batches 	Training Loss: 0.167304
Trained 501 batches 	Training Loss: 0.124127
Trained 551 batches 	Training Loss: 0.094884
Trained 601 batches 	Training Loss: 0.100223
Trained 651 batches 	Training Loss: 0.087845
Trained 701 batches 	Training Loss: 0.139197
Trained 751 batches 	Training Loss: 0.088974
Trained 801 batches 	Training Loss: 0.154556
Trained 851 batches 	Training Loss: 0.117757
Trained 901 batches 	Training Loss: 0.116609
Trained 951 batches 	Training Loss: 0.171690
Trained 1001 batches 	Training Loss: 0.167685
Trained 1051 batches 	Training Loss: 0.208490
Trained 1101 batches 	Training Loss: 0.174917
Trained 1151 batches 	Training Loss: 0.141907
Trained 1201 batches 	Training Loss: 0.087568
Trained 1251 batches 	Training Loss: 0.192496
Trained 1301 batches 	Training Loss: 0.115634
Trained 1351 batches 	Training Loss: 0.147592
Trained 1401 batches 	Training Loss: 0.134389
Trained 1451 batches 	Training Loss: 0.144832
Trained 1501 batches 	Training Loss: 0.127872
Trained 1551 batches 	Training Loss: 0.135302
Trained 1601 batches 	Training Loss: 0.127858
Trained 1651 batches 	Training Loss: 0.130860
Trained 1701 batches 	Training Loss: 0.113213
Trained 1751 batches 	Training Loss: 0.083495
Trained 1801 batches 	Training Loss: 0.092287
Trained 1851 batches 	Training Loss: 0.187289
Trained 1901 batches 	Training Loss: 0.087818
Trained 1951 batches 	Training Loss: 0.108642
Trained 2001 batches 	Training Loss: 0.089153
Trained 2051 batches 	Training Loss: 0.167313
Trained 2101 batches 	Training Loss: 0.162906
Trained 2151 batches 	Training Loss: 0.152626
Trained 2201 batches 	Training Loss: 0.134205
Trained 2251 batches 	Training Loss: 0.186035
Trained 2301 batches 	Training Loss: 0.110448
Trained 2351 batches 	Training Loss: 0.142115
Trained 2401 batches 	Training Loss: 0.179508
Trained 2451 batches 	Training Loss: 0.117381
Trained 2501 batches 	Training Loss: 0.084955
Trained 2551 batches 	Training Loss: 0.240224
Trained 2601 batches 	Training Loss: 0.150119
Trained 2651 batches 	Training Loss: 0.168449
Trained 2701 batches 	Training Loss: 0.139382
Trained 2751 batches 	Training Loss: 0.144901
Trained 2801 batches 	Training Loss: 0.095258
Trained 2851 batches 	Training Loss: 0.163071
Trained 2901 batches 	Training Loss: 0.183216
Trained 2951 batches 	Training Loss: 0.143364
Trained 3001 batches 	Training Loss: 0.087618
Trained 3051 batches 	Training Loss: 0.067955
Trained 3101 batches 	Training Loss: 0.086275
Trained 3151 batches 	Training Loss: 0.110312
Trained 3201 batches 	Training Loss: 0.179229
Trained 3251 batches 	Training Loss: 0.113520
Trained 3301 batches 	Training Loss: 0.121465
Trained 3351 batches 	Training Loss: 0.155430
Trained 3401 batches 	Training Loss: 0.176473
Trained 3451 batches 	Training Loss: 0.203276
Trained 3501 batches 	Training Loss: 0.139165
Trained 3551 batches 	Training Loss: 0.153516
Trained 3601 batches 	Training Loss: 0.064179
Trained 3651 batches 	Training Loss: 0.158746
Trained 3701 batches 	Training Loss: 0.134145
Trained 3751 batches 	Training Loss: 0.158153
Trained 3801 batches 	Training Loss: 0.148523
Trained 3851 batches 	Training Loss: 0.082761
Trained 3901 batches 	Training Loss: 0.099272
Trained 3951 batches 	Training Loss: 0.212624
Trained 4001 batches 	Training Loss: 0.171628
Trained 4051 batches 	Training Loss: 0.110035
Trained 4101 batches 	Training Loss: 0.155462
Trained 4151 batches 	Training Loss: 0.091249
Trained 4201 batches 	Training Loss: 0.150397
Trained 4251 batches 	Training Loss: 0.083170
Trained 4301 batches 	Training Loss: 0.153693
Trained 4351 batches 	Training Loss: 0.136798
Trained 4401 batches 	Training Loss: 0.115136
Trained 4451 batches 	Training Loss: 0.056251
Trained 4501 batches 	Training Loss: 0.081355
Trained 4551 batches 	Training Loss: 0.124259
Trained 4601 batches 	Training Loss: 0.166676
Trained 4651 batches 	Training Loss: 0.179459
Trained 4701 batches 	Training Loss: 0.117235
Trained 4751 batches 	Training Loss: 0.173765
Trained 4801 batches 	Training Loss: 0.128589
Trained 4851 batches 	Training Loss: 0.066610
Trained 4901 batches 	Training Loss: 0.098674
Epoch: 16 	Training Loss: 0.138492
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.813
The AUROC of Atelectasis is 0.7820086556022845
The AUROC of Cardiomegaly is 0.9011021737901453
The AUROC of Effusion is 0.8829005050309349
The AUROC of Infiltration is 0.6977124401781516
The AUROC of Mass is 0.8177321142916165
The AUROC of Nodule is 0.7445716483066477
The AUROC of Pneumonia is 0.738479996475989
The AUROC of Pneumothorax is 0.8536894720459168
The AUROC of Consolidation is 0.7582304563605793
The AUROC of Edema is 0.9031595543931669
The AUROC of Emphysema is 0.8697601991543887
The AUROC of Fibrosis is 0.7655153183925272
The AUROC of Pleural_Thickening is 0.7705440755247448
The AUROC of Hernia is 0.901845281155626
Started epoch 17
Trained 1 batches 	Training Loss: 0.149813
Trained 51 batches 	Training Loss: 0.105215
Trained 101 batches 	Training Loss: 0.231095
Trained 151 batches 	Training Loss: 0.118534
Trained 201 batches 	Training Loss: 0.129007
Trained 251 batches 	Training Loss: 0.143123
Trained 301 batches 	Training Loss: 0.176856
Trained 351 batches 	Training Loss: 0.114476
Trained 401 batches 	Training Loss: 0.185236
Trained 451 batches 	Training Loss: 0.153324
Trained 501 batches 	Training Loss: 0.153682
Trained 551 batches 	Training Loss: 0.175305
Trained 601 batches 	Training Loss: 0.102947
Trained 651 batches 	Training Loss: 0.117984
Trained 701 batches 	Training Loss: 0.103677
Trained 751 batches 	Training Loss: 0.168385
Trained 801 batches 	Training Loss: 0.203192
Trained 851 batches 	Training Loss: 0.057737
Trained 901 batches 	Training Loss: 0.188030
Trained 951 batches 	Training Loss: 0.085726
Trained 1001 batches 	Training Loss: 0.112272
Trained 1051 batches 	Training Loss: 0.149576
Trained 1101 batches 	Training Loss: 0.173831
Trained 1151 batches 	Training Loss: 0.122945
Trained 1201 batches 	Training Loss: 0.103763
Trained 1251 batches 	Training Loss: 0.093158
Trained 1301 batches 	Training Loss: 0.136784
Trained 1351 batches 	Training Loss: 0.201085
Trained 1401 batches 	Training Loss: 0.118835
Trained 1451 batches 	Training Loss: 0.201993
Trained 1501 batches 	Training Loss: 0.194272
Trained 1551 batches 	Training Loss: 0.138276
Trained 1601 batches 	Training Loss: 0.162831
Trained 1651 batches 	Training Loss: 0.139648
Trained 1701 batches 	Training Loss: 0.150006
Trained 1751 batches 	Training Loss: 0.108356
Trained 1801 batches 	Training Loss: 0.093365
Trained 1851 batches 	Training Loss: 0.220064
Trained 1901 batches 	Training Loss: 0.163793
Trained 1951 batches 	Training Loss: 0.105602
Trained 2001 batches 	Training Loss: 0.133885
Trained 2051 batches 	Training Loss: 0.079793
Trained 2101 batches 	Training Loss: 0.111614
Trained 2151 batches 	Training Loss: 0.154239
Trained 2201 batches 	Training Loss: 0.150964
Trained 2251 batches 	Training Loss: 0.203960
Trained 2301 batches 	Training Loss: 0.136934
Trained 2351 batches 	Training Loss: 0.160097
Trained 2401 batches 	Training Loss: 0.187111
Trained 2451 batches 	Training Loss: 0.110131
Trained 2501 batches 	Training Loss: 0.237935
Trained 2551 batches 	Training Loss: 0.147045
Trained 2601 batches 	Training Loss: 0.184125
Trained 2651 batches 	Training Loss: 0.168284
Trained 2701 batches 	Training Loss: 0.131064
Trained 2751 batches 	Training Loss: 0.131357
Trained 2801 batches 	Training Loss: 0.071326
Trained 2851 batches 	Training Loss: 0.087298
Trained 2901 batches 	Training Loss: 0.104914
Trained 2951 batches 	Training Loss: 0.094088
Trained 3001 batches 	Training Loss: 0.150093
Trained 3051 batches 	Training Loss: 0.164429
Trained 3101 batches 	Training Loss: 0.100542
Trained 3151 batches 	Training Loss: 0.176510
Trained 3201 batches 	Training Loss: 0.170084
Trained 3251 batches 	Training Loss: 0.068650
Trained 3301 batches 	Training Loss: 0.115169
Trained 3351 batches 	Training Loss: 0.147811
Trained 3401 batches 	Training Loss: 0.182416
Trained 3451 batches 	Training Loss: 0.128884
Trained 3501 batches 	Training Loss: 0.112195
Trained 3551 batches 	Training Loss: 0.146622
Trained 3601 batches 	Training Loss: 0.134454
Trained 3651 batches 	Training Loss: 0.084384
Trained 3701 batches 	Training Loss: 0.178100
Trained 3751 batches 	Training Loss: 0.115960
Trained 3801 batches 	Training Loss: 0.103278
Trained 3851 batches 	Training Loss: 0.130909
Trained 3901 batches 	Training Loss: 0.077779
Trained 3951 batches 	Training Loss: 0.112754
Trained 4001 batches 	Training Loss: 0.157813
Trained 4051 batches 	Training Loss: 0.107175
Trained 4101 batches 	Training Loss: 0.143932
Trained 4151 batches 	Training Loss: 0.151108
Trained 4201 batches 	Training Loss: 0.088107
Trained 4251 batches 	Training Loss: 0.117519
Trained 4301 batches 	Training Loss: 0.093987
Trained 4351 batches 	Training Loss: 0.171479
Trained 4401 batches 	Training Loss: 0.156573
Trained 4451 batches 	Training Loss: 0.089172
Trained 4501 batches 	Training Loss: 0.111223
Trained 4551 batches 	Training Loss: 0.132820
Trained 4601 batches 	Training Loss: 0.103557
Trained 4651 batches 	Training Loss: 0.122497
Trained 4701 batches 	Training Loss: 0.174299
Trained 4751 batches 	Training Loss: 0.136650
Trained 4801 batches 	Training Loss: 0.267322
Trained 4851 batches 	Training Loss: 0.127827
Trained 4901 batches 	Training Loss: 0.148829
Epoch: 17 	Training Loss: 0.137276
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.811
The AUROC of Atelectasis is 0.7840448662920578
The AUROC of Cardiomegaly is 0.9048008427496664
The AUROC of Effusion is 0.8804974656193318
The AUROC of Infiltration is 0.6991470026833384
The AUROC of Mass is 0.8053170470737172
The AUROC of Nodule is 0.7400894778934242
The AUROC of Pneumonia is 0.7336841867154313
The AUROC of Pneumothorax is 0.8483916152935103
The AUROC of Consolidation is 0.7617652725234693
The AUROC of Edema is 0.9033348093094086
The AUROC of Emphysema is 0.8579133248536669
The AUROC of Fibrosis is 0.7646600515304987
The AUROC of Pleural_Thickening is 0.7616211635676977
The AUROC of Hernia is 0.9140905071939555
Started epoch 18
Trained 1 batches 	Training Loss: 0.080582
Trained 51 batches 	Training Loss: 0.134126
Trained 101 batches 	Training Loss: 0.090396
Trained 151 batches 	Training Loss: 0.147852
Trained 201 batches 	Training Loss: 0.099595
Trained 251 batches 	Training Loss: 0.125619
Trained 301 batches 	Training Loss: 0.114506
Trained 351 batches 	Training Loss: 0.133357
Trained 401 batches 	Training Loss: 0.107062
Trained 451 batches 	Training Loss: 0.200279
Trained 501 batches 	Training Loss: 0.196155
Trained 551 batches 	Training Loss: 0.074308
Trained 601 batches 	Training Loss: 0.140017
Trained 651 batches 	Training Loss: 0.079467
Trained 701 batches 	Training Loss: 0.109700
Trained 751 batches 	Training Loss: 0.188931
Trained 801 batches 	Training Loss: 0.163456
Trained 851 batches 	Training Loss: 0.113986
Trained 901 batches 	Training Loss: 0.107728
Trained 951 batches 	Training Loss: 0.172390
Trained 1001 batches 	Training Loss: 0.147492
Trained 1051 batches 	Training Loss: 0.122612
Trained 1101 batches 	Training Loss: 0.149385
Trained 1151 batches 	Training Loss: 0.077485
Trained 1201 batches 	Training Loss: 0.128486
Trained 1251 batches 	Training Loss: 0.197342
Trained 1301 batches 	Training Loss: 0.114290
Trained 1351 batches 	Training Loss: 0.145363
Trained 1401 batches 	Training Loss: 0.129936
Trained 1451 batches 	Training Loss: 0.116791
Trained 1501 batches 	Training Loss: 0.101379
Trained 1551 batches 	Training Loss: 0.228772
Trained 1601 batches 	Training Loss: 0.189793
Trained 1651 batches 	Training Loss: 0.107093
Trained 1701 batches 	Training Loss: 0.113842
Trained 1751 batches 	Training Loss: 0.096296
Trained 1801 batches 	Training Loss: 0.155701
Trained 1851 batches 	Training Loss: 0.166301
Trained 1901 batches 	Training Loss: 0.107925
Trained 1951 batches 	Training Loss: 0.169746
Trained 2001 batches 	Training Loss: 0.107379
Trained 2051 batches 	Training Loss: 0.132540
Trained 2101 batches 	Training Loss: 0.176014
Trained 2151 batches 	Training Loss: 0.163247
Trained 2201 batches 	Training Loss: 0.131584
Trained 2251 batches 	Training Loss: 0.145820
Trained 2301 batches 	Training Loss: 0.245169
Trained 2351 batches 	Training Loss: 0.168702
Trained 2401 batches 	Training Loss: 0.134535
Trained 2451 batches 	Training Loss: 0.143560
Trained 2501 batches 	Training Loss: 0.146076
Trained 2551 batches 	Training Loss: 0.118541
Trained 2601 batches 	Training Loss: 0.118027
Trained 2651 batches 	Training Loss: 0.158942
Trained 2701 batches 	Training Loss: 0.108826
Trained 2751 batches 	Training Loss: 0.109921
Trained 2801 batches 	Training Loss: 0.125230
Trained 2851 batches 	Training Loss: 0.083415
Trained 2901 batches 	Training Loss: 0.140946
Trained 2951 batches 	Training Loss: 0.229759
Trained 3001 batches 	Training Loss: 0.127213
Trained 3051 batches 	Training Loss: 0.219037
Trained 3101 batches 	Training Loss: 0.119091
Trained 3151 batches 	Training Loss: 0.140174
Trained 3201 batches 	Training Loss: 0.092285
Trained 3251 batches 	Training Loss: 0.068911
Trained 3301 batches 	Training Loss: 0.149762
Trained 3351 batches 	Training Loss: 0.083296
Trained 3401 batches 	Training Loss: 0.122593
Trained 3451 batches 	Training Loss: 0.131801
Trained 3501 batches 	Training Loss: 0.128098
Trained 3551 batches 	Training Loss: 0.132149
Trained 3601 batches 	Training Loss: 0.112779
Trained 3651 batches 	Training Loss: 0.137803
Trained 3701 batches 	Training Loss: 0.104972
Trained 3751 batches 	Training Loss: 0.075384
Trained 3801 batches 	Training Loss: 0.150267
Trained 3851 batches 	Training Loss: 0.128107
Trained 3901 batches 	Training Loss: 0.142279
Trained 3951 batches 	Training Loss: 0.143962
Trained 4001 batches 	Training Loss: 0.102905
Trained 4051 batches 	Training Loss: 0.121229
Trained 4101 batches 	Training Loss: 0.130569
Trained 4151 batches 	Training Loss: 0.146315
Trained 4201 batches 	Training Loss: 0.076557
Trained 4251 batches 	Training Loss: 0.125825
Trained 4301 batches 	Training Loss: 0.099140
Trained 4351 batches 	Training Loss: 0.128906
Trained 4401 batches 	Training Loss: 0.208798
Trained 4451 batches 	Training Loss: 0.115065
Trained 4501 batches 	Training Loss: 0.148429
Trained 4551 batches 	Training Loss: 0.141329
Trained 4601 batches 	Training Loss: 0.195340
Trained 4651 batches 	Training Loss: 0.148974
Trained 4701 batches 	Training Loss: 0.123235
Trained 4751 batches 	Training Loss: 0.186041
Trained 4801 batches 	Training Loss: 0.234735
Trained 4851 batches 	Training Loss: 0.064628
Trained 4901 batches 	Training Loss: 0.176866
Epoch: 18 	Training Loss: 0.135761
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.803
The AUROC of Atelectasis is 0.7809565860453066
The AUROC of Cardiomegaly is 0.8871547554869439
The AUROC of Effusion is 0.8782479098509633
The AUROC of Infiltration is 0.6719066087581953
The AUROC of Mass is 0.8132448792037671
The AUROC of Nodule is 0.7333029413842034
The AUROC of Pneumonia is 0.7332002304893613
The AUROC of Pneumothorax is 0.8479848056930569
The AUROC of Consolidation is 0.754191306189257
The AUROC of Edema is 0.900764334900351
The AUROC of Emphysema is 0.8374314707510393
The AUROC of Fibrosis is 0.7694018226031749
The AUROC of Pleural_Thickening is 0.755082890511463
The AUROC of Hernia is 0.875930965586138
Started epoch 19
Trained 1 batches 	Training Loss: 0.108199
Trained 51 batches 	Training Loss: 0.131194
Trained 101 batches 	Training Loss: 0.149297
Trained 151 batches 	Training Loss: 0.156827
Trained 201 batches 	Training Loss: 0.122062
Trained 251 batches 	Training Loss: 0.126121
Trained 301 batches 	Training Loss: 0.153307
Trained 351 batches 	Training Loss: 0.133191
Trained 401 batches 	Training Loss: 0.105187
Trained 451 batches 	Training Loss: 0.184974
Trained 501 batches 	Training Loss: 0.144206
Trained 551 batches 	Training Loss: 0.117226
Trained 601 batches 	Training Loss: 0.118931
Trained 651 batches 	Training Loss: 0.160425
Trained 701 batches 	Training Loss: 0.104184
Trained 751 batches 	Training Loss: 0.188689
Trained 801 batches 	Training Loss: 0.144240
Trained 851 batches 	Training Loss: 0.092711
Trained 901 batches 	Training Loss: 0.177065
Trained 951 batches 	Training Loss: 0.084379
Trained 1001 batches 	Training Loss: 0.140203
Trained 1051 batches 	Training Loss: 0.155776
Trained 1101 batches 	Training Loss: 0.172448
Trained 1151 batches 	Training Loss: 0.139994
Trained 1201 batches 	Training Loss: 0.088385
Trained 1251 batches 	Training Loss: 0.124199
Trained 1301 batches 	Training Loss: 0.106614
Trained 1351 batches 	Training Loss: 0.104030
Trained 1401 batches 	Training Loss: 0.135736
Trained 1451 batches 	Training Loss: 0.117641
Trained 1501 batches 	Training Loss: 0.119072
Trained 1551 batches 	Training Loss: 0.116495
Trained 1601 batches 	Training Loss: 0.119873
Trained 1651 batches 	Training Loss: 0.148303
Trained 1701 batches 	Training Loss: 0.133932
Trained 1751 batches 	Training Loss: 0.088697
Trained 1801 batches 	Training Loss: 0.136920
Trained 1851 batches 	Training Loss: 0.073706
Trained 1901 batches 	Training Loss: 0.119827
Trained 1951 batches 	Training Loss: 0.157980
Trained 2001 batches 	Training Loss: 0.157171
Trained 2051 batches 	Training Loss: 0.136583
Trained 2101 batches 	Training Loss: 0.160220
Trained 2151 batches 	Training Loss: 0.165864
Trained 2201 batches 	Training Loss: 0.124179
Trained 2251 batches 	Training Loss: 0.163363
Trained 2301 batches 	Training Loss: 0.162318
Trained 2351 batches 	Training Loss: 0.146288
Trained 2401 batches 	Training Loss: 0.118555
Trained 2451 batches 	Training Loss: 0.093031
Trained 2501 batches 	Training Loss: 0.128501
Trained 2551 batches 	Training Loss: 0.077658
Trained 2601 batches 	Training Loss: 0.087876
Trained 2651 batches 	Training Loss: 0.119248
Trained 2701 batches 	Training Loss: 0.127910
Trained 2751 batches 	Training Loss: 0.159397
Trained 2801 batches 	Training Loss: 0.094469
Trained 2851 batches 	Training Loss: 0.130738
Trained 2901 batches 	Training Loss: 0.105273
Trained 2951 batches 	Training Loss: 0.130783
Trained 3001 batches 	Training Loss: 0.129148
Trained 3051 batches 	Training Loss: 0.062209
Trained 3101 batches 	Training Loss: 0.165655
Trained 3151 batches 	Training Loss: 0.180555
Trained 3201 batches 	Training Loss: 0.116055
Trained 3251 batches 	Training Loss: 0.153164
Trained 3301 batches 	Training Loss: 0.133307
Trained 3351 batches 	Training Loss: 0.184351
Trained 3401 batches 	Training Loss: 0.058952
Trained 3451 batches 	Training Loss: 0.115374
Trained 3501 batches 	Training Loss: 0.079814
Trained 3551 batches 	Training Loss: 0.100113
Trained 3601 batches 	Training Loss: 0.122491
Trained 3651 batches 	Training Loss: 0.112907
Trained 3701 batches 	Training Loss: 0.092044
Trained 3751 batches 	Training Loss: 0.203351
Trained 3801 batches 	Training Loss: 0.149078
Trained 3851 batches 	Training Loss: 0.139873
Trained 3901 batches 	Training Loss: 0.108769
Trained 3951 batches 	Training Loss: 0.134908
Trained 4001 batches 	Training Loss: 0.106498
Trained 4051 batches 	Training Loss: 0.135138
Trained 4101 batches 	Training Loss: 0.120645
Trained 4151 batches 	Training Loss: 0.077773
Trained 4201 batches 	Training Loss: 0.139073
Trained 4251 batches 	Training Loss: 0.139344
Trained 4301 batches 	Training Loss: 0.130496
Trained 4351 batches 	Training Loss: 0.080298
Trained 4401 batches 	Training Loss: 0.111102
Trained 4451 batches 	Training Loss: 0.157795
Trained 4501 batches 	Training Loss: 0.095667
Trained 4551 batches 	Training Loss: 0.080675
Trained 4601 batches 	Training Loss: 0.116634
Trained 4651 batches 	Training Loss: 0.090606
Trained 4701 batches 	Training Loss: 0.084105
Trained 4751 batches 	Training Loss: 0.148710
Trained 4801 batches 	Training Loss: 0.142234
Trained 4851 batches 	Training Loss: 0.130468
Trained 4901 batches 	Training Loss: 0.102240
Epoch: 19 	Training Loss: 0.134256
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.808
The AUROC of Atelectasis is 0.7809277207861758
The AUROC of Cardiomegaly is 0.8952556164528322
The AUROC of Effusion is 0.880743836574425
The AUROC of Infiltration is 0.69287324683947
The AUROC of Mass is 0.8130705774700856
The AUROC of Nodule is 0.7442707252046997
The AUROC of Pneumonia is 0.7267055141246166
The AUROC of Pneumothorax is 0.8453128637877453
The AUROC of Consolidation is 0.7516121173908059
The AUROC of Edema is 0.8973817287956034
The AUROC of Emphysema is 0.8545188819795309
The AUROC of Fibrosis is 0.754497658070207
The AUROC of Pleural_Thickening is 0.7585694869008935
The AUROC of Hernia is 0.9149333425195494
Started epoch 20
Trained 1 batches 	Training Loss: 0.141798
Trained 51 batches 	Training Loss: 0.172387
Trained 101 batches 	Training Loss: 0.157209
Trained 151 batches 	Training Loss: 0.127675
Trained 201 batches 	Training Loss: 0.172855
Trained 251 batches 	Training Loss: 0.128343
Trained 301 batches 	Training Loss: 0.203947
Trained 351 batches 	Training Loss: 0.158687
Trained 401 batches 	Training Loss: 0.069846
Trained 451 batches 	Training Loss: 0.089174
Trained 501 batches 	Training Loss: 0.119412
Trained 551 batches 	Training Loss: 0.101362
Trained 601 batches 	Training Loss: 0.096123
Trained 651 batches 	Training Loss: 0.147294
Trained 701 batches 	Training Loss: 0.081590
Trained 751 batches 	Training Loss: 0.147401
Trained 801 batches 	Training Loss: 0.146793
Trained 851 batches 	Training Loss: 0.202873
Trained 901 batches 	Training Loss: 0.183684
Trained 951 batches 	Training Loss: 0.135873
Trained 1001 batches 	Training Loss: 0.192611
Trained 1051 batches 	Training Loss: 0.154065
Trained 1101 batches 	Training Loss: 0.113465
Trained 1151 batches 	Training Loss: 0.168669
Trained 1201 batches 	Training Loss: 0.170755
Trained 1251 batches 	Training Loss: 0.192105
Trained 1301 batches 	Training Loss: 0.109341
Trained 1351 batches 	Training Loss: 0.189450
Trained 1401 batches 	Training Loss: 0.115680
Trained 1451 batches 	Training Loss: 0.118348
Trained 1501 batches 	Training Loss: 0.122598
Trained 1551 batches 	Training Loss: 0.115764
Trained 1601 batches 	Training Loss: 0.166819
Trained 1651 batches 	Training Loss: 0.117856
Trained 1701 batches 	Training Loss: 0.158089
Trained 1751 batches 	Training Loss: 0.113267
Trained 1801 batches 	Training Loss: 0.223993
Trained 1851 batches 	Training Loss: 0.165137
Trained 1901 batches 	Training Loss: 0.122935
Trained 1951 batches 	Training Loss: 0.140200
Trained 2001 batches 	Training Loss: 0.077962
Trained 2051 batches 	Training Loss: 0.209129
Trained 2101 batches 	Training Loss: 0.088204
Trained 2151 batches 	Training Loss: 0.129994
Trained 2201 batches 	Training Loss: 0.147281
Trained 2251 batches 	Training Loss: 0.119786
Trained 2301 batches 	Training Loss: 0.174734
Trained 2351 batches 	Training Loss: 0.103032
Trained 2401 batches 	Training Loss: 0.168240
Trained 2451 batches 	Training Loss: 0.167475
Trained 2501 batches 	Training Loss: 0.165282
Trained 2551 batches 	Training Loss: 0.113440
Trained 2601 batches 	Training Loss: 0.177308
Trained 2651 batches 	Training Loss: 0.096825
Trained 2701 batches 	Training Loss: 0.121076
Trained 2751 batches 	Training Loss: 0.109016
Trained 2801 batches 	Training Loss: 0.065960
Trained 2851 batches 	Training Loss: 0.099221
Trained 2901 batches 	Training Loss: 0.148400
Trained 2951 batches 	Training Loss: 0.123238
Trained 3001 batches 	Training Loss: 0.138349
Trained 3051 batches 	Training Loss: 0.123326
Trained 3101 batches 	Training Loss: 0.142230
Trained 3151 batches 	Training Loss: 0.143027
Trained 3201 batches 	Training Loss: 0.093038
Trained 3251 batches 	Training Loss: 0.140406
Trained 3301 batches 	Training Loss: 0.144319
Trained 3351 batches 	Training Loss: 0.143833
Trained 3401 batches 	Training Loss: 0.121547
Trained 3451 batches 	Training Loss: 0.152048
Trained 3501 batches 	Training Loss: 0.197519
Trained 3551 batches 	Training Loss: 0.109751
Trained 3601 batches 	Training Loss: 0.098039
Trained 3651 batches 	Training Loss: 0.115428
Trained 3701 batches 	Training Loss: 0.135646
Trained 3751 batches 	Training Loss: 0.072992
Trained 3801 batches 	Training Loss: 0.121207
Trained 3851 batches 	Training Loss: 0.085380
Trained 3901 batches 	Training Loss: 0.122488
Trained 3951 batches 	Training Loss: 0.156497
Trained 4001 batches 	Training Loss: 0.134594
Trained 4051 batches 	Training Loss: 0.171296
Trained 4101 batches 	Training Loss: 0.121007
Trained 4151 batches 	Training Loss: 0.142540
Trained 4201 batches 	Training Loss: 0.097403
Trained 4251 batches 	Training Loss: 0.108311
Trained 4301 batches 	Training Loss: 0.143487
Trained 4351 batches 	Training Loss: 0.095656
Trained 4401 batches 	Training Loss: 0.071967
Trained 4451 batches 	Training Loss: 0.143735
Trained 4501 batches 	Training Loss: 0.109443
Trained 4551 batches 	Training Loss: 0.165542
Trained 4601 batches 	Training Loss: 0.084558
Trained 4651 batches 	Training Loss: 0.117016
Trained 4701 batches 	Training Loss: 0.170901
Trained 4751 batches 	Training Loss: 0.097072
Trained 4801 batches 	Training Loss: 0.142873
Trained 4851 batches 	Training Loss: 0.095910
Trained 4901 batches 	Training Loss: 0.174656
Epoch: 20 	Training Loss: 0.131994
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.804
The AUROC of Atelectasis is 0.7810069706143883
The AUROC of Cardiomegaly is 0.8886604573860498
The AUROC of Effusion is 0.8766658732311462
The AUROC of Infiltration is 0.6828761238208525
The AUROC of Mass is 0.7995780838661151
The AUROC of Nodule is 0.72168957834381
The AUROC of Pneumonia is 0.7528513534106711
The AUROC of Pneumothorax is 0.8336496705032039
The AUROC of Consolidation is 0.7515271329308213
The AUROC of Edema is 0.8982693835661788
The AUROC of Emphysema is 0.831039333286224
The AUROC of Fibrosis is 0.7676065665191176
The AUROC of Pleural_Thickening is 0.757690237264694
The AUROC of Hernia is 0.9129583060617543
Training time lapse: 235.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.805
The AUROC of Atelectasis is 0.7897498442140304
The AUROC of Cardiomegaly is 0.8923593230096678
The AUROC of Effusion is 0.8651081444987971
The AUROC of Infiltration is 0.6872755968575905
The AUROC of Mass is 0.8243209463821057
The AUROC of Nodule is 0.7353637346782509
The AUROC of Pneumonia is 0.6954553231477958
The AUROC of Pneumothorax is 0.8344892509735794
The AUROC of Consolidation is 0.8065236049143434
The AUROC of Edema is 0.8613883409773078
The AUROC of Emphysema is 0.8758645988682286
The AUROC of Fibrosis is 0.7632158529395172
The AUROC of Pleural_Thickening is 0.7645910297371596
The AUROC of Hernia is 0.8773678051634608
