Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.677781
Trained 51 batches 	Training Loss: 0.184348
Trained 101 batches 	Training Loss: 0.235287
Trained 151 batches 	Training Loss: 0.134854
Trained 201 batches 	Training Loss: 0.303456
Trained 251 batches 	Training Loss: 0.216060
Trained 301 batches 	Training Loss: 0.241374
Trained 351 batches 	Training Loss: 0.176705
Trained 401 batches 	Training Loss: 0.155257
Trained 451 batches 	Training Loss: 0.180031
Trained 501 batches 	Training Loss: 0.235924
Trained 551 batches 	Training Loss: 0.234890
Trained 601 batches 	Training Loss: 0.158205
Trained 651 batches 	Training Loss: 0.302550
Trained 701 batches 	Training Loss: 0.199241
Trained 751 batches 	Training Loss: 0.168006
Trained 801 batches 	Training Loss: 0.243220
Trained 851 batches 	Training Loss: 0.256787
Trained 901 batches 	Training Loss: 0.119746
Trained 951 batches 	Training Loss: 0.169280
Trained 1001 batches 	Training Loss: 0.168669
Trained 1051 batches 	Training Loss: 0.142285
Trained 1101 batches 	Training Loss: 0.160926
Trained 1151 batches 	Training Loss: 0.137301
Trained 1201 batches 	Training Loss: 0.140683
Trained 1251 batches 	Training Loss: 0.171097
Trained 1301 batches 	Training Loss: 0.215025
Trained 1351 batches 	Training Loss: 0.176107
Trained 1401 batches 	Training Loss: 0.162875
Trained 1451 batches 	Training Loss: 0.184095
Trained 1501 batches 	Training Loss: 0.243755
Trained 1551 batches 	Training Loss: 0.189413
Trained 1601 batches 	Training Loss: 0.188501
Trained 1651 batches 	Training Loss: 0.126744
Trained 1701 batches 	Training Loss: 0.172721
Trained 1751 batches 	Training Loss: 0.113342
Trained 1801 batches 	Training Loss: 0.161917
Trained 1851 batches 	Training Loss: 0.152475
Trained 1901 batches 	Training Loss: 0.265817
Trained 1951 batches 	Training Loss: 0.165397
Trained 2001 batches 	Training Loss: 0.208112
Trained 2051 batches 	Training Loss: 0.115676
Trained 2101 batches 	Training Loss: 0.193307
Trained 2151 batches 	Training Loss: 0.095993
Trained 2201 batches 	Training Loss: 0.270184
Trained 2251 batches 	Training Loss: 0.084503
Trained 2301 batches 	Training Loss: 0.154392
Trained 2351 batches 	Training Loss: 0.138761
Trained 2401 batches 	Training Loss: 0.110809
Trained 2451 batches 	Training Loss: 0.112547
Trained 2501 batches 	Training Loss: 0.162147
Trained 2551 batches 	Training Loss: 0.182046
Trained 2601 batches 	Training Loss: 0.242859
Trained 2651 batches 	Training Loss: 0.181456
Trained 2701 batches 	Training Loss: 0.162903
Trained 2751 batches 	Training Loss: 0.176567
Trained 2801 batches 	Training Loss: 0.255429
Trained 2851 batches 	Training Loss: 0.160480
Trained 2901 batches 	Training Loss: 0.261066
Trained 2951 batches 	Training Loss: 0.159948
Trained 3001 batches 	Training Loss: 0.212664
Trained 3051 batches 	Training Loss: 0.228051
Trained 3101 batches 	Training Loss: 0.193191
Trained 3151 batches 	Training Loss: 0.113173
Trained 3201 batches 	Training Loss: 0.112348
Trained 3251 batches 	Training Loss: 0.174101
Trained 3301 batches 	Training Loss: 0.176320
Trained 3351 batches 	Training Loss: 0.209571
Trained 3401 batches 	Training Loss: 0.186397
Trained 3451 batches 	Training Loss: 0.161215
Trained 3501 batches 	Training Loss: 0.204855
Trained 3551 batches 	Training Loss: 0.154285
Trained 3601 batches 	Training Loss: 0.138896
Trained 3651 batches 	Training Loss: 0.144849
Trained 3701 batches 	Training Loss: 0.263157
Trained 3751 batches 	Training Loss: 0.198583
Trained 3801 batches 	Training Loss: 0.132407
Trained 3851 batches 	Training Loss: 0.195675
Trained 3901 batches 	Training Loss: 0.213350
Trained 3951 batches 	Training Loss: 0.141197
Trained 4001 batches 	Training Loss: 0.298892
Trained 4051 batches 	Training Loss: 0.205902
Trained 4101 batches 	Training Loss: 0.254077
Trained 4151 batches 	Training Loss: 0.279557
Trained 4201 batches 	Training Loss: 0.178622
Trained 4251 batches 	Training Loss: 0.287789
Trained 4301 batches 	Training Loss: 0.199010
Trained 4351 batches 	Training Loss: 0.153005
Trained 4401 batches 	Training Loss: 0.192339
Trained 4451 batches 	Training Loss: 0.156942
Trained 4501 batches 	Training Loss: 0.189003
Trained 4551 batches 	Training Loss: 0.181801
Trained 4601 batches 	Training Loss: 0.137098
Trained 4651 batches 	Training Loss: 0.190198
Trained 4701 batches 	Training Loss: 0.228533
Trained 4751 batches 	Training Loss: 0.193301
Trained 4801 batches 	Training Loss: 0.214594
Trained 4851 batches 	Training Loss: 0.201055
Trained 4901 batches 	Training Loss: 0.144974
Epoch: 1 	Training Loss: 0.182080
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.491
The AUROC of Atelectasis is 0.46751249653984184
The AUROC of Cardiomegaly is 0.5265966074965849
The AUROC of Effusion is 0.5144508964898782
The AUROC of Infiltration is 0.4754217820686602
The AUROC of Mass is 0.5244110985015016
The AUROC of Nodule is 0.4940255563474585
The AUROC of Pneumonia is 0.502352217745776
The AUROC of Pneumothorax is 0.478876190060433
The AUROC of Consolidation is 0.5292530642172035
The AUROC of Edema is 0.479007688869761
The AUROC of Emphysema is 0.4988189561766126
The AUROC of Fibrosis is 0.515019987506133
The AUROC of Pleural_Thickening is 0.4508330793788213
The AUROC of Hernia is 0.41541561886389466
Started epoch 2
Trained 1 batches 	Training Loss: 0.232778
Trained 51 batches 	Training Loss: 0.198258
Trained 101 batches 	Training Loss: 0.172518
Trained 151 batches 	Training Loss: 0.165516
Trained 201 batches 	Training Loss: 0.263116
Trained 251 batches 	Training Loss: 0.196283
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.683528
Trained 51 batches 	Training Loss: 0.180956
Trained 101 batches 	Training Loss: 0.223510
Trained 151 batches 	Training Loss: 0.130989
Trained 201 batches 	Training Loss: 0.257268
Trained 251 batches 	Training Loss: 0.205237
Trained 301 batches 	Training Loss: 0.235698
Trained 351 batches 	Training Loss: 0.164731
Trained 401 batches 	Training Loss: 0.168440
Trained 451 batches 	Training Loss: 0.162205
Trained 501 batches 	Training Loss: 0.255393
Trained 551 batches 	Training Loss: 0.229909
Trained 601 batches 	Training Loss: 0.149619
Trained 651 batches 	Training Loss: 0.293372
Trained 701 batches 	Training Loss: 0.185910
Trained 751 batches 	Training Loss: 0.154119
Trained 801 batches 	Training Loss: 0.226004
Trained 851 batches 	Training Loss: 0.274301
Trained 901 batches 	Training Loss: 0.110514
Trained 951 batches 	Training Loss: 0.151505
Trained 1001 batches 	Training Loss: 0.165607
Trained 1051 batches 	Training Loss: 0.136183
Trained 1101 batches 	Training Loss: 0.150505
Trained 1151 batches 	Training Loss: 0.126547
Trained 1201 batches 	Training Loss: 0.148700
Trained 1251 batches 	Training Loss: 0.153936
Trained 1301 batches 	Training Loss: 0.180377
Trained 1351 batches 	Training Loss: 0.173554
Trained 1401 batches 	Training Loss: 0.167163
Trained 1451 batches 	Training Loss: 0.190824
Trained 1501 batches 	Training Loss: 0.224073
Trained 1551 batches 	Training Loss: 0.182716
Trained 1601 batches 	Training Loss: 0.169144
Trained 1651 batches 	Training Loss: 0.115128
Trained 1701 batches 	Training Loss: 0.163364
Trained 1751 batches 	Training Loss: 0.112421
Trained 1801 batches 	Training Loss: 0.145279
Trained 1851 batches 	Training Loss: 0.154401
Trained 1901 batches 	Training Loss: 0.264129
Trained 1951 batches 	Training Loss: 0.143606
Trained 2001 batches 	Training Loss: 0.187176
Trained 2051 batches 	Training Loss: 0.110476
Trained 2101 batches 	Training Loss: 0.174019
Trained 2151 batches 	Training Loss: 0.098588
Trained 2201 batches 	Training Loss: 0.251688
Trained 2251 batches 	Training Loss: 0.071274
Trained 2301 batches 	Training Loss: 0.145261
Trained 2351 batches 	Training Loss: 0.124298
Trained 2401 batches 	Training Loss: 0.104908
Trained 2451 batches 	Training Loss: 0.105960
Trained 2501 batches 	Training Loss: 0.155820
Trained 2551 batches 	Training Loss: 0.183407
Trained 2601 batches 	Training Loss: 0.211633
Trained 2651 batches 	Training Loss: 0.169797
Trained 2701 batches 	Training Loss: 0.135486
Trained 2751 batches 	Training Loss: 0.171973
Trained 2801 batches 	Training Loss: 0.235232
Trained 2851 batches 	Training Loss: 0.161094
Trained 2901 batches 	Training Loss: 0.235638
Trained 2951 batches 	Training Loss: 0.159201
Trained 3001 batches 	Training Loss: 0.197563
Trained 3051 batches 	Training Loss: 0.217850
Trained 3101 batches 	Training Loss: 0.171453
Trained 3151 batches 	Training Loss: 0.097878
Trained 3201 batches 	Training Loss: 0.127968
Trained 3251 batches 	Training Loss: 0.153212
Trained 3301 batches 	Training Loss: 0.158092
Trained 3351 batches 	Training Loss: 0.206197
Trained 3401 batches 	Training Loss: 0.166138
Trained 3451 batches 	Training Loss: 0.166736
Trained 3501 batches 	Training Loss: 0.218567
Trained 3551 batches 	Training Loss: 0.156588
Trained 3601 batches 	Training Loss: 0.134350
Trained 3651 batches 	Training Loss: 0.126723
Trained 3701 batches 	Training Loss: 0.240628
Trained 3751 batches 	Training Loss: 0.185738
Trained 3801 batches 	Training Loss: 0.127853
Trained 3851 batches 	Training Loss: 0.187420
Trained 3901 batches 	Training Loss: 0.208626
Trained 3951 batches 	Training Loss: 0.153645
Trained 4001 batches 	Training Loss: 0.323670
Trained 4051 batches 	Training Loss: 0.197125
Trained 4101 batches 	Training Loss: 0.211581
Trained 4151 batches 	Training Loss: 0.227203
Trained 4201 batches 	Training Loss: 0.164425
Trained 4251 batches 	Training Loss: 0.268658
Trained 4301 batches 	Training Loss: 0.202254
Trained 4351 batches 	Training Loss: 0.141448
Trained 4401 batches 	Training Loss: 0.159756
Trained 4451 batches 	Training Loss: 0.128434
Trained 4501 batches 	Training Loss: 0.171107
Trained 4551 batches 	Training Loss: 0.165420
Trained 4601 batches 	Training Loss: 0.133237
Trained 4651 batches 	Training Loss: 0.181857
Trained 4701 batches 	Training Loss: 0.212082
Trained 4751 batches 	Training Loss: 0.179931
Trained 4801 batches 	Training Loss: 0.205478
Trained 4851 batches 	Training Loss: 0.187073
Trained 4901 batches 	Training Loss: 0.133275
Epoch: 1 	Training Loss: 0.173353
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.727
The AUROC of Atelectasis is 0.7189746645126616
The AUROC of Cardiomegaly is 0.7755225709333394
The AUROC of Effusion is 0.8384400688628223
The AUROC of Infiltration is 0.6748839248665247
The AUROC of Mass is 0.6912959044223974
The AUROC of Nodule is 0.6435383933788356
The AUROC of Pneumonia is 0.6757915928529249
The AUROC of Pneumothorax is 0.7811813548341382
The AUROC of Consolidation is 0.7190996901550182
The AUROC of Edema is 0.850336439780176
The AUROC of Emphysema is 0.6770016282137348
The AUROC of Fibrosis is 0.6660885348662003
The AUROC of Pleural_Thickening is 0.705647296874393
The AUROC of Hernia is 0.7541515955309058
Started epoch 2
Trained 1 batches 	Training Loss: 0.161726
Trained 51 batches 	Training Loss: 0.167358
Trained 101 batches 	Training Loss: 0.134456
Trained 151 batches 	Training Loss: 0.151315
Trained 201 batches 	Training Loss: 0.260787
Trained 251 batches 	Training Loss: 0.162426
Trained 301 batches 	Training Loss: 0.218990
Trained 351 batches 	Training Loss: 0.181409
Trained 401 batches 	Training Loss: 0.191159
Trained 451 batches 	Training Loss: 0.166441
Trained 501 batches 	Training Loss: 0.120027
Trained 551 batches 	Training Loss: 0.110872
Trained 601 batches 	Training Loss: 0.160327
Trained 651 batches 	Training Loss: 0.156307
Trained 701 batches 	Training Loss: 0.178919
Trained 751 batches 	Training Loss: 0.104095
Trained 801 batches 	Training Loss: 0.168112
Trained 851 batches 	Training Loss: 0.148307
Trained 901 batches 	Training Loss: 0.142972
Trained 951 batches 	Training Loss: 0.209346
Trained 1001 batches 	Training Loss: 0.186144
Trained 1051 batches 	Training Loss: 0.216987
Trained 1101 batches 	Training Loss: 0.245030
Trained 1151 batches 	Training Loss: 0.181971
Trained 1201 batches 	Training Loss: 0.147623
Trained 1251 batches 	Training Loss: 0.156852
Trained 1301 batches 	Training Loss: 0.234747
Trained 1351 batches 	Training Loss: 0.128312
Trained 1401 batches 	Training Loss: 0.110740
Trained 1451 batches 	Training Loss: 0.171634
Trained 1501 batches 	Training Loss: 0.146893
Trained 1551 batches 	Training Loss: 0.216508
Trained 1601 batches 	Training Loss: 0.145576
Trained 1651 batches 	Training Loss: 0.103965
Trained 1701 batches 	Training Loss: 0.196094
Trained 1751 batches 	Training Loss: 0.187315
Trained 1801 batches 	Training Loss: 0.185708
Trained 1851 batches 	Training Loss: 0.105766
Trained 1901 batches 	Training Loss: 0.167735
Trained 1951 batches 	Training Loss: 0.247074
Trained 2001 batches 	Training Loss: 0.091471
Trained 2051 batches 	Training Loss: 0.273603
Trained 2101 batches 	Training Loss: 0.134119
Trained 2151 batches 	Training Loss: 0.176633
Trained 2201 batches 	Training Loss: 0.249823
Trained 2251 batches 	Training Loss: 0.172229
Trained 2301 batches 	Training Loss: 0.199607
Trained 2351 batches 	Training Loss: 0.141839
Trained 2401 batches 	Training Loss: 0.109423
Trained 2451 batches 	Training Loss: 0.127395
Trained 2501 batches 	Training Loss: 0.086272
Trained 2551 batches 	Training Loss: 0.236206
Trained 2601 batches 	Training Loss: 0.088017
Trained 2651 batches 	Training Loss: 0.152264
Trained 2701 batches 	Training Loss: 0.219255
Trained 2751 batches 	Training Loss: 0.167625
Trained 2801 batches 	Training Loss: 0.181872
Trained 2851 batches 	Training Loss: 0.177246
Trained 2901 batches 	Training Loss: 0.204417
Trained 2951 batches 	Training Loss: 0.153016
Trained 3001 batches 	Training Loss: 0.202460
Trained 3051 batches 	Training Loss: 0.153199
Trained 3101 batches 	Training Loss: 0.133269
Trained 3151 batches 	Training Loss: 0.175244
Trained 3201 batches 	Training Loss: 0.194178
Trained 3251 batches 	Training Loss: 0.199269
Trained 3301 batches 	Training Loss: 0.138628
Trained 3351 batches 	Training Loss: 0.139052
Trained 3401 batches 	Training Loss: 0.153858
Trained 3451 batches 	Training Loss: 0.146638
Trained 3501 batches 	Training Loss: 0.183948
Trained 3551 batches 	Training Loss: 0.157856
Trained 3601 batches 	Training Loss: 0.092518
Trained 3651 batches 	Training Loss: 0.129580
Trained 3701 batches 	Training Loss: 0.186232
Trained 3751 batches 	Training Loss: 0.151729
Trained 3801 batches 	Training Loss: 0.154087
Trained 3851 batches 	Training Loss: 0.119469
Trained 3901 batches 	Training Loss: 0.202258
Trained 3951 batches 	Training Loss: 0.137203
Trained 4001 batches 	Training Loss: 0.157052
Trained 4051 batches 	Training Loss: 0.146950
Trained 4101 batches 	Training Loss: 0.183129
Trained 4151 batches 	Training Loss: 0.174376
Trained 4201 batches 	Training Loss: 0.143712
Trained 4251 batches 	Training Loss: 0.084152
Trained 4301 batches 	Training Loss: 0.140301
Trained 4351 batches 	Training Loss: 0.143732
Trained 4401 batches 	Training Loss: 0.193415
Trained 4451 batches 	Training Loss: 0.177339
Trained 4501 batches 	Training Loss: 0.180126
Trained 4551 batches 	Training Loss: 0.096056
Trained 4601 batches 	Training Loss: 0.141662
Trained 4651 batches 	Training Loss: 0.067910
Trained 4701 batches 	Training Loss: 0.147051
Trained 4751 batches 	Training Loss: 0.177546
Trained 4801 batches 	Training Loss: 0.188162
Trained 4851 batches 	Training Loss: 0.200562
Trained 4901 batches 	Training Loss: 0.129647
Epoch: 2 	Training Loss: 0.164082
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.756
The AUROC of Atelectasis is 0.7509178763500449
The AUROC of Cardiomegaly is 0.849694739724185
The AUROC of Effusion is 0.8520781183399853
The AUROC of Infiltration is 0.680358073529005
The AUROC of Mass is 0.7263563753714464
The AUROC of Nodule is 0.6660793030112999
The AUROC of Pneumonia is 0.6996161685429644
The AUROC of Pneumothorax is 0.8082843327057203
The AUROC of Consolidation is 0.7388873364795495
The AUROC of Edema is 0.8702881877772629
The AUROC of Emphysema is 0.7537481303513173
The AUROC of Fibrosis is 0.7286374982237797
The AUROC of Pleural_Thickening is 0.7403822419562772
The AUROC of Hernia is 0.7200270993374441
Started epoch 3
Trained 1 batches 	Training Loss: 0.182659
Trained 51 batches 	Training Loss: 0.175924
Trained 101 batches 	Training Loss: 0.215596
Trained 151 batches 	Training Loss: 0.312017
Trained 201 batches 	Training Loss: 0.152632
Trained 251 batches 	Training Loss: 0.143603
Trained 301 batches 	Training Loss: 0.119756
Trained 351 batches 	Training Loss: 0.245204
Trained 401 batches 	Training Loss: 0.212026
Trained 451 batches 	Training Loss: 0.182121
Trained 501 batches 	Training Loss: 0.110772
Trained 551 batches 	Training Loss: 0.160371
Trained 601 batches 	Training Loss: 0.180631
Trained 651 batches 	Training Loss: 0.149057
Trained 701 batches 	Training Loss: 0.213150
Trained 751 batches 	Training Loss: 0.142481
Trained 801 batches 	Training Loss: 0.110536
Trained 851 batches 	Training Loss: 0.113441
Trained 901 batches 	Training Loss: 0.139461
Trained 951 batches 	Training Loss: 0.080810
Trained 1001 batches 	Training Loss: 0.156836
Trained 1051 batches 	Training Loss: 0.099183
Trained 1101 batches 	Training Loss: 0.174344
Trained 1151 batches 	Training Loss: 0.163659
Trained 1201 batches 	Training Loss: 0.147093
Trained 1251 batches 	Training Loss: 0.164604
Trained 1301 batches 	Training Loss: 0.123097
Trained 1351 batches 	Training Loss: 0.091425
Trained 1401 batches 	Training Loss: 0.107531
Trained 1451 batches 	Training Loss: 0.204004
Trained 1501 batches 	Training Loss: 0.164853
Trained 1551 batches 	Training Loss: 0.107725
Trained 1601 batches 	Training Loss: 0.204889
Trained 1651 batches 	Training Loss: 0.164153
Trained 1701 batches 	Training Loss: 0.195425
Trained 1751 batches 	Training Loss: 0.103903
Trained 1801 batches 	Training Loss: 0.188802
Trained 1851 batches 	Training Loss: 0.143607
Trained 1901 batches 	Training Loss: 0.090762
Trained 1951 batches 	Training Loss: 0.196152
Trained 2001 batches 	Training Loss: 0.184515
Trained 2051 batches 	Training Loss: 0.162377
Trained 2101 batches 	Training Loss: 0.162554
Trained 2151 batches 	Training Loss: 0.178564
Trained 2201 batches 	Training Loss: 0.169936
Trained 2251 batches 	Training Loss: 0.194465
Trained 2301 batches 	Training Loss: 0.089670
Trained 2351 batches 	Training Loss: 0.178269
Trained 2401 batches 	Training Loss: 0.167648
Trained 2451 batches 	Training Loss: 0.229875
Trained 2501 batches 	Training Loss: 0.159396
Trained 2551 batches 	Training Loss: 0.112853
Trained 2601 batches 	Training Loss: 0.170709
Trained 2651 batches 	Training Loss: 0.128623
Trained 2701 batches 	Training Loss: 0.183358
Trained 2751 batches 	Training Loss: 0.185310
Trained 2801 batches 	Training Loss: 0.190478
Trained 2851 batches 	Training Loss: 0.106888
Trained 2901 batches 	Training Loss: 0.202687
Trained 2951 batches 	Training Loss: 0.213217
Trained 3001 batches 	Training Loss: 0.167451
Trained 3051 batches 	Training Loss: 0.105431
Trained 3101 batches 	Training Loss: 0.178086
Trained 3151 batches 	Training Loss: 0.133198
Trained 3201 batches 	Training Loss: 0.136449
Trained 3251 batches 	Training Loss: 0.222380
Trained 3301 batches 	Training Loss: 0.151233
Trained 3351 batches 	Training Loss: 0.119452
Trained 3401 batches 	Training Loss: 0.224712
Trained 3451 batches 	Training Loss: 0.165867
Trained 3501 batches 	Training Loss: 0.156645
Trained 3551 batches 	Training Loss: 0.194355
Trained 3601 batches 	Training Loss: 0.142500
Trained 3651 batches 	Training Loss: 0.179527
Trained 3701 batches 	Training Loss: 0.133994
Trained 3751 batches 	Training Loss: 0.156302
Trained 3801 batches 	Training Loss: 0.218460
Trained 3851 batches 	Training Loss: 0.194754
Trained 3901 batches 	Training Loss: 0.142743
Trained 3951 batches 	Training Loss: 0.097516
Trained 4001 batches 	Training Loss: 0.110049
Trained 4051 batches 	Training Loss: 0.158090
Trained 4101 batches 	Training Loss: 0.173058
Trained 4151 batches 	Training Loss: 0.158349
Trained 4201 batches 	Training Loss: 0.205135
Trained 4251 batches 	Training Loss: 0.173929
Trained 4301 batches 	Training Loss: 0.202040
Trained 4351 batches 	Training Loss: 0.178455
Trained 4401 batches 	Training Loss: 0.141430
Trained 4451 batches 	Training Loss: 0.149456
Trained 4501 batches 	Training Loss: 0.137206
Trained 4551 batches 	Training Loss: 0.201430
Trained 4601 batches 	Training Loss: 0.149684
Trained 4651 batches 	Training Loss: 0.136354
Trained 4701 batches 	Training Loss: 0.152669
Trained 4751 batches 	Training Loss: 0.239061
Trained 4801 batches 	Training Loss: 0.155357
Trained 4851 batches 	Training Loss: 0.174827
Trained 4901 batches 	Training Loss: 0.146306
Epoch: 3 	Training Loss: 0.159489
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.773
The AUROC of Atelectasis is 0.7579599192913528
The AUROC of Cardiomegaly is 0.8685154592169093
The AUROC of Effusion is 0.8660805945968887
The AUROC of Infiltration is 0.6806780823812553
The AUROC of Mass is 0.7648818588477083
The AUROC of Nodule is 0.6791547613696924
The AUROC of Pneumonia is 0.7272552479189287
The AUROC of Pneumothorax is 0.8118557002692661
The AUROC of Consolidation is 0.7445197960054517
The AUROC of Edema is 0.876537773952195
The AUROC of Emphysema is 0.7757764930690504
The AUROC of Fibrosis is 0.7459594673215669
The AUROC of Pleural_Thickening is 0.7551420058135677
The AUROC of Hernia is 0.7633447012757357
Started epoch 4
Trained 1 batches 	Training Loss: 0.122827
Trained 51 batches 	Training Loss: 0.188984
Trained 101 batches 	Training Loss: 0.166290
Trained 151 batches 	Training Loss: 0.114104
Trained 201 batches 	Training Loss: 0.144703
Trained 251 batches 	Training Loss: 0.172786
Trained 301 batches 	Training Loss: 0.148315
Trained 351 batches 	Training Loss: 0.113727
Trained 401 batches 	Training Loss: 0.134744
Trained 451 batches 	Training Loss: 0.334867
Trained 501 batches 	Training Loss: 0.179993
Trained 551 batches 	Training Loss: 0.133343
Trained 601 batches 	Training Loss: 0.155018
Trained 651 batches 	Training Loss: 0.133268
Trained 701 batches 	Training Loss: 0.170279
Trained 751 batches 	Training Loss: 0.174738
Trained 801 batches 	Training Loss: 0.111681
Trained 851 batches 	Training Loss: 0.088587
Trained 901 batches 	Training Loss: 0.165661
Trained 951 batches 	Training Loss: 0.214519
Trained 1001 batches 	Training Loss: 0.186393
Trained 1051 batches 	Training Loss: 0.201696
Trained 1101 batches 	Training Loss: 0.183656
Trained 1151 batches 	Training Loss: 0.148076
Trained 1201 batches 	Training Loss: 0.130188
Trained 1251 batches 	Training Loss: 0.132349
Trained 1301 batches 	Training Loss: 0.187928
Trained 1351 batches 	Training Loss: 0.088765
Trained 1401 batches 	Training Loss: 0.110204
Trained 1451 batches 	Training Loss: 0.116178
Trained 1501 batches 	Training Loss: 0.122434
Trained 1551 batches 	Training Loss: 0.132464
Trained 1601 batches 	Training Loss: 0.141756
Trained 1651 batches 	Training Loss: 0.122489
Trained 1701 batches 	Training Loss: 0.123976
Trained 1751 batches 	Training Loss: 0.167168
Trained 1801 batches 	Training Loss: 0.122121
Trained 1851 batches 	Training Loss: 0.168473
Trained 1901 batches 	Training Loss: 0.098510
Trained 1951 batches 	Training Loss: 0.158001
Trained 2001 batches 	Training Loss: 0.108357
Trained 2051 batches 	Training Loss: 0.210845
Trained 2101 batches 	Training Loss: 0.111469
Trained 2151 batches 	Training Loss: 0.176897
Trained 2201 batches 	Training Loss: 0.155324
Trained 2251 batches 	Training Loss: 0.122682
Trained 2301 batches 	Training Loss: 0.129042
Trained 2351 batches 	Training Loss: 0.138140
Trained 2401 batches 	Training Loss: 0.092489
Trained 2451 batches 	Training Loss: 0.249443
Trained 2501 batches 	Training Loss: 0.155118
Trained 2551 batches 	Training Loss: 0.185330
Trained 2601 batches 	Training Loss: 0.169750
Trained 2651 batches 	Training Loss: 0.178738
Trained 2701 batches 	Training Loss: 0.153387
Trained 2751 batches 	Training Loss: 0.196268
Trained 2801 batches 	Training Loss: 0.131907
Trained 2851 batches 	Training Loss: 0.172803
Trained 2901 batches 	Training Loss: 0.183995
Trained 2951 batches 	Training Loss: 0.135563
Trained 3001 batches 	Training Loss: 0.241560
Trained 3051 batches 	Training Loss: 0.104170
Trained 3101 batches 	Training Loss: 0.151887
Trained 3151 batches 	Training Loss: 0.288153
Trained 3201 batches 	Training Loss: 0.132973
Trained 3251 batches 	Training Loss: 0.122679
Trained 3301 batches 	Training Loss: 0.127969
Trained 3351 batches 	Training Loss: 0.147722
Trained 3401 batches 	Training Loss: 0.174352
Trained 3451 batches 	Training Loss: 0.130722
Trained 3501 batches 	Training Loss: 0.074187
Trained 3551 batches 	Training Loss: 0.111153
Trained 3601 batches 	Training Loss: 0.150822
Trained 3651 batches 	Training Loss: 0.117454
Trained 3701 batches 	Training Loss: 0.157597
Trained 3751 batches 	Training Loss: 0.109800
Trained 3801 batches 	Training Loss: 0.216169
Trained 3851 batches 	Training Loss: 0.149344
Trained 3901 batches 	Training Loss: 0.117060
Trained 3951 batches 	Training Loss: 0.328939
Trained 4001 batches 	Training Loss: 0.213161
Trained 4051 batches 	Training Loss: 0.132080
Trained 4101 batches 	Training Loss: 0.172343
Trained 4151 batches 	Training Loss: 0.218259
Trained 4201 batches 	Training Loss: 0.129905
Trained 4251 batches 	Training Loss: 0.156041
Trained 4301 batches 	Training Loss: 0.252607
Trained 4351 batches 	Training Loss: 0.108859
Trained 4401 batches 	Training Loss: 0.164326
Trained 4451 batches 	Training Loss: 0.158564
Trained 4501 batches 	Training Loss: 0.238163
Trained 4551 batches 	Training Loss: 0.172244
Trained 4601 batches 	Training Loss: 0.213117
Trained 4651 batches 	Training Loss: 0.176992
Trained 4701 batches 	Training Loss: 0.135607
Trained 4751 batches 	Training Loss: 0.176324
Trained 4801 batches 	Training Loss: 0.172316
Trained 4851 batches 	Training Loss: 0.210736
Trained 4901 batches 	Training Loss: 0.147757
Epoch: 4 	Training Loss: 0.156603
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.780
The AUROC of Atelectasis is 0.7680544633857583
The AUROC of Cardiomegaly is 0.8718802872768374
The AUROC of Effusion is 0.8676679453894939
The AUROC of Infiltration is 0.6854455171650669
The AUROC of Mass is 0.7657618922447812
The AUROC of Nodule is 0.6842126665938972
The AUROC of Pneumonia is 0.728899984761034
The AUROC of Pneumothorax is 0.8269958344721471
The AUROC of Consolidation is 0.7457689879564878
The AUROC of Edema is 0.892031384493147
The AUROC of Emphysema is 0.7981614875925991
The AUROC of Fibrosis is 0.7459600035390352
The AUROC of Pleural_Thickening is 0.7464190000924129
The AUROC of Hernia is 0.7953586643241816
Started epoch 5
Trained 1 batches 	Training Loss: 0.186727
Trained 51 batches 	Training Loss: 0.137444
Trained 101 batches 	Training Loss: 0.150472
Trained 151 batches 	Training Loss: 0.186603
Trained 201 batches 	Training Loss: 0.174148
Trained 251 batches 	Training Loss: 0.190399
Trained 301 batches 	Training Loss: 0.144603
Trained 351 batches 	Training Loss: 0.169801
Trained 401 batches 	Training Loss: 0.125521
Trained 451 batches 	Training Loss: 0.109696
Trained 501 batches 	Training Loss: 0.143473
Trained 551 batches 	Training Loss: 0.107285
Trained 601 batches 	Training Loss: 0.121927
Trained 651 batches 	Training Loss: 0.099792
Trained 701 batches 	Training Loss: 0.129746
Trained 751 batches 	Training Loss: 0.147754
Trained 801 batches 	Training Loss: 0.202157
Trained 851 batches 	Training Loss: 0.162742
Trained 901 batches 	Training Loss: 0.188631
Trained 951 batches 	Training Loss: 0.189881
Trained 1001 batches 	Training Loss: 0.225836
Trained 1051 batches 	Training Loss: 0.132785
Trained 1101 batches 	Training Loss: 0.300436
Trained 1151 batches 	Training Loss: 0.176221
Trained 1201 batches 	Training Loss: 0.133781
Trained 1251 batches 	Training Loss: 0.119933
Trained 1301 batches 	Training Loss: 0.112006
Trained 1351 batches 	Training Loss: 0.168482
Trained 1401 batches 	Training Loss: 0.183876
Trained 1451 batches 	Training Loss: 0.211913
Trained 1501 batches 	Training Loss: 0.156707
Trained 1551 batches 	Training Loss: 0.078058
Trained 1601 batches 	Training Loss: 0.130155
Trained 1651 batches 	Training Loss: 0.106361
Trained 1701 batches 	Training Loss: 0.190240
Trained 1751 batches 	Training Loss: 0.274877
Trained 1801 batches 	Training Loss: 0.172916
Trained 1851 batches 	Training Loss: 0.245365
Trained 1901 batches 	Training Loss: 0.147390
Trained 1951 batches 	Training Loss: 0.186099
Trained 2001 batches 	Training Loss: 0.137404
Trained 2051 batches 	Training Loss: 0.163038
Trained 2101 batches 	Training Loss: 0.163090
Trained 2151 batches 	Training Loss: 0.142492
Trained 2201 batches 	Training Loss: 0.227535
Trained 2251 batches 	Training Loss: 0.131286
Trained 2301 batches 	Training Loss: 0.140328
Trained 2351 batches 	Training Loss: 0.121596
Trained 2401 batches 	Training Loss: 0.123581
Trained 2451 batches 	Training Loss: 0.139493
Trained 2501 batches 	Training Loss: 0.155636
Trained 2551 batches 	Training Loss: 0.121080
Trained 2601 batches 	Training Loss: 0.205790
Trained 2651 batches 	Training Loss: 0.172792
Trained 2701 batches 	Training Loss: 0.158310
Trained 2751 batches 	Training Loss: 0.137542
Trained 2801 batches 	Training Loss: 0.146078
Trained 2851 batches 	Training Loss: 0.153075
Trained 2901 batches 	Training Loss: 0.207960
Trained 2951 batches 	Training Loss: 0.132500
Trained 3001 batches 	Training Loss: 0.133164
Trained 3051 batches 	Training Loss: 0.135068
Trained 3101 batches 	Training Loss: 0.148154
Trained 3151 batches 	Training Loss: 0.211195
Trained 3201 batches 	Training Loss: 0.111364
Trained 3251 batches 	Training Loss: 0.222403
Trained 3301 batches 	Training Loss: 0.126983
Trained 3351 batches 	Training Loss: 0.147314
Trained 3401 batches 	Training Loss: 0.141893
Trained 3451 batches 	Training Loss: 0.161853
Trained 3501 batches 	Training Loss: 0.154566
Trained 3551 batches 	Training Loss: 0.144903
Trained 3601 batches 	Training Loss: 0.121662
Trained 3651 batches 	Training Loss: 0.230577
Trained 3701 batches 	Training Loss: 0.134931
Trained 3751 batches 	Training Loss: 0.147827
Trained 3801 batches 	Training Loss: 0.125780
Trained 3851 batches 	Training Loss: 0.190895
Trained 3901 batches 	Training Loss: 0.263681
Trained 3951 batches 	Training Loss: 0.195732
Trained 4001 batches 	Training Loss: 0.187522
Trained 4051 batches 	Training Loss: 0.091502
Trained 4101 batches 	Training Loss: 0.168687
Trained 4151 batches 	Training Loss: 0.158668
Trained 4201 batches 	Training Loss: 0.132419
Trained 4251 batches 	Training Loss: 0.093456
Trained 4301 batches 	Training Loss: 0.120123
Trained 4351 batches 	Training Loss: 0.248733
Trained 4401 batches 	Training Loss: 0.149445
Trained 4451 batches 	Training Loss: 0.090017
Trained 4501 batches 	Training Loss: 0.111921
Trained 4551 batches 	Training Loss: 0.134425
Trained 4601 batches 	Training Loss: 0.169228
Trained 4651 batches 	Training Loss: 0.131141
Trained 4701 batches 	Training Loss: 0.164678
Trained 4751 batches 	Training Loss: 0.114340
Trained 4801 batches 	Training Loss: 0.116479
Trained 4851 batches 	Training Loss: 0.122772
Trained 4901 batches 	Training Loss: 0.140234
Epoch: 5 	Training Loss: 0.154449
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.781
The AUROC of Atelectasis is 0.7675960329454586
The AUROC of Cardiomegaly is 0.8824850473640241
The AUROC of Effusion is 0.8706034722952614
The AUROC of Infiltration is 0.6820049793908546
The AUROC of Mass is 0.7856164818924831
The AUROC of Nodule is 0.7058296181672513
The AUROC of Pneumonia is 0.6931893131988495
The AUROC of Pneumothorax is 0.8323983197434884
The AUROC of Consolidation is 0.74086546422612
The AUROC of Edema is 0.8934843243064291
The AUROC of Emphysema is 0.8107944976386486
The AUROC of Fibrosis is 0.7246271277779417
The AUROC of Pleural_Thickening is 0.7353521329645503
The AUROC of Hernia is 0.8095306991858717
Started epoch 6
Trained 1 batches 	Training Loss: 0.187319
Trained 51 batches 	Training Loss: 0.168728
Trained 101 batches 	Training Loss: 0.125099
Trained 151 batches 	Training Loss: 0.122295
Trained 201 batches 	Training Loss: 0.223354
Trained 251 batches 	Training Loss: 0.122413
Trained 301 batches 	Training Loss: 0.254845
Trained 351 batches 	Training Loss: 0.094631
Trained 401 batches 	Training Loss: 0.132201
Trained 451 batches 	Training Loss: 0.122510
Trained 501 batches 	Training Loss: 0.188623
Trained 551 batches 	Training Loss: 0.125047
Trained 601 batches 	Training Loss: 0.115098
Trained 651 batches 	Training Loss: 0.106444
Trained 701 batches 	Training Loss: 0.142160
Trained 751 batches 	Training Loss: 0.198152
Trained 801 batches 	Training Loss: 0.253119
Trained 851 batches 	Training Loss: 0.181315
Trained 901 batches 	Training Loss: 0.174252
Trained 951 batches 	Training Loss: 0.145156
Trained 1001 batches 	Training Loss: 0.111916
Trained 1051 batches 	Training Loss: 0.079968
Trained 1101 batches 	Training Loss: 0.077883
Trained 1151 batches 	Training Loss: 0.129130
Trained 1201 batches 	Training Loss: 0.148338
Trained 1251 batches 	Training Loss: 0.138140
Trained 1301 batches 	Training Loss: 0.191808
Trained 1351 batches 	Training Loss: 0.115928
Trained 1401 batches 	Training Loss: 0.163440
Trained 1451 batches 	Training Loss: 0.136075
Trained 1501 batches 	Training Loss: 0.204719
Trained 1551 batches 	Training Loss: 0.166005
Trained 1601 batches 	Training Loss: 0.119271
Trained 1651 batches 	Training Loss: 0.108445
Trained 1701 batches 	Training Loss: 0.173008
Trained 1751 batches 	Training Loss: 0.166533
Trained 1801 batches 	Training Loss: 0.164995
Trained 1851 batches 	Training Loss: 0.155093
Trained 1901 batches 	Training Loss: 0.175484
Trained 1951 batches 	Training Loss: 0.101967
Trained 2001 batches 	Training Loss: 0.216111
Trained 2051 batches 	Training Loss: 0.171626
Trained 2101 batches 	Training Loss: 0.138503
Trained 2151 batches 	Training Loss: 0.197182
Trained 2201 batches 	Training Loss: 0.084428
Trained 2251 batches 	Training Loss: 0.166123
Trained 2301 batches 	Training Loss: 0.156818
Trained 2351 batches 	Training Loss: 0.172171
Trained 2401 batches 	Training Loss: 0.188979
Trained 2451 batches 	Training Loss: 0.144212
Trained 2501 batches 	Training Loss: 0.165319
Trained 2551 batches 	Training Loss: 0.166426
Trained 2601 batches 	Training Loss: 0.205315
Trained 2651 batches 	Training Loss: 0.142127
Trained 2701 batches 	Training Loss: 0.106075
Trained 2751 batches 	Training Loss: 0.210124
Trained 2801 batches 	Training Loss: 0.112322
Trained 2851 batches 	Training Loss: 0.070595
Trained 2901 batches 	Training Loss: 0.164046
Trained 2951 batches 	Training Loss: 0.175152
Trained 3001 batches 	Training Loss: 0.169341
Trained 3051 batches 	Training Loss: 0.141438
Trained 3101 batches 	Training Loss: 0.116244
Trained 3151 batches 	Training Loss: 0.143589
Trained 3201 batches 	Training Loss: 0.136713
Trained 3251 batches 	Training Loss: 0.208685
Trained 3301 batches 	Training Loss: 0.170765
Trained 3351 batches 	Training Loss: 0.169456
Trained 3401 batches 	Training Loss: 0.124803
Trained 3451 batches 	Training Loss: 0.116980
Trained 3501 batches 	Training Loss: 0.100658
Trained 3551 batches 	Training Loss: 0.230844
Trained 3601 batches 	Training Loss: 0.206799
Trained 3651 batches 	Training Loss: 0.166800
Trained 3701 batches 	Training Loss: 0.128129
Trained 3751 batches 	Training Loss: 0.141672
Trained 3801 batches 	Training Loss: 0.139355
Trained 3851 batches 	Training Loss: 0.103980
Trained 3901 batches 	Training Loss: 0.203069
Trained 3951 batches 	Training Loss: 0.177796
Trained 4001 batches 	Training Loss: 0.181571
Trained 4051 batches 	Training Loss: 0.184756
Trained 4101 batches 	Training Loss: 0.163967
Trained 4151 batches 	Training Loss: 0.166536
Trained 4201 batches 	Training Loss: 0.160086
Trained 4251 batches 	Training Loss: 0.204121
Trained 4301 batches 	Training Loss: 0.128406
Trained 4351 batches 	Training Loss: 0.160911
Trained 4401 batches 	Training Loss: 0.106981
Trained 4451 batches 	Training Loss: 0.162292
Trained 4501 batches 	Training Loss: 0.214730
Trained 4551 batches 	Training Loss: 0.215574
Trained 4601 batches 	Training Loss: 0.176519
Trained 4651 batches 	Training Loss: 0.136581
Trained 4701 batches 	Training Loss: 0.149824
Trained 4751 batches 	Training Loss: 0.153612
Trained 4801 batches 	Training Loss: 0.109550
Trained 4851 batches 	Training Loss: 0.151966
Trained 4901 batches 	Training Loss: 0.142712
Epoch: 6 	Training Loss: 0.152526
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.793
The AUROC of Atelectasis is 0.7730184669384321
The AUROC of Cardiomegaly is 0.90192356650713
The AUROC of Effusion is 0.8767563986884326
The AUROC of Infiltration is 0.6910077733823896
The AUROC of Mass is 0.7831151775262329
The AUROC of Nodule is 0.7150866063536295
The AUROC of Pneumonia is 0.7364007109930091
The AUROC of Pneumothorax is 0.8492095050462105
The AUROC of Consolidation is 0.7529863977097584
The AUROC of Edema is 0.8995716910547574
The AUROC of Emphysema is 0.8171078743124993
The AUROC of Fibrosis is 0.752220342482097
The AUROC of Pleural_Thickening is 0.7531028898213825
The AUROC of Hernia is 0.7964931620104034
Started epoch 7
Trained 1 batches 	Training Loss: 0.129758
Trained 51 batches 	Training Loss: 0.135868
Trained 101 batches 	Training Loss: 0.126110
Trained 151 batches 	Training Loss: 0.138734
Trained 201 batches 	Training Loss: 0.129837
Trained 251 batches 	Training Loss: 0.167639
Trained 301 batches 	Training Loss: 0.251391
Trained 351 batches 	Training Loss: 0.157704
Trained 401 batches 	Training Loss: 0.141803
Trained 451 batches 	Training Loss: 0.158578
Trained 501 batches 	Training Loss: 0.115626
Trained 551 batches 	Training Loss: 0.180107
Trained 601 batches 	Training Loss: 0.186423
Trained 651 batches 	Training Loss: 0.207720
Trained 701 batches 	Training Loss: 0.185665
Trained 751 batches 	Training Loss: 0.088528
Trained 801 batches 	Training Loss: 0.124177
Trained 851 batches 	Training Loss: 0.226129
Trained 901 batches 	Training Loss: 0.171843
Trained 951 batches 	Training Loss: 0.157069
Trained 1001 batches 	Training Loss: 0.180754
Trained 1051 batches 	Training Loss: 0.121104
Trained 1101 batches 	Training Loss: 0.174442
Trained 1151 batches 	Training Loss: 0.143670
Trained 1201 batches 	Training Loss: 0.161065
Trained 1251 batches 	Training Loss: 0.116289
Trained 1301 batches 	Training Loss: 0.199271
Trained 1351 batches 	Training Loss: 0.053337
Trained 1401 batches 	Training Loss: 0.148642
Trained 1451 batches 	Training Loss: 0.163465
Trained 1501 batches 	Training Loss: 0.213786
Trained 1551 batches 	Training Loss: 0.158515
Trained 1601 batches 	Training Loss: 0.110500
Trained 1651 batches 	Training Loss: 0.151223
Trained 1701 batches 	Training Loss: 0.249774
Trained 1751 batches 	Training Loss: 0.167341
Trained 1801 batches 	Training Loss: 0.084352
Trained 1851 batches 	Training Loss: 0.288875
Trained 1901 batches 	Training Loss: 0.132450
Trained 1951 batches 	Training Loss: 0.153707
Trained 2001 batches 	Training Loss: 0.195458
Trained 2051 batches 	Training Loss: 0.120819
Trained 2101 batches 	Training Loss: 0.161008
Trained 2151 batches 	Training Loss: 0.181243
Trained 2201 batches 	Training Loss: 0.177119
Trained 2251 batches 	Training Loss: 0.101497
Trained 2301 batches 	Training Loss: 0.098662
Trained 2351 batches 	Training Loss: 0.156469
Trained 2401 batches 	Training Loss: 0.176457
Trained 2451 batches 	Training Loss: 0.155076
Trained 2501 batches 	Training Loss: 0.254963
Trained 2551 batches 	Training Loss: 0.124786
Trained 2601 batches 	Training Loss: 0.153741
Trained 2651 batches 	Training Loss: 0.127163
Trained 2701 batches 	Training Loss: 0.141012
Trained 2751 batches 	Training Loss: 0.138626
Trained 2801 batches 	Training Loss: 0.171648
Trained 2851 batches 	Training Loss: 0.144187
Trained 2901 batches 	Training Loss: 0.150576
Trained 2951 batches 	Training Loss: 0.106907
Trained 3001 batches 	Training Loss: 0.207359
Trained 3051 batches 	Training Loss: 0.195324
Trained 3101 batches 	Training Loss: 0.149027
Trained 3151 batches 	Training Loss: 0.169304
Trained 3201 batches 	Training Loss: 0.151693
Trained 3251 batches 	Training Loss: 0.165529
Trained 3301 batches 	Training Loss: 0.098333
Trained 3351 batches 	Training Loss: 0.145594
Trained 3401 batches 	Training Loss: 0.143863
Trained 3451 batches 	Training Loss: 0.115359
Trained 3501 batches 	Training Loss: 0.132859
Trained 3551 batches 	Training Loss: 0.125287
Trained 3601 batches 	Training Loss: 0.171886
Trained 3651 batches 	Training Loss: 0.104564
Trained 3701 batches 	Training Loss: 0.194063
Trained 3751 batches 	Training Loss: 0.104324
Trained 3801 batches 	Training Loss: 0.145029
Trained 3851 batches 	Training Loss: 0.185208
Trained 3901 batches 	Training Loss: 0.116442
Trained 3951 batches 	Training Loss: 0.107042
Trained 4001 batches 	Training Loss: 0.166999
Trained 4051 batches 	Training Loss: 0.146943
Trained 4101 batches 	Training Loss: 0.182323
Trained 4151 batches 	Training Loss: 0.105205
Trained 4201 batches 	Training Loss: 0.127098
Trained 4251 batches 	Training Loss: 0.237492
Trained 4301 batches 	Training Loss: 0.105688
Trained 4351 batches 	Training Loss: 0.202634
Trained 4401 batches 	Training Loss: 0.174317
Trained 4451 batches 	Training Loss: 0.173616
Trained 4501 batches 	Training Loss: 0.160735
Trained 4551 batches 	Training Loss: 0.125170
Trained 4601 batches 	Training Loss: 0.072430
Trained 4651 batches 	Training Loss: 0.050441
Trained 4701 batches 	Training Loss: 0.081288
Trained 4751 batches 	Training Loss: 0.157117
Trained 4801 batches 	Training Loss: 0.142139
Trained 4851 batches 	Training Loss: 0.163681
Trained 4901 batches 	Training Loss: 0.099804
Epoch: 7 	Training Loss: 0.151079
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.798
The AUROC of Atelectasis is 0.7758602257729948
The AUROC of Cardiomegaly is 0.8969183547463233
The AUROC of Effusion is 0.8767382419314065
The AUROC of Infiltration is 0.6952819441754958
The AUROC of Mass is 0.7935837098559768
The AUROC of Nodule is 0.7090734236673379
The AUROC of Pneumonia is 0.7365581604663123
The AUROC of Pneumothorax is 0.8389208758490491
The AUROC of Consolidation is 0.7591483681391469
The AUROC of Edema is 0.8983956002118785
The AUROC of Emphysema is 0.8329490984465722
The AUROC of Fibrosis is 0.7585616502628806
The AUROC of Pleural_Thickening is 0.7705321318208502
The AUROC of Hernia is 0.8323538530435082
Started epoch 8
Trained 1 batches 	Training Loss: 0.127505
Trained 51 batches 	Training Loss: 0.156783
Trained 101 batches 	Training Loss: 0.136860
Trained 151 batches 	Training Loss: 0.149879
Trained 201 batches 	Training Loss: 0.123890
Trained 251 batches 	Training Loss: 0.100532
Trained 301 batches 	Training Loss: 0.209681
Trained 351 batches 	Training Loss: 0.120904
Trained 401 batches 	Training Loss: 0.136947
Trained 451 batches 	Training Loss: 0.205241
Trained 501 batches 	Training Loss: 0.105972
Trained 551 batches 	Training Loss: 0.111645
Trained 601 batches 	Training Loss: 0.163801
Trained 651 batches 	Training Loss: 0.134876
Trained 701 batches 	Training Loss: 0.140986
Trained 751 batches 	Training Loss: 0.198389
Trained 801 batches 	Training Loss: 0.194243
Trained 851 batches 	Training Loss: 0.214263
Trained 901 batches 	Training Loss: 0.158782
Trained 951 batches 	Training Loss: 0.172922
Trained 1001 batches 	Training Loss: 0.138029
Trained 1051 batches 	Training Loss: 0.146482
Trained 1101 batches 	Training Loss: 0.123010
Trained 1151 batches 	Training Loss: 0.172834
Trained 1201 batches 	Training Loss: 0.158750
Trained 1251 batches 	Training Loss: 0.125416
Trained 1301 batches 	Training Loss: 0.157236
Trained 1351 batches 	Training Loss: 0.142988
Trained 1401 batches 	Training Loss: 0.064186
Trained 1451 batches 	Training Loss: 0.141916
Trained 1501 batches 	Training Loss: 0.150392
Trained 1551 batches 	Training Loss: 0.096135
Trained 1601 batches 	Training Loss: 0.132755
Trained 1651 batches 	Training Loss: 0.176195
Trained 1701 batches 	Training Loss: 0.120519
Trained 1751 batches 	Training Loss: 0.195506
Trained 1801 batches 	Training Loss: 0.149504
Trained 1851 batches 	Training Loss: 0.096571
Trained 1901 batches 	Training Loss: 0.179550
Trained 1951 batches 	Training Loss: 0.164886
Trained 2001 batches 	Training Loss: 0.141806
Trained 2051 batches 	Training Loss: 0.140511
Trained 2101 batches 	Training Loss: 0.138956
Trained 2151 batches 	Training Loss: 0.126782
Trained 2201 batches 	Training Loss: 0.139881
Trained 2251 batches 	Training Loss: 0.248010
Trained 2301 batches 	Training Loss: 0.124338
Trained 2351 batches 	Training Loss: 0.151094
Trained 2401 batches 	Training Loss: 0.085012
Trained 2451 batches 	Training Loss: 0.080306
Trained 2501 batches 	Training Loss: 0.170344
Trained 2551 batches 	Training Loss: 0.191056
Trained 2601 batches 	Training Loss: 0.115324
Trained 2651 batches 	Training Loss: 0.157900
Trained 2701 batches 	Training Loss: 0.090881
Trained 2751 batches 	Training Loss: 0.108344
Trained 2801 batches 	Training Loss: 0.126743
Trained 2851 batches 	Training Loss: 0.157820
Trained 2901 batches 	Training Loss: 0.154128
Trained 2951 batches 	Training Loss: 0.225869
Trained 3001 batches 	Training Loss: 0.081658
Trained 3051 batches 	Training Loss: 0.110518
Trained 3101 batches 	Training Loss: 0.127870
Trained 3151 batches 	Training Loss: 0.178137
Trained 3201 batches 	Training Loss: 0.131425
Trained 3251 batches 	Training Loss: 0.181730
Trained 3301 batches 	Training Loss: 0.124842
Trained 3351 batches 	Training Loss: 0.143578
Trained 3401 batches 	Training Loss: 0.097990
Trained 3451 batches 	Training Loss: 0.191894
Trained 3501 batches 	Training Loss: 0.159217
Trained 3551 batches 	Training Loss: 0.121155
Trained 3601 batches 	Training Loss: 0.206941
Trained 3651 batches 	Training Loss: 0.197370
Trained 3701 batches 	Training Loss: 0.109061
Trained 3751 batches 	Training Loss: 0.112327
Trained 3801 batches 	Training Loss: 0.117847
Trained 3851 batches 	Training Loss: 0.120861
Trained 3901 batches 	Training Loss: 0.171075
Trained 3951 batches 	Training Loss: 0.082280
Trained 4001 batches 	Training Loss: 0.183753
Trained 4051 batches 	Training Loss: 0.148875
Trained 4101 batches 	Training Loss: 0.178661
Trained 4151 batches 	Training Loss: 0.165317
Trained 4201 batches 	Training Loss: 0.087688
Trained 4251 batches 	Training Loss: 0.186258
Trained 4301 batches 	Training Loss: 0.113774
Trained 4351 batches 	Training Loss: 0.142837
Trained 4401 batches 	Training Loss: 0.161146
Trained 4451 batches 	Training Loss: 0.142286
Trained 4501 batches 	Training Loss: 0.164270
Trained 4551 batches 	Training Loss: 0.159294
Trained 4601 batches 	Training Loss: 0.164264
Trained 4651 batches 	Training Loss: 0.165807
Trained 4701 batches 	Training Loss: 0.094772
Trained 4751 batches 	Training Loss: 0.093986
Trained 4801 batches 	Training Loss: 0.172261
Trained 4851 batches 	Training Loss: 0.135512
Trained 4901 batches 	Training Loss: 0.194084
Epoch: 8 	Training Loss: 0.149889
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.799
The AUROC of Atelectasis is 0.767139201329392
The AUROC of Cardiomegaly is 0.8997402063790768
The AUROC of Effusion is 0.8777044987794008
The AUROC of Infiltration is 0.6944348391380122
The AUROC of Mass is 0.8065308691184337
The AUROC of Nodule is 0.7098309394322698
The AUROC of Pneumonia is 0.7314025087147838
The AUROC of Pneumothorax is 0.8444207929687106
The AUROC of Consolidation is 0.7505353821952182
The AUROC of Edema is 0.9001291134211746
The AUROC of Emphysema is 0.844690712998622
The AUROC of Fibrosis is 0.7505781764852553
The AUROC of Pleural_Thickening is 0.7677970236289896
The AUROC of Hernia is 0.8351303868545248
Started epoch 9
Trained 1 batches 	Training Loss: 0.215116
Trained 51 batches 	Training Loss: 0.183353
Trained 101 batches 	Training Loss: 0.233951
Trained 151 batches 	Training Loss: 0.129293
Trained 201 batches 	Training Loss: 0.128614
Trained 251 batches 	Training Loss: 0.176830
Trained 301 batches 	Training Loss: 0.228715
Trained 351 batches 	Training Loss: 0.131431
Trained 401 batches 	Training Loss: 0.111573
Trained 451 batches 	Training Loss: 0.134614
Trained 501 batches 	Training Loss: 0.153663
Trained 551 batches 	Training Loss: 0.153113
Trained 601 batches 	Training Loss: 0.228298
Trained 651 batches 	Training Loss: 0.159246
Trained 701 batches 	Training Loss: 0.145932
Trained 751 batches 	Training Loss: 0.138912
Trained 801 batches 	Training Loss: 0.144506
Trained 851 batches 	Training Loss: 0.220659
Trained 901 batches 	Training Loss: 0.110858
Trained 951 batches 	Training Loss: 0.104191
Trained 1001 batches 	Training Loss: 0.117323
Trained 1051 batches 	Training Loss: 0.100355
Trained 1101 batches 	Training Loss: 0.072670
Trained 1151 batches 	Training Loss: 0.127131
Trained 1201 batches 	Training Loss: 0.169534
Trained 1251 batches 	Training Loss: 0.120433
Trained 1301 batches 	Training Loss: 0.132305
Trained 1351 batches 	Training Loss: 0.157351
Trained 1401 batches 	Training Loss: 0.185647
Trained 1451 batches 	Training Loss: 0.163084
Trained 1501 batches 	Training Loss: 0.093855
Trained 1551 batches 	Training Loss: 0.167652
Trained 1601 batches 	Training Loss: 0.207565
Trained 1651 batches 	Training Loss: 0.189349
Trained 1701 batches 	Training Loss: 0.184823
Trained 1751 batches 	Training Loss: 0.131171
Trained 1801 batches 	Training Loss: 0.184308
Trained 1851 batches 	Training Loss: 0.187746
Trained 1901 batches 	Training Loss: 0.229624
Trained 1951 batches 	Training Loss: 0.183726
Trained 2001 batches 	Training Loss: 0.245154
Trained 2051 batches 	Training Loss: 0.093696
Trained 2101 batches 	Training Loss: 0.128177
Trained 2151 batches 	Training Loss: 0.060339
Trained 2201 batches 	Training Loss: 0.132264
Trained 2251 batches 	Training Loss: 0.201713
Trained 2301 batches 	Training Loss: 0.113118
Trained 2351 batches 	Training Loss: 0.078130
Trained 2401 batches 	Training Loss: 0.139350
Trained 2451 batches 	Training Loss: 0.142504
Trained 2501 batches 	Training Loss: 0.101643
Trained 2551 batches 	Training Loss: 0.119534
Trained 2601 batches 	Training Loss: 0.104948
Trained 2651 batches 	Training Loss: 0.204561
Trained 2701 batches 	Training Loss: 0.133209
Trained 2751 batches 	Training Loss: 0.093820
Trained 2801 batches 	Training Loss: 0.178597
Trained 2851 batches 	Training Loss: 0.140845
Trained 2901 batches 	Training Loss: 0.084130
Trained 2951 batches 	Training Loss: 0.100306
Trained 3001 batches 	Training Loss: 0.270104
Trained 3051 batches 	Training Loss: 0.096378
Trained 3101 batches 	Training Loss: 0.081352
Trained 3151 batches 	Training Loss: 0.120034
Trained 3201 batches 	Training Loss: 0.163765
Trained 3251 batches 	Training Loss: 0.153714
Trained 3301 batches 	Training Loss: 0.133391
Trained 3351 batches 	Training Loss: 0.203461
Trained 3401 batches 	Training Loss: 0.174107
Trained 3451 batches 	Training Loss: 0.096346
Trained 3501 batches 	Training Loss: 0.119165
Trained 3551 batches 	Training Loss: 0.160554
Trained 3601 batches 	Training Loss: 0.156430
Trained 3651 batches 	Training Loss: 0.161113
Trained 3701 batches 	Training Loss: 0.177175
Trained 3751 batches 	Training Loss: 0.120311
Trained 3801 batches 	Training Loss: 0.115395
Trained 3851 batches 	Training Loss: 0.099501
Trained 3901 batches 	Training Loss: 0.143870
Trained 3951 batches 	Training Loss: 0.093782
Trained 4001 batches 	Training Loss: 0.168441
Trained 4051 batches 	Training Loss: 0.198078
Trained 4101 batches 	Training Loss: 0.143349
Trained 4151 batches 	Training Loss: 0.091776
Trained 4201 batches 	Training Loss: 0.149067
Trained 4251 batches 	Training Loss: 0.154171
Trained 4301 batches 	Training Loss: 0.106447
Trained 4351 batches 	Training Loss: 0.141141
Trained 4401 batches 	Training Loss: 0.152916
Trained 4451 batches 	Training Loss: 0.131964
Trained 4501 batches 	Training Loss: 0.088173
Trained 4551 batches 	Training Loss: 0.067587
Trained 4601 batches 	Training Loss: 0.156347
Trained 4651 batches 	Training Loss: 0.169372
Trained 4701 batches 	Training Loss: 0.134629
Trained 4751 batches 	Training Loss: 0.239974
Trained 4801 batches 	Training Loss: 0.067559
Trained 4851 batches 	Training Loss: 0.139187
Trained 4901 batches 	Training Loss: 0.201541
Epoch: 9 	Training Loss: 0.148759
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.803
The AUROC of Atelectasis is 0.775903955776349
The AUROC of Cardiomegaly is 0.9101507955833237
The AUROC of Effusion is 0.8762170839583945
The AUROC of Infiltration is 0.6911759384768597
The AUROC of Mass is 0.8078693342832475
The AUROC of Nodule is 0.7234261286578244
The AUROC of Pneumonia is 0.7399529973141322
The AUROC of Pneumothorax is 0.8572584670806888
The AUROC of Consolidation is 0.7552119158164239
The AUROC of Edema is 0.9008843441700325
The AUROC of Emphysema is 0.8503187971828664
The AUROC of Fibrosis is 0.7543040835641303
The AUROC of Pleural_Thickening is 0.7551880916205146
The AUROC of Hernia is 0.8452237417754659
Started epoch 10
Trained 1 batches 	Training Loss: 0.211084
Trained 51 batches 	Training Loss: 0.131248
Trained 101 batches 	Training Loss: 0.154751
Trained 151 batches 	Training Loss: 0.148950
Trained 201 batches 	Training Loss: 0.148772
Trained 251 batches 	Training Loss: 0.149437
Trained 301 batches 	Training Loss: 0.140503
Trained 351 batches 	Training Loss: 0.138476
Trained 401 batches 	Training Loss: 0.169673
Trained 451 batches 	Training Loss: 0.108590
Trained 501 batches 	Training Loss: 0.136726
Trained 551 batches 	Training Loss: 0.171746
Trained 601 batches 	Training Loss: 0.163044
Trained 651 batches 	Training Loss: 0.184957
Trained 701 batches 	Training Loss: 0.140665
Trained 751 batches 	Training Loss: 0.077048
Trained 801 batches 	Training Loss: 0.135552
Trained 851 batches 	Training Loss: 0.142867
Trained 901 batches 	Training Loss: 0.095377
Trained 951 batches 	Training Loss: 0.185997
Trained 1001 batches 	Training Loss: 0.168132
Trained 1051 batches 	Training Loss: 0.129211
Trained 1101 batches 	Training Loss: 0.093516
Trained 1151 batches 	Training Loss: 0.142335
Trained 1201 batches 	Training Loss: 0.219270
Trained 1251 batches 	Training Loss: 0.183029
Trained 1301 batches 	Training Loss: 0.117178
Trained 1351 batches 	Training Loss: 0.153090
Trained 1401 batches 	Training Loss: 0.212377
Trained 1451 batches 	Training Loss: 0.164510
Trained 1501 batches 	Training Loss: 0.167505
Trained 1551 batches 	Training Loss: 0.176135
Trained 1601 batches 	Training Loss: 0.174591
Trained 1651 batches 	Training Loss: 0.205082
Trained 1701 batches 	Training Loss: 0.167808
Trained 1751 batches 	Training Loss: 0.144128
Trained 1801 batches 	Training Loss: 0.175485
Trained 1851 batches 	Training Loss: 0.200499
Trained 1901 batches 	Training Loss: 0.090424
Trained 1951 batches 	Training Loss: 0.133326
Trained 2001 batches 	Training Loss: 0.178496
Trained 2051 batches 	Training Loss: 0.176741
Trained 2101 batches 	Training Loss: 0.154478
Trained 2151 batches 	Training Loss: 0.156399
Trained 2201 batches 	Training Loss: 0.126090
Trained 2251 batches 	Training Loss: 0.175866
Trained 2301 batches 	Training Loss: 0.154912
Trained 2351 batches 	Training Loss: 0.137784
Trained 2401 batches 	Training Loss: 0.131894
Trained 2451 batches 	Training Loss: 0.163108
Trained 2501 batches 	Training Loss: 0.112244
Trained 2551 batches 	Training Loss: 0.101965
Trained 2601 batches 	Training Loss: 0.116528
Trained 2651 batches 	Training Loss: 0.161081
Trained 2701 batches 	Training Loss: 0.136732
Trained 2751 batches 	Training Loss: 0.178007
Trained 2801 batches 	Training Loss: 0.200408
Trained 2851 batches 	Training Loss: 0.138919
Trained 2901 batches 	Training Loss: 0.076620
Trained 2951 batches 	Training Loss: 0.187735
Trained 3001 batches 	Training Loss: 0.162895
Trained 3051 batches 	Training Loss: 0.153480
Trained 3101 batches 	Training Loss: 0.111457
Trained 3151 batches 	Training Loss: 0.157523
Trained 3201 batches 	Training Loss: 0.107612
Trained 3251 batches 	Training Loss: 0.129688
Trained 3301 batches 	Training Loss: 0.135609
Trained 3351 batches 	Training Loss: 0.140938
Trained 3401 batches 	Training Loss: 0.116329
Trained 3451 batches 	Training Loss: 0.137683
Trained 3501 batches 	Training Loss: 0.133802
Trained 3551 batches 	Training Loss: 0.135622
Trained 3601 batches 	Training Loss: 0.126930
Trained 3651 batches 	Training Loss: 0.063884
Trained 3701 batches 	Training Loss: 0.171250
Trained 3751 batches 	Training Loss: 0.227769
Trained 3801 batches 	Training Loss: 0.148102
Trained 3851 batches 	Training Loss: 0.123917
Trained 3901 batches 	Training Loss: 0.173109
Trained 3951 batches 	Training Loss: 0.198817
Trained 4001 batches 	Training Loss: 0.056629
Trained 4051 batches 	Training Loss: 0.184207
Trained 4101 batches 	Training Loss: 0.211739
Trained 4151 batches 	Training Loss: 0.124858
Trained 4201 batches 	Training Loss: 0.129840
Trained 4251 batches 	Training Loss: 0.131906
Trained 4301 batches 	Training Loss: 0.146651
Trained 4351 batches 	Training Loss: 0.131437
Trained 4401 batches 	Training Loss: 0.102773
Trained 4451 batches 	Training Loss: 0.107348
Trained 4501 batches 	Training Loss: 0.154637
Trained 4551 batches 	Training Loss: 0.124653
Trained 4601 batches 	Training Loss: 0.122812
Trained 4651 batches 	Training Loss: 0.161890
Trained 4701 batches 	Training Loss: 0.126900
Trained 4751 batches 	Training Loss: 0.182900
Trained 4801 batches 	Training Loss: 0.119899
Trained 4851 batches 	Training Loss: 0.169464
Trained 4901 batches 	Training Loss: 0.251855
Epoch: 10 	Training Loss: 0.147423
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.802
The AUROC of Atelectasis is 0.7762559563764697
The AUROC of Cardiomegaly is 0.9022213814103641
The AUROC of Effusion is 0.8813833825078836
The AUROC of Infiltration is 0.694396110542477
The AUROC of Mass is 0.8043242058594515
The AUROC of Nodule is 0.7276611682922078
The AUROC of Pneumonia is 0.7227192744347296
The AUROC of Pneumothorax is 0.8544170475366192
The AUROC of Consolidation is 0.7544627787967951
The AUROC of Edema is 0.9021043004701053
The AUROC of Emphysema is 0.8417246699407601
The AUROC of Fibrosis is 0.757650616784143
The AUROC of Pleural_Thickening is 0.7615591528222249
The AUROC of Hernia is 0.8480737653151447
Training time lapse: 144.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.7835924091965871
The AUROC of Cardiomegaly is 0.9012076971796911
The AUROC of Effusion is 0.872475864561527
The AUROC of Infiltration is 0.7018980584566252
The AUROC of Mass is 0.8283345538149177
The AUROC of Nodule is 0.7354897784936694
The AUROC of Pneumonia is 0.7212223966861828
The AUROC of Pneumothorax is 0.8466680392048318
The AUROC of Consolidation is 0.8044639578516575
The AUROC of Edema is 0.870602829733884
The AUROC of Emphysema is 0.8979224882128694
The AUROC of Fibrosis is 0.7579643073858287
The AUROC of Pleural_Thickening is 0.7709741330050415
The AUROC of Hernia is 0.8308923913308466
