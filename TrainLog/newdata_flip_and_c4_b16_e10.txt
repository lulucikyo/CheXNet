Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.682972
Trained 51 batches 	Training Loss: 0.184368
Trained 101 batches 	Training Loss: 0.225406
Trained 151 batches 	Training Loss: 0.133577
Trained 201 batches 	Training Loss: 0.252442
Trained 251 batches 	Training Loss: 0.213191
Trained 301 batches 	Training Loss: 0.255858
Trained 351 batches 	Training Loss: 0.158115
Trained 401 batches 	Training Loss: 0.147369
Trained 451 batches 	Training Loss: 0.164325
Trained 501 batches 	Training Loss: 0.260867
Trained 551 batches 	Training Loss: 0.228760
Trained 601 batches 	Training Loss: 0.152144
Trained 651 batches 	Training Loss: 0.295405
Trained 701 batches 	Training Loss: 0.183271
Trained 751 batches 	Training Loss: 0.144339
Trained 801 batches 	Training Loss: 0.245320
Trained 851 batches 	Training Loss: 0.260825
Trained 901 batches 	Training Loss: 0.109461
Trained 951 batches 	Training Loss: 0.150297
Trained 1001 batches 	Training Loss: 0.165577
Trained 1051 batches 	Training Loss: 0.132872
Trained 1101 batches 	Training Loss: 0.152148
Trained 1151 batches 	Training Loss: 0.127554
Trained 1201 batches 	Training Loss: 0.155089
Trained 1251 batches 	Training Loss: 0.164343
Trained 1301 batches 	Training Loss: 0.188945
Trained 1351 batches 	Training Loss: 0.168822
Trained 1401 batches 	Training Loss: 0.163560
Trained 1451 batches 	Training Loss: 0.180850
Trained 1501 batches 	Training Loss: 0.244695
Trained 1551 batches 	Training Loss: 0.181132
Trained 1601 batches 	Training Loss: 0.171830
Trained 1651 batches 	Training Loss: 0.119573
Trained 1701 batches 	Training Loss: 0.169684
Trained 1751 batches 	Training Loss: 0.115002
Trained 1801 batches 	Training Loss: 0.139323
Trained 1851 batches 	Training Loss: 0.145238
Trained 1901 batches 	Training Loss: 0.262821
Trained 1951 batches 	Training Loss: 0.148668
Trained 2001 batches 	Training Loss: 0.190180
Trained 2051 batches 	Training Loss: 0.111221
Trained 2101 batches 	Training Loss: 0.171901
Trained 2151 batches 	Training Loss: 0.099298
Trained 2201 batches 	Training Loss: 0.260175
Trained 2251 batches 	Training Loss: 0.066995
Trained 2301 batches 	Training Loss: 0.151237
Trained 2351 batches 	Training Loss: 0.125863
Trained 2401 batches 	Training Loss: 0.107738
Trained 2451 batches 	Training Loss: 0.102626
Trained 2501 batches 	Training Loss: 0.159523
Trained 2551 batches 	Training Loss: 0.183567
Trained 2601 batches 	Training Loss: 0.215807
Trained 2651 batches 	Training Loss: 0.170273
Trained 2701 batches 	Training Loss: 0.135511
Trained 2751 batches 	Training Loss: 0.178725
Trained 2801 batches 	Training Loss: 0.235579
Trained 2851 batches 	Training Loss: 0.159643
Trained 2901 batches 	Training Loss: 0.243123
Trained 2951 batches 	Training Loss: 0.159876
Trained 3001 batches 	Training Loss: 0.197193
Trained 3051 batches 	Training Loss: 0.213843
Trained 3101 batches 	Training Loss: 0.174407
Trained 3151 batches 	Training Loss: 0.097410
Trained 3201 batches 	Training Loss: 0.118281
Trained 3251 batches 	Training Loss: 0.153899
Trained 3301 batches 	Training Loss: 0.148466
Trained 3351 batches 	Training Loss: 0.212065
Trained 3401 batches 	Training Loss: 0.175990
Trained 3451 batches 	Training Loss: 0.163240
Trained 3501 batches 	Training Loss: 0.210255
Trained 3551 batches 	Training Loss: 0.146139
Trained 3601 batches 	Training Loss: 0.133533
Trained 3651 batches 	Training Loss: 0.130636
Trained 3701 batches 	Training Loss: 0.247907
Trained 3751 batches 	Training Loss: 0.184045
Trained 3801 batches 	Training Loss: 0.124979
Trained 3851 batches 	Training Loss: 0.188601
Trained 3901 batches 	Training Loss: 0.205822
Trained 3951 batches 	Training Loss: 0.141013
Trained 4001 batches 	Training Loss: 0.302947
Trained 4051 batches 	Training Loss: 0.192027
Trained 4101 batches 	Training Loss: 0.199526
Trained 4151 batches 	Training Loss: 0.219142
Trained 4201 batches 	Training Loss: 0.169702
Trained 4251 batches 	Training Loss: 0.282429
Trained 4301 batches 	Training Loss: 0.197367
Trained 4351 batches 	Training Loss: 0.141863
Trained 4401 batches 	Training Loss: 0.172771
Trained 4451 batches 	Training Loss: 0.130605
Trained 4501 batches 	Training Loss: 0.185075
Trained 4551 batches 	Training Loss: 0.168871
Trained 4601 batches 	Training Loss: 0.132280
Trained 4651 batches 	Training Loss: 0.180354
Trained 4701 batches 	Training Loss: 0.218738
Trained 4751 batches 	Training Loss: 0.178281
Trained 4801 batches 	Training Loss: 0.192935
Trained 4851 batches 	Training Loss: 0.183283
Trained 4901 batches 	Training Loss: 0.116057
Epoch: 1 	Training Loss: 0.172417
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.716
The AUROC of Atelectasis is 0.7169056995318384
The AUROC of Cardiomegaly is 0.7608550022003568
The AUROC of Effusion is 0.8382404552474689
The AUROC of Infiltration is 0.6475275941243188
The AUROC of Mass is 0.7040282074167457
The AUROC of Nodule is 0.6428241112316802
The AUROC of Pneumonia is 0.6795370449739984
The AUROC of Pneumothorax is 0.7459635377124518
The AUROC of Consolidation is 0.6939458822143247
The AUROC of Edema is 0.8276021320267496
The AUROC of Emphysema is 0.6573208155201451
The AUROC of Fibrosis is 0.6757962159133258
The AUROC of Pleural_Thickening is 0.6695079411153683
The AUROC of Hernia is 0.7649775511844477
Started epoch 2
Trained 1 batches 	Training Loss: 0.129770
Trained 51 batches 	Training Loss: 0.105111
Trained 101 batches 	Training Loss: 0.151112
Trained 151 batches 	Training Loss: 0.090453
Trained 201 batches 	Training Loss: 0.091178
Trained 251 batches 	Training Loss: 0.226110
Trained 301 batches 	Training Loss: 0.155302
Trained 351 batches 	Training Loss: 0.117468
Trained 401 batches 	Training Loss: 0.162395
Trained 451 batches 	Training Loss: 0.151618
Trained 501 batches 	Training Loss: 0.186845
Trained 551 batches 	Training Loss: 0.105966
Trained 601 batches 	Training Loss: 0.153943
Trained 651 batches 	Training Loss: 0.243904
Trained 701 batches 	Training Loss: 0.141737
Trained 751 batches 	Training Loss: 0.109363
Trained 801 batches 	Training Loss: 0.088050
Trained 851 batches 	Training Loss: 0.178596
Trained 901 batches 	Training Loss: 0.184633
Trained 951 batches 	Training Loss: 0.275693
Trained 1001 batches 	Training Loss: 0.129898
Trained 1051 batches 	Training Loss: 0.096575
Trained 1101 batches 	Training Loss: 0.155883
Trained 1151 batches 	Training Loss: 0.208306
Trained 1201 batches 	Training Loss: 0.134174
Trained 1251 batches 	Training Loss: 0.222112
Trained 1301 batches 	Training Loss: 0.150204
Trained 1351 batches 	Training Loss: 0.116636
Trained 1401 batches 	Training Loss: 0.165230
Trained 1451 batches 	Training Loss: 0.149738
Trained 1501 batches 	Training Loss: 0.125528
Trained 1551 batches 	Training Loss: 0.146368
Trained 1601 batches 	Training Loss: 0.210785
Trained 1651 batches 	Training Loss: 0.236580
Trained 1701 batches 	Training Loss: 0.108440
Trained 1751 batches 	Training Loss: 0.136486
Trained 1801 batches 	Training Loss: 0.100169
Trained 1851 batches 	Training Loss: 0.129973
Trained 1901 batches 	Training Loss: 0.101630
Trained 1951 batches 	Training Loss: 0.162964
Trained 2001 batches 	Training Loss: 0.151110
Trained 2051 batches 	Training Loss: 0.131933
Trained 2101 batches 	Training Loss: 0.163074
Trained 2151 batches 	Training Loss: 0.168421
Trained 2201 batches 	Training Loss: 0.231020
Trained 2251 batches 	Training Loss: 0.128963
Trained 2301 batches 	Training Loss: 0.162922
Trained 2351 batches 	Training Loss: 0.121301
Trained 2401 batches 	Training Loss: 0.172173
Trained 2451 batches 	Training Loss: 0.153617
Trained 2501 batches 	Training Loss: 0.154879
Trained 2551 batches 	Training Loss: 0.169973
Trained 2601 batches 	Training Loss: 0.276507
Trained 2651 batches 	Training Loss: 0.212182
Trained 2701 batches 	Training Loss: 0.153399
Trained 2751 batches 	Training Loss: 0.138813
Trained 2801 batches 	Training Loss: 0.199994
Trained 2851 batches 	Training Loss: 0.218300
Trained 2901 batches 	Training Loss: 0.152297
Trained 2951 batches 	Training Loss: 0.170360
Trained 3001 batches 	Training Loss: 0.166630
Trained 3051 batches 	Training Loss: 0.155157
Trained 3101 batches 	Training Loss: 0.138491
Trained 3151 batches 	Training Loss: 0.161036
Trained 3201 batches 	Training Loss: 0.166756
Trained 3251 batches 	Training Loss: 0.133860
Trained 3301 batches 	Training Loss: 0.178101
Trained 3351 batches 	Training Loss: 0.142213
Trained 3401 batches 	Training Loss: 0.272976
Trained 3451 batches 	Training Loss: 0.125929
Trained 3501 batches 	Training Loss: 0.176432
Trained 3551 batches 	Training Loss: 0.203509
Trained 3601 batches 	Training Loss: 0.153398
Trained 3651 batches 	Training Loss: 0.151028
Trained 3701 batches 	Training Loss: 0.108777
Trained 3751 batches 	Training Loss: 0.084551
Trained 3801 batches 	Training Loss: 0.103789
Trained 3851 batches 	Training Loss: 0.121100
Trained 3901 batches 	Training Loss: 0.110901
Trained 3951 batches 	Training Loss: 0.160348
Trained 4001 batches 	Training Loss: 0.158907
Trained 4051 batches 	Training Loss: 0.126051
Trained 4101 batches 	Training Loss: 0.179850
Trained 4151 batches 	Training Loss: 0.144240
Trained 4201 batches 	Training Loss: 0.132051
Trained 4251 batches 	Training Loss: 0.156652
Trained 4301 batches 	Training Loss: 0.071561
Trained 4351 batches 	Training Loss: 0.202619
Trained 4401 batches 	Training Loss: 0.103052
Trained 4451 batches 	Training Loss: 0.085828
Trained 4501 batches 	Training Loss: 0.171243
Trained 4551 batches 	Training Loss: 0.125638
Trained 4601 batches 	Training Loss: 0.229028
Trained 4651 batches 	Training Loss: 0.157012
Trained 4701 batches 	Training Loss: 0.163633
Trained 4751 batches 	Training Loss: 0.141313
Trained 4801 batches 	Training Loss: 0.178423
Trained 4851 batches 	Training Loss: 0.135515
Trained 4901 batches 	Training Loss: 0.209688
Epoch: 2 	Training Loss: 0.163416
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.760
The AUROC of Atelectasis is 0.7576975392712283
The AUROC of Cardiomegaly is 0.8774657947020724
The AUROC of Effusion is 0.8495954032405384
The AUROC of Infiltration is 0.6745239425710254
The AUROC of Mass is 0.7470948055765961
The AUROC of Nodule is 0.6576274161064488
The AUROC of Pneumonia is 0.6997465331352267
The AUROC of Pneumothorax is 0.8071477332543755
The AUROC of Consolidation is 0.7393994325346784
The AUROC of Edema is 0.8740279249155797
The AUROC of Emphysema is 0.7475422216726142
The AUROC of Fibrosis is 0.7086317606968682
The AUROC of Pleural_Thickening is 0.7386809276904038
The AUROC of Hernia is 0.7575459023734886
Started epoch 3
Trained 1 batches 	Training Loss: 0.186693
Trained 51 batches 	Training Loss: 0.131249
Trained 101 batches 	Training Loss: 0.140715
Trained 151 batches 	Training Loss: 0.158369
Trained 201 batches 	Training Loss: 0.263004
Trained 251 batches 	Training Loss: 0.211206
Trained 301 batches 	Training Loss: 0.174219
Trained 351 batches 	Training Loss: 0.187757
Trained 401 batches 	Training Loss: 0.175180
Trained 451 batches 	Training Loss: 0.123164
Trained 501 batches 	Training Loss: 0.091356
Trained 551 batches 	Training Loss: 0.088804
Trained 601 batches 	Training Loss: 0.122457
Trained 651 batches 	Training Loss: 0.168414
Trained 701 batches 	Training Loss: 0.165080
Trained 751 batches 	Training Loss: 0.125032
Trained 801 batches 	Training Loss: 0.094883
Trained 851 batches 	Training Loss: 0.126743
Trained 901 batches 	Training Loss: 0.176078
Trained 951 batches 	Training Loss: 0.165517
Trained 1001 batches 	Training Loss: 0.184057
Trained 1051 batches 	Training Loss: 0.182861
Trained 1101 batches 	Training Loss: 0.142623
Trained 1151 batches 	Training Loss: 0.205414
Trained 1201 batches 	Training Loss: 0.143877
Trained 1251 batches 	Training Loss: 0.151114
Trained 1301 batches 	Training Loss: 0.160593
Trained 1351 batches 	Training Loss: 0.174723
Trained 1401 batches 	Training Loss: 0.158732
Trained 1451 batches 	Training Loss: 0.144116
Trained 1501 batches 	Training Loss: 0.131054
Trained 1551 batches 	Training Loss: 0.144365
Trained 1601 batches 	Training Loss: 0.143640
Trained 1651 batches 	Training Loss: 0.107408
Trained 1701 batches 	Training Loss: 0.163102
Trained 1751 batches 	Training Loss: 0.131521
Trained 1801 batches 	Training Loss: 0.145422
Trained 1851 batches 	Training Loss: 0.198637
Trained 1901 batches 	Training Loss: 0.154986
Trained 1951 batches 	Training Loss: 0.200408
Trained 2001 batches 	Training Loss: 0.112433
Trained 2051 batches 	Training Loss: 0.122181
Trained 2101 batches 	Training Loss: 0.135128
Trained 2151 batches 	Training Loss: 0.087785
Trained 2201 batches 	Training Loss: 0.182537
Trained 2251 batches 	Training Loss: 0.131235
Trained 2301 batches 	Training Loss: 0.238601
Trained 2351 batches 	Training Loss: 0.155973
Trained 2401 batches 	Training Loss: 0.149207
Trained 2451 batches 	Training Loss: 0.156771
Trained 2501 batches 	Training Loss: 0.214341
Trained 2551 batches 	Training Loss: 0.153726
Trained 2601 batches 	Training Loss: 0.194836
Trained 2651 batches 	Training Loss: 0.182489
Trained 2701 batches 	Training Loss: 0.119789
Trained 2751 batches 	Training Loss: 0.175639
Trained 2801 batches 	Training Loss: 0.133236
Trained 2851 batches 	Training Loss: 0.143525
Trained 2901 batches 	Training Loss: 0.173679
Trained 2951 batches 	Training Loss: 0.138551
Trained 3001 batches 	Training Loss: 0.304889
Trained 3051 batches 	Training Loss: 0.177940
Trained 3101 batches 	Training Loss: 0.153714
Trained 3151 batches 	Training Loss: 0.155028
Trained 3201 batches 	Training Loss: 0.156623
Trained 3251 batches 	Training Loss: 0.088700
Trained 3301 batches 	Training Loss: 0.140289
Trained 3351 batches 	Training Loss: 0.132012
Trained 3401 batches 	Training Loss: 0.192236
Trained 3451 batches 	Training Loss: 0.190957
Trained 3501 batches 	Training Loss: 0.154191
Trained 3551 batches 	Training Loss: 0.157886
Trained 3601 batches 	Training Loss: 0.139077
Trained 3651 batches 	Training Loss: 0.075551
Trained 3701 batches 	Training Loss: 0.203873
Trained 3751 batches 	Training Loss: 0.215574
Trained 3801 batches 	Training Loss: 0.145946
Trained 3851 batches 	Training Loss: 0.118603
Trained 3901 batches 	Training Loss: 0.152184
Trained 3951 batches 	Training Loss: 0.088472
Trained 4001 batches 	Training Loss: 0.202829
Trained 4051 batches 	Training Loss: 0.148853
Trained 4101 batches 	Training Loss: 0.118074
Trained 4151 batches 	Training Loss: 0.185316
Trained 4201 batches 	Training Loss: 0.138250
Trained 4251 batches 	Training Loss: 0.117391
Trained 4301 batches 	Training Loss: 0.086861
Trained 4351 batches 	Training Loss: 0.164847
Trained 4401 batches 	Training Loss: 0.155082
Trained 4451 batches 	Training Loss: 0.144289
Trained 4501 batches 	Training Loss: 0.104455
Trained 4551 batches 	Training Loss: 0.259194
Trained 4601 batches 	Training Loss: 0.244846
Trained 4651 batches 	Training Loss: 0.123886
Trained 4701 batches 	Training Loss: 0.232843
Trained 4751 batches 	Training Loss: 0.224546
Trained 4801 batches 	Training Loss: 0.126366
Trained 4851 batches 	Training Loss: 0.136260
Trained 4901 batches 	Training Loss: 0.135439
Epoch: 3 	Training Loss: 0.158737
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.772
The AUROC of Atelectasis is 0.7622311566248836
The AUROC of Cardiomegaly is 0.8699346987247536
The AUROC of Effusion is 0.8645438760616813
The AUROC of Infiltration is 0.6755282580430995
The AUROC of Mass is 0.7777172862296001
The AUROC of Nodule is 0.682339067138384
The AUROC of Pneumonia is 0.703025887193554
The AUROC of Pneumothorax is 0.8141286619190783
The AUROC of Consolidation is 0.7411958487778161
The AUROC of Edema is 0.886963682712044
The AUROC of Emphysema is 0.7800402489724293
The AUROC of Fibrosis is 0.7211401055812195
The AUROC of Pleural_Thickening is 0.7515356104547704
The AUROC of Hernia is 0.7718695098005444
Started epoch 4
Trained 1 batches 	Training Loss: 0.195786
Trained 51 batches 	Training Loss: 0.135798
Trained 101 batches 	Training Loss: 0.150394
Trained 151 batches 	Training Loss: 0.120453
Trained 201 batches 	Training Loss: 0.140953
Trained 251 batches 	Training Loss: 0.172642
Trained 301 batches 	Training Loss: 0.157270
Trained 351 batches 	Training Loss: 0.216701
Trained 401 batches 	Training Loss: 0.170508
Trained 451 batches 	Training Loss: 0.184038
Trained 501 batches 	Training Loss: 0.192683
Trained 551 batches 	Training Loss: 0.138366
Trained 601 batches 	Training Loss: 0.162836
Trained 651 batches 	Training Loss: 0.188481
Trained 701 batches 	Training Loss: 0.124549
Trained 751 batches 	Training Loss: 0.127246
Trained 801 batches 	Training Loss: 0.130310
Trained 851 batches 	Training Loss: 0.171404
Trained 901 batches 	Training Loss: 0.166088
Trained 951 batches 	Training Loss: 0.216848
Trained 1001 batches 	Training Loss: 0.230069
Trained 1051 batches 	Training Loss: 0.114889
Trained 1101 batches 	Training Loss: 0.100250
Trained 1151 batches 	Training Loss: 0.060342
Trained 1201 batches 	Training Loss: 0.233776
Trained 1251 batches 	Training Loss: 0.132249
Trained 1301 batches 	Training Loss: 0.190293
Trained 1351 batches 	Training Loss: 0.189486
Trained 1401 batches 	Training Loss: 0.182311
Trained 1451 batches 	Training Loss: 0.090571
Trained 1501 batches 	Training Loss: 0.117593
Trained 1551 batches 	Training Loss: 0.198371
Trained 1601 batches 	Training Loss: 0.167162
Trained 1651 batches 	Training Loss: 0.140622
Trained 1701 batches 	Training Loss: 0.183761
Trained 1751 batches 	Training Loss: 0.083852
Trained 1801 batches 	Training Loss: 0.165687
Trained 1851 batches 	Training Loss: 0.225860
Trained 1901 batches 	Training Loss: 0.128360
Trained 1951 batches 	Training Loss: 0.227732
Trained 2001 batches 	Training Loss: 0.158462
Trained 2051 batches 	Training Loss: 0.147276
Trained 2101 batches 	Training Loss: 0.144045
Trained 2151 batches 	Training Loss: 0.156854
Trained 2201 batches 	Training Loss: 0.264234
Trained 2251 batches 	Training Loss: 0.213110
Trained 2301 batches 	Training Loss: 0.153838
Trained 2351 batches 	Training Loss: 0.191382
Trained 2401 batches 	Training Loss: 0.124562
Trained 2451 batches 	Training Loss: 0.109667
Trained 2501 batches 	Training Loss: 0.154683
Trained 2551 batches 	Training Loss: 0.222874
Trained 2601 batches 	Training Loss: 0.223457
Trained 2651 batches 	Training Loss: 0.127197
Trained 2701 batches 	Training Loss: 0.150921
Trained 2751 batches 	Training Loss: 0.168167
Trained 2801 batches 	Training Loss: 0.126448
Trained 2851 batches 	Training Loss: 0.237370
Trained 2901 batches 	Training Loss: 0.213705
Trained 2951 batches 	Training Loss: 0.153406
Trained 3001 batches 	Training Loss: 0.126395
Trained 3051 batches 	Training Loss: 0.180856
Trained 3101 batches 	Training Loss: 0.148946
Trained 3151 batches 	Training Loss: 0.114612
Trained 3201 batches 	Training Loss: 0.104768
Trained 3251 batches 	Training Loss: 0.207472
Trained 3301 batches 	Training Loss: 0.258257
Trained 3351 batches 	Training Loss: 0.186402
Trained 3401 batches 	Training Loss: 0.129241
Trained 3451 batches 	Training Loss: 0.098003
Trained 3501 batches 	Training Loss: 0.171428
Trained 3551 batches 	Training Loss: 0.088708
Trained 3601 batches 	Training Loss: 0.164021
Trained 3651 batches 	Training Loss: 0.105797
Trained 3701 batches 	Training Loss: 0.164881
Trained 3751 batches 	Training Loss: 0.197416
Trained 3801 batches 	Training Loss: 0.133501
Trained 3851 batches 	Training Loss: 0.179937
Trained 3901 batches 	Training Loss: 0.191325
Trained 3951 batches 	Training Loss: 0.140086
Trained 4001 batches 	Training Loss: 0.217869
Trained 4051 batches 	Training Loss: 0.239999
Trained 4101 batches 	Training Loss: 0.171677
Trained 4151 batches 	Training Loss: 0.178592
Trained 4201 batches 	Training Loss: 0.118862
Trained 4251 batches 	Training Loss: 0.099257
Trained 4301 batches 	Training Loss: 0.137637
Trained 4351 batches 	Training Loss: 0.116406
Trained 4401 batches 	Training Loss: 0.167878
Trained 4451 batches 	Training Loss: 0.201402
Trained 4501 batches 	Training Loss: 0.146252
Trained 4551 batches 	Training Loss: 0.126693
Trained 4601 batches 	Training Loss: 0.111225
Trained 4651 batches 	Training Loss: 0.134611
Trained 4701 batches 	Training Loss: 0.146884
Trained 4751 batches 	Training Loss: 0.213861
Trained 4801 batches 	Training Loss: 0.150033
Trained 4851 batches 	Training Loss: 0.092656
Trained 4901 batches 	Training Loss: 0.166618
Epoch: 4 	Training Loss: 0.155787
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.779
The AUROC of Atelectasis is 0.7616939084710288
The AUROC of Cardiomegaly is 0.8986564705091193
The AUROC of Effusion is 0.8757939798541186
The AUROC of Infiltration is 0.6795739854491133
The AUROC of Mass is 0.7867533330861438
The AUROC of Nodule is 0.7074716395315075
The AUROC of Pneumonia is 0.711088252662057
The AUROC of Pneumothorax is 0.8365579162744086
The AUROC of Consolidation is 0.7440275031207408
The AUROC of Edema is 0.8922022942461763
The AUROC of Emphysema is 0.8078759318800126
The AUROC of Fibrosis is 0.7279540890603593
The AUROC of Pleural_Thickening is 0.7510827148545656
The AUROC of Hernia is 0.7301571991227164
Started epoch 5
Trained 1 batches 	Training Loss: 0.141580
Trained 51 batches 	Training Loss: 0.119349
Trained 101 batches 	Training Loss: 0.126770
Trained 151 batches 	Training Loss: 0.131761
Trained 201 batches 	Training Loss: 0.137035
Trained 251 batches 	Training Loss: 0.177945
Trained 301 batches 	Training Loss: 0.184580
Trained 351 batches 	Training Loss: 0.141396
Trained 401 batches 	Training Loss: 0.091047
Trained 451 batches 	Training Loss: 0.108603
Trained 501 batches 	Training Loss: 0.123524
Trained 551 batches 	Training Loss: 0.167334
Trained 601 batches 	Training Loss: 0.113239
Trained 651 batches 	Training Loss: 0.101712
Trained 701 batches 	Training Loss: 0.087185
Trained 751 batches 	Training Loss: 0.144648
Trained 801 batches 	Training Loss: 0.114968
Trained 851 batches 	Training Loss: 0.201583
Trained 901 batches 	Training Loss: 0.151286
Trained 951 batches 	Training Loss: 0.196943
Trained 1001 batches 	Training Loss: 0.137492
Trained 1051 batches 	Training Loss: 0.177011
Trained 1101 batches 	Training Loss: 0.152852
Trained 1151 batches 	Training Loss: 0.120878
Trained 1201 batches 	Training Loss: 0.205234
Trained 1251 batches 	Training Loss: 0.192133
Trained 1301 batches 	Training Loss: 0.186052
Trained 1351 batches 	Training Loss: 0.183611
Trained 1401 batches 	Training Loss: 0.102374
Trained 1451 batches 	Training Loss: 0.121694
Trained 1501 batches 	Training Loss: 0.118612
Trained 1551 batches 	Training Loss: 0.216355
Trained 1601 batches 	Training Loss: 0.128132
Trained 1651 batches 	Training Loss: 0.097288
Trained 1701 batches 	Training Loss: 0.102645
Trained 1751 batches 	Training Loss: 0.126443
Trained 1801 batches 	Training Loss: 0.137755
Trained 1851 batches 	Training Loss: 0.146877
Trained 1901 batches 	Training Loss: 0.180142
Trained 1951 batches 	Training Loss: 0.142656
Trained 2001 batches 	Training Loss: 0.160588
Trained 2051 batches 	Training Loss: 0.237470
Trained 2101 batches 	Training Loss: 0.232584
Trained 2151 batches 	Training Loss: 0.146754
Trained 2201 batches 	Training Loss: 0.097718
Trained 2251 batches 	Training Loss: 0.139538
Trained 2301 batches 	Training Loss: 0.210887
Trained 2351 batches 	Training Loss: 0.099745
Trained 2401 batches 	Training Loss: 0.145136
Trained 2451 batches 	Training Loss: 0.238246
Trained 2501 batches 	Training Loss: 0.310297
Trained 2551 batches 	Training Loss: 0.125362
Trained 2601 batches 	Training Loss: 0.123193
Trained 2651 batches 	Training Loss: 0.199831
Trained 2701 batches 	Training Loss: 0.138157
Trained 2751 batches 	Training Loss: 0.167495
Trained 2801 batches 	Training Loss: 0.149322
Trained 2851 batches 	Training Loss: 0.190170
Trained 2901 batches 	Training Loss: 0.140525
Trained 2951 batches 	Training Loss: 0.137541
Trained 3001 batches 	Training Loss: 0.109017
Trained 3051 batches 	Training Loss: 0.142259
Trained 3101 batches 	Training Loss: 0.189568
Trained 3151 batches 	Training Loss: 0.110898
Trained 3201 batches 	Training Loss: 0.213445
Trained 3251 batches 	Training Loss: 0.115126
Trained 3301 batches 	Training Loss: 0.070271
Trained 3351 batches 	Training Loss: 0.131517
Trained 3401 batches 	Training Loss: 0.141387
Trained 3451 batches 	Training Loss: 0.169419
Trained 3501 batches 	Training Loss: 0.169994
Trained 3551 batches 	Training Loss: 0.142375
Trained 3601 batches 	Training Loss: 0.163578
Trained 3651 batches 	Training Loss: 0.208349
Trained 3701 batches 	Training Loss: 0.105925
Trained 3751 batches 	Training Loss: 0.150905
Trained 3801 batches 	Training Loss: 0.167116
Trained 3851 batches 	Training Loss: 0.361180
Trained 3901 batches 	Training Loss: 0.235201
Trained 3951 batches 	Training Loss: 0.127415
Trained 4001 batches 	Training Loss: 0.160293
Trained 4051 batches 	Training Loss: 0.176032
Trained 4101 batches 	Training Loss: 0.226565
Trained 4151 batches 	Training Loss: 0.137411
Trained 4201 batches 	Training Loss: 0.186331
Trained 4251 batches 	Training Loss: 0.144440
Trained 4301 batches 	Training Loss: 0.149464
Trained 4351 batches 	Training Loss: 0.160855
Trained 4401 batches 	Training Loss: 0.134872
Trained 4451 batches 	Training Loss: 0.162466
Trained 4501 batches 	Training Loss: 0.250472
Trained 4551 batches 	Training Loss: 0.087954
Trained 4601 batches 	Training Loss: 0.183538
Trained 4651 batches 	Training Loss: 0.192891
Trained 4701 batches 	Training Loss: 0.136135
Trained 4751 batches 	Training Loss: 0.162324
Trained 4801 batches 	Training Loss: 0.131130
Trained 4851 batches 	Training Loss: 0.134162
Trained 4901 batches 	Training Loss: 0.144202
Epoch: 5 	Training Loss: 0.153682
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.789
The AUROC of Atelectasis is 0.759107875090863
The AUROC of Cardiomegaly is 0.891909263262832
The AUROC of Effusion is 0.8731027203767157
The AUROC of Infiltration is 0.6821438490691305
The AUROC of Mass is 0.7905624798054971
The AUROC of Nodule is 0.7177713712119752
The AUROC of Pneumonia is 0.7287077118692498
The AUROC of Pneumothorax is 0.8411136460262989
The AUROC of Consolidation is 0.7504806498146662
The AUROC of Edema is 0.8969281765212209
The AUROC of Emphysema is 0.8195082382315184
The AUROC of Fibrosis is 0.7238297724024956
The AUROC of Pleural_Thickening is 0.7583250632232125
The AUROC of Hernia is 0.819196895058964
Started epoch 6
Trained 1 batches 	Training Loss: 0.173312
Trained 51 batches 	Training Loss: 0.219351
Trained 101 batches 	Training Loss: 0.119748
Trained 151 batches 	Training Loss: 0.068407
Trained 201 batches 	Training Loss: 0.110172
Trained 251 batches 	Training Loss: 0.087603
Trained 301 batches 	Training Loss: 0.183266
Trained 351 batches 	Training Loss: 0.211711
Trained 401 batches 	Training Loss: 0.128927
Trained 451 batches 	Training Loss: 0.144626
Trained 501 batches 	Training Loss: 0.138131
Trained 551 batches 	Training Loss: 0.172920
Trained 601 batches 	Training Loss: 0.149839
Trained 651 batches 	Training Loss: 0.131886
Trained 701 batches 	Training Loss: 0.153920
Trained 751 batches 	Training Loss: 0.075183
Trained 801 batches 	Training Loss: 0.215346
Trained 851 batches 	Training Loss: 0.173219
Trained 901 batches 	Training Loss: 0.191923
Trained 951 batches 	Training Loss: 0.129750
Trained 1001 batches 	Training Loss: 0.186117
Trained 1051 batches 	Training Loss: 0.150894
Trained 1101 batches 	Training Loss: 0.114804
Trained 1151 batches 	Training Loss: 0.153390
Trained 1201 batches 	Training Loss: 0.134731
Trained 1251 batches 	Training Loss: 0.093514
Trained 1301 batches 	Training Loss: 0.137974
Trained 1351 batches 	Training Loss: 0.138909
Trained 1401 batches 	Training Loss: 0.150590
Trained 1451 batches 	Training Loss: 0.129380
Trained 1501 batches 	Training Loss: 0.150059
Trained 1551 batches 	Training Loss: 0.160349
Trained 1601 batches 	Training Loss: 0.180095
Trained 1651 batches 	Training Loss: 0.141139
Trained 1701 batches 	Training Loss: 0.106100
Trained 1751 batches 	Training Loss: 0.195299
Trained 1801 batches 	Training Loss: 0.208207
Trained 1851 batches 	Training Loss: 0.254915
Trained 1901 batches 	Training Loss: 0.112718
Trained 1951 batches 	Training Loss: 0.114517
Trained 2001 batches 	Training Loss: 0.154463
Trained 2051 batches 	Training Loss: 0.138253
Trained 2101 batches 	Training Loss: 0.178441
Trained 2151 batches 	Training Loss: 0.130444
Trained 2201 batches 	Training Loss: 0.120842
Trained 2251 batches 	Training Loss: 0.133239
Trained 2301 batches 	Training Loss: 0.143616
Trained 2351 batches 	Training Loss: 0.173947
Trained 2401 batches 	Training Loss: 0.140319
Trained 2451 batches 	Training Loss: 0.075470
Trained 2501 batches 	Training Loss: 0.154327
Trained 2551 batches 	Training Loss: 0.178156
Trained 2601 batches 	Training Loss: 0.148618
Trained 2651 batches 	Training Loss: 0.154622
Trained 2701 batches 	Training Loss: 0.179938
Trained 2751 batches 	Training Loss: 0.184429
Trained 2801 batches 	Training Loss: 0.150218
Trained 2851 batches 	Training Loss: 0.130522
Trained 2901 batches 	Training Loss: 0.123794
Trained 2951 batches 	Training Loss: 0.149525
Trained 3001 batches 	Training Loss: 0.225450
Trained 3051 batches 	Training Loss: 0.189668
Trained 3101 batches 	Training Loss: 0.187552
Trained 3151 batches 	Training Loss: 0.159461
Trained 3201 batches 	Training Loss: 0.135758
Trained 3251 batches 	Training Loss: 0.124703
Trained 3301 batches 	Training Loss: 0.129585
Trained 3351 batches 	Training Loss: 0.153251
Trained 3401 batches 	Training Loss: 0.250173
Trained 3451 batches 	Training Loss: 0.177332
Trained 3501 batches 	Training Loss: 0.153207
Trained 3551 batches 	Training Loss: 0.102846
Trained 3601 batches 	Training Loss: 0.199841
Trained 3651 batches 	Training Loss: 0.189558
Trained 3701 batches 	Training Loss: 0.219493
Trained 3751 batches 	Training Loss: 0.177281
Trained 3801 batches 	Training Loss: 0.158969
Trained 3851 batches 	Training Loss: 0.151785
Trained 3901 batches 	Training Loss: 0.084346
Trained 3951 batches 	Training Loss: 0.183076
Trained 4001 batches 	Training Loss: 0.116707
Trained 4051 batches 	Training Loss: 0.141988
Trained 4101 batches 	Training Loss: 0.162890
Trained 4151 batches 	Training Loss: 0.103940
Trained 4201 batches 	Training Loss: 0.179253
Trained 4251 batches 	Training Loss: 0.160133
Trained 4301 batches 	Training Loss: 0.174991
Trained 4351 batches 	Training Loss: 0.201104
Trained 4401 batches 	Training Loss: 0.157244
Trained 4451 batches 	Training Loss: 0.212047
Trained 4501 batches 	Training Loss: 0.102403
Trained 4551 batches 	Training Loss: 0.169701
Trained 4601 batches 	Training Loss: 0.136370
Trained 4651 batches 	Training Loss: 0.161452
Trained 4701 batches 	Training Loss: 0.235317
Trained 4751 batches 	Training Loss: 0.267613
Trained 4801 batches 	Training Loss: 0.162593
Trained 4851 batches 	Training Loss: 0.131191
Trained 4901 batches 	Training Loss: 0.150683
Epoch: 6 	Training Loss: 0.152019
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.796
The AUROC of Atelectasis is 0.7728173607767624
The AUROC of Cardiomegaly is 0.8971448566516367
The AUROC of Effusion is 0.874824733783931
The AUROC of Infiltration is 0.6887476832000885
The AUROC of Mass is 0.7989912018179025
The AUROC of Nodule is 0.7222144458784308
The AUROC of Pneumonia is 0.724928924509972
The AUROC of Pneumothorax is 0.8501567767013878
The AUROC of Consolidation is 0.7554131319500172
The AUROC of Edema is 0.9019851188505593
The AUROC of Emphysema is 0.8176008653381855
The AUROC of Fibrosis is 0.7450752447162471
The AUROC of Pleural_Thickening is 0.7609500239236009
The AUROC of Hernia is 0.8317498593360663
Started epoch 7
Trained 1 batches 	Training Loss: 0.281635
Trained 51 batches 	Training Loss: 0.186185
Trained 101 batches 	Training Loss: 0.136020
Trained 151 batches 	Training Loss: 0.143761
Trained 201 batches 	Training Loss: 0.250811
Trained 251 batches 	Training Loss: 0.151000
Trained 301 batches 	Training Loss: 0.145781
Trained 351 batches 	Training Loss: 0.123262
Trained 401 batches 	Training Loss: 0.158989
Trained 451 batches 	Training Loss: 0.129681
Trained 501 batches 	Training Loss: 0.221601
Trained 551 batches 	Training Loss: 0.211490
Trained 601 batches 	Training Loss: 0.204910
Trained 651 batches 	Training Loss: 0.119541
Trained 701 batches 	Training Loss: 0.161474
Trained 751 batches 	Training Loss: 0.219030
Trained 801 batches 	Training Loss: 0.079575
Trained 851 batches 	Training Loss: 0.172742
Trained 901 batches 	Training Loss: 0.163631
Trained 951 batches 	Training Loss: 0.213794
Trained 1001 batches 	Training Loss: 0.134202
Trained 1051 batches 	Training Loss: 0.164840
Trained 1101 batches 	Training Loss: 0.184487
Trained 1151 batches 	Training Loss: 0.137950
Trained 1201 batches 	Training Loss: 0.085795
Trained 1251 batches 	Training Loss: 0.128892
Trained 1301 batches 	Training Loss: 0.167803
Trained 1351 batches 	Training Loss: 0.170069
Trained 1401 batches 	Training Loss: 0.188036
Trained 1451 batches 	Training Loss: 0.110407
Trained 1501 batches 	Training Loss: 0.099845
Trained 1551 batches 	Training Loss: 0.111766
Trained 1601 batches 	Training Loss: 0.145326
Trained 1651 batches 	Training Loss: 0.137038
Trained 1701 batches 	Training Loss: 0.138902
Trained 1751 batches 	Training Loss: 0.181155
Trained 1801 batches 	Training Loss: 0.206261
Trained 1851 batches 	Training Loss: 0.154545
Trained 1901 batches 	Training Loss: 0.143216
Trained 1951 batches 	Training Loss: 0.169982
Trained 2001 batches 	Training Loss: 0.127623
Trained 2051 batches 	Training Loss: 0.113641
Trained 2101 batches 	Training Loss: 0.115922
Trained 2151 batches 	Training Loss: 0.172909
Trained 2201 batches 	Training Loss: 0.150021
Trained 2251 batches 	Training Loss: 0.131225
Trained 2301 batches 	Training Loss: 0.148499
Trained 2351 batches 	Training Loss: 0.180149
Trained 2401 batches 	Training Loss: 0.213719
Trained 2451 batches 	Training Loss: 0.183202
Trained 2501 batches 	Training Loss: 0.178726
Trained 2551 batches 	Training Loss: 0.154486
Trained 2601 batches 	Training Loss: 0.140178
Trained 2651 batches 	Training Loss: 0.116003
Trained 2701 batches 	Training Loss: 0.107120
Trained 2751 batches 	Training Loss: 0.085206
Trained 2801 batches 	Training Loss: 0.099671
Trained 2851 batches 	Training Loss: 0.121113
Trained 2901 batches 	Training Loss: 0.209163
Trained 2951 batches 	Training Loss: 0.123118
Trained 3001 batches 	Training Loss: 0.145344
Trained 3051 batches 	Training Loss: 0.233891
Trained 3101 batches 	Training Loss: 0.231781
Trained 3151 batches 	Training Loss: 0.189408
Trained 3201 batches 	Training Loss: 0.140630
Trained 3251 batches 	Training Loss: 0.158409
Trained 3301 batches 	Training Loss: 0.118934
Trained 3351 batches 	Training Loss: 0.141462
Trained 3401 batches 	Training Loss: 0.162430
Trained 3451 batches 	Training Loss: 0.158311
Trained 3501 batches 	Training Loss: 0.169615
Trained 3551 batches 	Training Loss: 0.131106
Trained 3601 batches 	Training Loss: 0.140832
Trained 3651 batches 	Training Loss: 0.147095
Trained 3701 batches 	Training Loss: 0.126731
Trained 3751 batches 	Training Loss: 0.201129
Trained 3801 batches 	Training Loss: 0.108021
Trained 3851 batches 	Training Loss: 0.188668
Trained 3901 batches 	Training Loss: 0.136067
Trained 3951 batches 	Training Loss: 0.148225
Trained 4001 batches 	Training Loss: 0.177808
Trained 4051 batches 	Training Loss: 0.193646
Trained 4101 batches 	Training Loss: 0.143246
Trained 4151 batches 	Training Loss: 0.122952
Trained 4201 batches 	Training Loss: 0.130296
Trained 4251 batches 	Training Loss: 0.098389
Trained 4301 batches 	Training Loss: 0.227041
Trained 4351 batches 	Training Loss: 0.193774
Trained 4401 batches 	Training Loss: 0.080134
Trained 4451 batches 	Training Loss: 0.188772
Trained 4501 batches 	Training Loss: 0.146549
Trained 4551 batches 	Training Loss: 0.212718
Trained 4601 batches 	Training Loss: 0.142996
Trained 4651 batches 	Training Loss: 0.152207
Trained 4701 batches 	Training Loss: 0.177563
Trained 4751 batches 	Training Loss: 0.162164
Trained 4801 batches 	Training Loss: 0.110892
Trained 4851 batches 	Training Loss: 0.133426
Trained 4901 batches 	Training Loss: 0.114268
Epoch: 7 	Training Loss: 0.150563
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.791
The AUROC of Atelectasis is 0.7683679625698828
The AUROC of Cardiomegaly is 0.8958179914934308
The AUROC of Effusion is 0.8700425687382715
The AUROC of Infiltration is 0.69110177321641
The AUROC of Mass is 0.7930224019937604
The AUROC of Nodule is 0.7128556517967277
The AUROC of Pneumonia is 0.7253078152084881
The AUROC of Pneumothorax is 0.8386892379564115
The AUROC of Consolidation is 0.7536975206032583
The AUROC of Edema is 0.8832440160895186
The AUROC of Emphysema is 0.8261208323028184
The AUROC of Fibrosis is 0.7193813122850102
The AUROC of Pleural_Thickening is 0.750860851506463
The AUROC of Hernia is 0.8467831019555158
Started epoch 8
Trained 1 batches 	Training Loss: 0.123628
Trained 51 batches 	Training Loss: 0.218365
Trained 101 batches 	Training Loss: 0.131222
Trained 151 batches 	Training Loss: 0.113093
Trained 201 batches 	Training Loss: 0.108705
Trained 251 batches 	Training Loss: 0.133564
Trained 301 batches 	Training Loss: 0.152480
Trained 351 batches 	Training Loss: 0.096325
Trained 401 batches 	Training Loss: 0.249150
Trained 451 batches 	Training Loss: 0.168756
Trained 501 batches 	Training Loss: 0.208095
Trained 551 batches 	Training Loss: 0.154321
Trained 601 batches 	Training Loss: 0.096041
Trained 651 batches 	Training Loss: 0.138987
Trained 701 batches 	Training Loss: 0.136642
Trained 751 batches 	Training Loss: 0.205736
Trained 801 batches 	Training Loss: 0.184510
Trained 851 batches 	Training Loss: 0.245991
Trained 901 batches 	Training Loss: 0.191648
Trained 951 batches 	Training Loss: 0.164232
Trained 1001 batches 	Training Loss: 0.089161
Trained 1051 batches 	Training Loss: 0.143849
Trained 1101 batches 	Training Loss: 0.147852
Trained 1151 batches 	Training Loss: 0.078917
Trained 1201 batches 	Training Loss: 0.126550
Trained 1251 batches 	Training Loss: 0.136823
Trained 1301 batches 	Training Loss: 0.081791
Trained 1351 batches 	Training Loss: 0.184034
Trained 1401 batches 	Training Loss: 0.184210
Trained 1451 batches 	Training Loss: 0.137273
Trained 1501 batches 	Training Loss: 0.128028
Trained 1551 batches 	Training Loss: 0.088221
Trained 1601 batches 	Training Loss: 0.084606
Trained 1651 batches 	Training Loss: 0.142358
Trained 1701 batches 	Training Loss: 0.156111
Trained 1751 batches 	Training Loss: 0.171314
Trained 1801 batches 	Training Loss: 0.181994
Trained 1851 batches 	Training Loss: 0.133386
Trained 1901 batches 	Training Loss: 0.075034
Trained 1951 batches 	Training Loss: 0.259202
Trained 2001 batches 	Training Loss: 0.125118
Trained 2051 batches 	Training Loss: 0.144113
Trained 2101 batches 	Training Loss: 0.199330
Trained 2151 batches 	Training Loss: 0.109523
Trained 2201 batches 	Training Loss: 0.166560
Trained 2251 batches 	Training Loss: 0.094968
Trained 2301 batches 	Training Loss: 0.180160
Trained 2351 batches 	Training Loss: 0.178017
Trained 2401 batches 	Training Loss: 0.077793
Trained 2451 batches 	Training Loss: 0.178417
Trained 2501 batches 	Training Loss: 0.143079
Trained 2551 batches 	Training Loss: 0.217888
Trained 2601 batches 	Training Loss: 0.111113
Trained 2651 batches 	Training Loss: 0.075152
Trained 2701 batches 	Training Loss: 0.151937
Trained 2751 batches 	Training Loss: 0.177682
Trained 2801 batches 	Training Loss: 0.142329
Trained 2851 batches 	Training Loss: 0.130267
Trained 2901 batches 	Training Loss: 0.208534
Trained 2951 batches 	Training Loss: 0.153399
Trained 3001 batches 	Training Loss: 0.156291
Trained 3051 batches 	Training Loss: 0.234335
Trained 3101 batches 	Training Loss: 0.115821
Trained 3151 batches 	Training Loss: 0.105438
Trained 3201 batches 	Training Loss: 0.133876
Trained 3251 batches 	Training Loss: 0.198814
Trained 3301 batches 	Training Loss: 0.158134
Trained 3351 batches 	Training Loss: 0.157654
Trained 3401 batches 	Training Loss: 0.173827
Trained 3451 batches 	Training Loss: 0.192346
Trained 3501 batches 	Training Loss: 0.166864
Trained 3551 batches 	Training Loss: 0.164523
Trained 3601 batches 	Training Loss: 0.131416
Trained 3651 batches 	Training Loss: 0.186406
Trained 3701 batches 	Training Loss: 0.153968
Trained 3751 batches 	Training Loss: 0.165309
Trained 3801 batches 	Training Loss: 0.110029
Trained 3851 batches 	Training Loss: 0.143436
Trained 3901 batches 	Training Loss: 0.084754
Trained 3951 batches 	Training Loss: 0.193780
Trained 4001 batches 	Training Loss: 0.146764
Trained 4051 batches 	Training Loss: 0.113286
Trained 4101 batches 	Training Loss: 0.165401
Trained 4151 batches 	Training Loss: 0.162958
Trained 4201 batches 	Training Loss: 0.151099
Trained 4251 batches 	Training Loss: 0.100135
Trained 4301 batches 	Training Loss: 0.185349
Trained 4351 batches 	Training Loss: 0.098124
Trained 4401 batches 	Training Loss: 0.194319
Trained 4451 batches 	Training Loss: 0.237851
Trained 4501 batches 	Training Loss: 0.104490
Trained 4551 batches 	Training Loss: 0.225813
Trained 4601 batches 	Training Loss: 0.135966
Trained 4651 batches 	Training Loss: 0.143990
Trained 4701 batches 	Training Loss: 0.134326
Trained 4751 batches 	Training Loss: 0.141053
Trained 4801 batches 	Training Loss: 0.148545
Trained 4851 batches 	Training Loss: 0.167441
Trained 4901 batches 	Training Loss: 0.216705
Epoch: 8 	Training Loss: 0.149259
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.795
The AUROC of Atelectasis is 0.7776431308504544
The AUROC of Cardiomegaly is 0.8966840933956404
The AUROC of Effusion is 0.8790370645099981
The AUROC of Infiltration is 0.6802969930011895
The AUROC of Mass is 0.7875604510856034
The AUROC of Nodule is 0.7051519347757381
The AUROC of Pneumonia is 0.7244437777396805
The AUROC of Pneumothorax is 0.8442351030499963
The AUROC of Consolidation is 0.748383603532169
The AUROC of Edema is 0.9026310997815005
The AUROC of Emphysema is 0.8328331655066012
The AUROC of Fibrosis is 0.7401050450020511
The AUROC of Pleural_Thickening is 0.7593817794091992
The AUROC of Hernia is 0.8566927325548015
Started epoch 9
Trained 1 batches 	Training Loss: 0.185994
Trained 51 batches 	Training Loss: 0.132832
Trained 101 batches 	Training Loss: 0.064039
Trained 151 batches 	Training Loss: 0.097918
Trained 201 batches 	Training Loss: 0.173218
Trained 251 batches 	Training Loss: 0.105390
Trained 301 batches 	Training Loss: 0.066885
Trained 351 batches 	Training Loss: 0.165524
Trained 401 batches 	Training Loss: 0.171524
Trained 451 batches 	Training Loss: 0.107565
Trained 501 batches 	Training Loss: 0.160401
Trained 551 batches 	Training Loss: 0.182598
Trained 601 batches 	Training Loss: 0.121556
Trained 651 batches 	Training Loss: 0.119674
Trained 701 batches 	Training Loss: 0.166811
Trained 751 batches 	Training Loss: 0.174151
Trained 801 batches 	Training Loss: 0.093451
Trained 851 batches 	Training Loss: 0.214326
Trained 901 batches 	Training Loss: 0.111722
Trained 951 batches 	Training Loss: 0.117309
Trained 1001 batches 	Training Loss: 0.200172
Trained 1051 batches 	Training Loss: 0.188324
Trained 1101 batches 	Training Loss: 0.192217
Trained 1151 batches 	Training Loss: 0.103455
Trained 1201 batches 	Training Loss: 0.109573
Trained 1251 batches 	Training Loss: 0.230997
Trained 1301 batches 	Training Loss: 0.122952
Trained 1351 batches 	Training Loss: 0.157329
Trained 1401 batches 	Training Loss: 0.126373
Trained 1451 batches 	Training Loss: 0.206295
Trained 1501 batches 	Training Loss: 0.188483
Trained 1551 batches 	Training Loss: 0.099541
Trained 1601 batches 	Training Loss: 0.103557
Trained 1651 batches 	Training Loss: 0.158746
Trained 1701 batches 	Training Loss: 0.172182
Trained 1751 batches 	Training Loss: 0.106050
Trained 1801 batches 	Training Loss: 0.158240
Trained 1851 batches 	Training Loss: 0.156972
Trained 1901 batches 	Training Loss: 0.164594
Trained 1951 batches 	Training Loss: 0.093426
Trained 2001 batches 	Training Loss: 0.194054
Trained 2051 batches 	Training Loss: 0.146841
Trained 2101 batches 	Training Loss: 0.201928
Trained 2151 batches 	Training Loss: 0.193710
Trained 2201 batches 	Training Loss: 0.167981
Trained 2251 batches 	Training Loss: 0.117310
Trained 2301 batches 	Training Loss: 0.180989
Trained 2351 batches 	Training Loss: 0.163832
Trained 2401 batches 	Training Loss: 0.212272
Trained 2451 batches 	Training Loss: 0.129475
Trained 2501 batches 	Training Loss: 0.145466
Trained 2551 batches 	Training Loss: 0.181143
Trained 2601 batches 	Training Loss: 0.140655
Trained 2651 batches 	Training Loss: 0.227406
Trained 2701 batches 	Training Loss: 0.100354
Trained 2751 batches 	Training Loss: 0.152860
Trained 2801 batches 	Training Loss: 0.120755
Trained 2851 batches 	Training Loss: 0.260174
Trained 2901 batches 	Training Loss: 0.159861
Trained 2951 batches 	Training Loss: 0.201440
Trained 3001 batches 	Training Loss: 0.091929
Trained 3051 batches 	Training Loss: 0.136858
Trained 3101 batches 	Training Loss: 0.085375
Trained 3151 batches 	Training Loss: 0.176408
Trained 3201 batches 	Training Loss: 0.124477
Trained 3251 batches 	Training Loss: 0.157073
Trained 3301 batches 	Training Loss: 0.146371
Trained 3351 batches 	Training Loss: 0.120961
Trained 3401 batches 	Training Loss: 0.162495
Trained 3451 batches 	Training Loss: 0.163918
Trained 3501 batches 	Training Loss: 0.137815
Trained 3551 batches 	Training Loss: 0.155838
Trained 3601 batches 	Training Loss: 0.158487
Trained 3651 batches 	Training Loss: 0.121135
Trained 3701 batches 	Training Loss: 0.201863
Trained 3751 batches 	Training Loss: 0.123310
Trained 3801 batches 	Training Loss: 0.127179
Trained 3851 batches 	Training Loss: 0.122937
Trained 3901 batches 	Training Loss: 0.155074
Trained 3951 batches 	Training Loss: 0.168247
Trained 4001 batches 	Training Loss: 0.118490
Trained 4051 batches 	Training Loss: 0.178683
Trained 4101 batches 	Training Loss: 0.138556
Trained 4151 batches 	Training Loss: 0.096915
Trained 4201 batches 	Training Loss: 0.208134
Trained 4251 batches 	Training Loss: 0.108915
Trained 4301 batches 	Training Loss: 0.241950
Trained 4351 batches 	Training Loss: 0.221350
Trained 4401 batches 	Training Loss: 0.217457
Trained 4451 batches 	Training Loss: 0.238008
Trained 4501 batches 	Training Loss: 0.127986
Trained 4551 batches 	Training Loss: 0.170402
Trained 4601 batches 	Training Loss: 0.162012
Trained 4651 batches 	Training Loss: 0.200831
Trained 4701 batches 	Training Loss: 0.160456
Trained 4751 batches 	Training Loss: 0.166673
Trained 4801 batches 	Training Loss: 0.154954
Trained 4851 batches 	Training Loss: 0.105534
Trained 4901 batches 	Training Loss: 0.154457
Epoch: 9 	Training Loss: 0.147953
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.802
The AUROC of Atelectasis is 0.7785960733049675
The AUROC of Cardiomegaly is 0.89645685249553
The AUROC of Effusion is 0.8773047548930061
The AUROC of Infiltration is 0.6943368281280258
The AUROC of Mass is 0.807224765478921
The AUROC of Nodule is 0.7337924943300465
The AUROC of Pneumonia is 0.7306584185762995
The AUROC of Pneumothorax is 0.8498780836547318
The AUROC of Consolidation is 0.754245441489294
The AUROC of Edema is 0.9014272826590743
The AUROC of Emphysema is 0.844340889952773
The AUROC of Fibrosis is 0.7465391184048603
The AUROC of Pleural_Thickening is 0.7589842591634147
The AUROC of Hernia is 0.8559119041877663
Started epoch 10
Trained 1 batches 	Training Loss: 0.124782
Trained 51 batches 	Training Loss: 0.159158
Trained 101 batches 	Training Loss: 0.115345
Trained 151 batches 	Training Loss: 0.140938
Trained 201 batches 	Training Loss: 0.161694
Trained 251 batches 	Training Loss: 0.161939
Trained 301 batches 	Training Loss: 0.201495
Trained 351 batches 	Training Loss: 0.088097
Trained 401 batches 	Training Loss: 0.136434
Trained 451 batches 	Training Loss: 0.146197
Trained 501 batches 	Training Loss: 0.086064
Trained 551 batches 	Training Loss: 0.165220
Trained 601 batches 	Training Loss: 0.157495
Trained 651 batches 	Training Loss: 0.143179
Trained 701 batches 	Training Loss: 0.160681
Trained 751 batches 	Training Loss: 0.140743
Trained 801 batches 	Training Loss: 0.160623
Trained 851 batches 	Training Loss: 0.142943
Trained 901 batches 	Training Loss: 0.196255
Trained 951 batches 	Training Loss: 0.154335
Trained 1001 batches 	Training Loss: 0.138880
Trained 1051 batches 	Training Loss: 0.217051
Trained 1101 batches 	Training Loss: 0.091325
Trained 1151 batches 	Training Loss: 0.170896
Trained 1201 batches 	Training Loss: 0.120849
Trained 1251 batches 	Training Loss: 0.095559
Trained 1301 batches 	Training Loss: 0.108575
Trained 1351 batches 	Training Loss: 0.114540
Trained 1401 batches 	Training Loss: 0.231558
Trained 1451 batches 	Training Loss: 0.160449
Trained 1501 batches 	Training Loss: 0.162939
Trained 1551 batches 	Training Loss: 0.083863
Trained 1601 batches 	Training Loss: 0.110204
Trained 1651 batches 	Training Loss: 0.221198
Trained 1701 batches 	Training Loss: 0.214537
Trained 1751 batches 	Training Loss: 0.158520
Trained 1801 batches 	Training Loss: 0.131552
Trained 1851 batches 	Training Loss: 0.156704
Trained 1901 batches 	Training Loss: 0.156618
Trained 1951 batches 	Training Loss: 0.221049
Trained 2001 batches 	Training Loss: 0.130507
Trained 2051 batches 	Training Loss: 0.103260
Trained 2101 batches 	Training Loss: 0.117486
Trained 2151 batches 	Training Loss: 0.215846
Trained 2201 batches 	Training Loss: 0.125159
Trained 2251 batches 	Training Loss: 0.141497
Trained 2301 batches 	Training Loss: 0.145121
Trained 2351 batches 	Training Loss: 0.082479
Trained 2401 batches 	Training Loss: 0.082092
Trained 2451 batches 	Training Loss: 0.135530
Trained 2501 batches 	Training Loss: 0.120999
Trained 2551 batches 	Training Loss: 0.117423
Trained 2601 batches 	Training Loss: 0.156805
Trained 2651 batches 	Training Loss: 0.158916
Trained 2701 batches 	Training Loss: 0.208337
Trained 2751 batches 	Training Loss: 0.127907
Trained 2801 batches 	Training Loss: 0.157789
Trained 2851 batches 	Training Loss: 0.204295
Trained 2901 batches 	Training Loss: 0.178451
Trained 2951 batches 	Training Loss: 0.171331
Trained 3001 batches 	Training Loss: 0.185208
Trained 3051 batches 	Training Loss: 0.082094
Trained 3101 batches 	Training Loss: 0.172062
Trained 3151 batches 	Training Loss: 0.121630
Trained 3201 batches 	Training Loss: 0.097350
Trained 3251 batches 	Training Loss: 0.157692
Trained 3301 batches 	Training Loss: 0.143514
Trained 3351 batches 	Training Loss: 0.220077
Trained 3401 batches 	Training Loss: 0.197618
Trained 3451 batches 	Training Loss: 0.236253
Trained 3501 batches 	Training Loss: 0.100068
Trained 3551 batches 	Training Loss: 0.150051
Trained 3601 batches 	Training Loss: 0.088861
Trained 3651 batches 	Training Loss: 0.141258
Trained 3701 batches 	Training Loss: 0.138271
Trained 3751 batches 	Training Loss: 0.199404
Trained 3801 batches 	Training Loss: 0.134351
Trained 3851 batches 	Training Loss: 0.116196
Trained 3901 batches 	Training Loss: 0.151870
Trained 3951 batches 	Training Loss: 0.135984
Trained 4001 batches 	Training Loss: 0.100273
Trained 4051 batches 	Training Loss: 0.156178
Trained 4101 batches 	Training Loss: 0.123923
Trained 4151 batches 	Training Loss: 0.111019
Trained 4201 batches 	Training Loss: 0.183764
Trained 4251 batches 	Training Loss: 0.113642
Trained 4301 batches 	Training Loss: 0.165217
Trained 4351 batches 	Training Loss: 0.165023
Trained 4401 batches 	Training Loss: 0.126768
Trained 4451 batches 	Training Loss: 0.160192
Trained 4501 batches 	Training Loss: 0.169050
Trained 4551 batches 	Training Loss: 0.169430
Trained 4601 batches 	Training Loss: 0.174607
Trained 4651 batches 	Training Loss: 0.161167
Trained 4701 batches 	Training Loss: 0.151997
Trained 4751 batches 	Training Loss: 0.141373
Trained 4801 batches 	Training Loss: 0.113862
Trained 4851 batches 	Training Loss: 0.152783
Trained 4901 batches 	Training Loss: 0.152791
Epoch: 10 	Training Loss: 0.146639
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.806
The AUROC of Atelectasis is 0.7831815876290067
The AUROC of Cardiomegaly is 0.9041826736018865
The AUROC of Effusion is 0.8812934844182179
The AUROC of Infiltration is 0.6984196243326233
The AUROC of Mass is 0.7995947195016658
The AUROC of Nodule is 0.7403016507965046
The AUROC of Pneumonia is 0.7414343319618264
The AUROC of Pneumothorax is 0.8573202319131059
The AUROC of Consolidation is 0.7548319735819735
The AUROC of Edema is 0.9053648281798319
The AUROC of Emphysema is 0.8553734733656032
The AUROC of Fibrosis is 0.7407039999142052
The AUROC of Pleural_Thickening is 0.7710203759588442
The AUROC of Hernia is 0.845990790818377
Training time lapse: 148.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.811
The AUROC of Atelectasis is 0.7946787115407083
The AUROC of Cardiomegaly is 0.9017022986733609
The AUROC of Effusion is 0.8718817742609491
The AUROC of Infiltration is 0.7008218521640663
The AUROC of Mass is 0.8334330796141316
The AUROC of Nodule is 0.7379183124340818
The AUROC of Pneumonia is 0.7113851028139836
The AUROC of Pneumothorax is 0.8479585360781609
The AUROC of Consolidation is 0.8072257655394801
The AUROC of Edema is 0.8749374081088533
The AUROC of Emphysema is 0.8938451437317136
The AUROC of Fibrosis is 0.7586623092862521
The AUROC of Pleural_Thickening is 0.775685671677562
The AUROC of Hernia is 0.8423951908465183
