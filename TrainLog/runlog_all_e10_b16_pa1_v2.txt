Started training, total epoch : 10
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.771083
Trained 51 batches 	Training Loss: 0.180500
Trained 101 batches 	Training Loss: 0.120580
Trained 151 batches 	Training Loss: 0.188373
Trained 201 batches 	Training Loss: 0.127535
Trained 251 batches 	Training Loss: 0.172133
Trained 301 batches 	Training Loss: 0.270540
Trained 351 batches 	Training Loss: 0.166316
Trained 401 batches 	Training Loss: 0.246223
Trained 451 batches 	Training Loss: 0.197343
Trained 501 batches 	Training Loss: 0.234440
Trained 551 batches 	Training Loss: 0.145054
Trained 601 batches 	Training Loss: 0.138926
Trained 651 batches 	Training Loss: 0.145901
Trained 701 batches 	Training Loss: 0.154163
Trained 751 batches 	Training Loss: 0.119416
Trained 801 batches 	Training Loss: 0.165034
Trained 851 batches 	Training Loss: 0.188152
Trained 901 batches 	Training Loss: 0.308658
Trained 951 batches 	Training Loss: 0.165212
Trained 1001 batches 	Training Loss: 0.147222
Trained 1051 batches 	Training Loss: 0.186418
Trained 1101 batches 	Training Loss: 0.172828
Trained 1151 batches 	Training Loss: 0.180021
Trained 1201 batches 	Training Loss: 0.158780
Trained 1251 batches 	Training Loss: 0.123586
Trained 1301 batches 	Training Loss: 0.249196
Trained 1351 batches 	Training Loss: 0.114783
Trained 1401 batches 	Training Loss: 0.111626
Trained 1451 batches 	Training Loss: 0.195599
Trained 1501 batches 	Training Loss: 0.231493
Trained 1551 batches 	Training Loss: 0.228542
Trained 1601 batches 	Training Loss: 0.217953
Trained 1651 batches 	Training Loss: 0.172516
Trained 1701 batches 	Training Loss: 0.120337
Trained 1751 batches 	Training Loss: 0.135229
Trained 1801 batches 	Training Loss: 0.187229
Trained 1851 batches 	Training Loss: 0.174547
Trained 1901 batches 	Training Loss: 0.147945
Trained 1951 batches 	Training Loss: 0.181810
Trained 2001 batches 	Training Loss: 0.116481
Trained 2051 batches 	Training Loss: 0.252385
Trained 2101 batches 	Training Loss: 0.236911
Trained 2151 batches 	Training Loss: 0.197137
Trained 2201 batches 	Training Loss: 0.149022
Trained 2251 batches 	Training Loss: 0.147520
Trained 2301 batches 	Training Loss: 0.134492
Trained 2351 batches 	Training Loss: 0.109573
Trained 2401 batches 	Training Loss: 0.272688
Trained 2451 batches 	Training Loss: 0.302717
Trained 2501 batches 	Training Loss: 0.147809
Trained 2551 batches 	Training Loss: 0.154552
Trained 2601 batches 	Training Loss: 0.113863
Trained 2651 batches 	Training Loss: 0.185055
Trained 2701 batches 	Training Loss: 0.172127
Trained 2751 batches 	Training Loss: 0.141117
Trained 2801 batches 	Training Loss: 0.219673
Trained 2851 batches 	Training Loss: 0.190438
Trained 2901 batches 	Training Loss: 0.225970
Trained 2951 batches 	Training Loss: 0.117310
Trained 3001 batches 	Training Loss: 0.176934
Trained 3051 batches 	Training Loss: 0.112873
Trained 3101 batches 	Training Loss: 0.162592
Trained 3151 batches 	Training Loss: 0.134869
Trained 3201 batches 	Training Loss: 0.147863
Trained 3251 batches 	Training Loss: 0.171056
Trained 3301 batches 	Training Loss: 0.167026
Trained 3351 batches 	Training Loss: 0.183915
Trained 3401 batches 	Training Loss: 0.127445
Trained 3451 batches 	Training Loss: 0.137281
Trained 3501 batches 	Training Loss: 0.226563
Trained 3551 batches 	Training Loss: 0.189766
Trained 3601 batches 	Training Loss: 0.262835
Trained 3651 batches 	Training Loss: 0.165826
Trained 3701 batches 	Training Loss: 0.144501
Trained 3751 batches 	Training Loss: 0.261872
Trained 3801 batches 	Training Loss: 0.116232
Trained 3851 batches 	Training Loss: 0.236454
Trained 3901 batches 	Training Loss: 0.118909
Trained 3951 batches 	Training Loss: 0.151399
Trained 4001 batches 	Training Loss: 0.213679
Trained 4051 batches 	Training Loss: 0.096567
Trained 4101 batches 	Training Loss: 0.132573
Trained 4151 batches 	Training Loss: 0.213752
Trained 4201 batches 	Training Loss: 0.203107
Trained 4251 batches 	Training Loss: 0.144083
Trained 4301 batches 	Training Loss: 0.158020
Trained 4351 batches 	Training Loss: 0.192599
Trained 4401 batches 	Training Loss: 0.142960
Trained 4451 batches 	Training Loss: 0.178559
Trained 4501 batches 	Training Loss: 0.153084
Trained 4551 batches 	Training Loss: 0.242630
Trained 4601 batches 	Training Loss: 0.150765
Trained 4651 batches 	Training Loss: 0.160901
Trained 4701 batches 	Training Loss: 0.167779
Trained 4751 batches 	Training Loss: 0.207394
Trained 4801 batches 	Training Loss: 0.127992
Trained 4851 batches 	Training Loss: 0.235800
Trained 4901 batches 	Training Loss: 0.150979
Epoch: 1 	Training Loss: 0.175159
Started epoch 2
Trained 1 batches 	Training Loss: 0.137786
Trained 51 batches 	Training Loss: 0.243028
Trained 101 batches 	Training Loss: 0.128734
Trained 151 batches 	Training Loss: 0.185839
Trained 201 batches 	Training Loss: 0.123979
Trained 251 batches 	Training Loss: 0.187937
Trained 301 batches 	Training Loss: 0.122854
Trained 351 batches 	Training Loss: 0.143524
Trained 401 batches 	Training Loss: 0.212925
Trained 451 batches 	Training Loss: 0.150774
Trained 501 batches 	Training Loss: 0.153255
Trained 551 batches 	Training Loss: 0.174048
Trained 601 batches 	Training Loss: 0.166479
Trained 651 batches 	Training Loss: 0.187346
Trained 701 batches 	Training Loss: 0.185786
Trained 751 batches 	Training Loss: 0.160929
Trained 801 batches 	Training Loss: 0.142132
Trained 851 batches 	Training Loss: 0.151189
Trained 901 batches 	Training Loss: 0.143207
Trained 951 batches 	Training Loss: 0.133097
Trained 1001 batches 	Training Loss: 0.185371
Trained 1051 batches 	Training Loss: 0.222803
Trained 1101 batches 	Training Loss: 0.233968
Trained 1151 batches 	Training Loss: 0.191595
Trained 1201 batches 	Training Loss: 0.204762
Trained 1251 batches 	Training Loss: 0.134896
Trained 1301 batches 	Training Loss: 0.173559
Trained 1351 batches 	Training Loss: 0.139967
Trained 1401 batches 	Training Loss: 0.061206
Trained 1451 batches 	Training Loss: 0.200941
Trained 1501 batches 	Training Loss: 0.140918
Trained 1551 batches 	Training Loss: 0.177904
Trained 1601 batches 	Training Loss: 0.124016
Trained 1651 batches 	Training Loss: 0.175528
Trained 1701 batches 	Training Loss: 0.147789
Trained 1751 batches 	Training Loss: 0.194304
Trained 1801 batches 	Training Loss: 0.191123
Trained 1851 batches 	Training Loss: 0.168256
Trained 1901 batches 	Training Loss: 0.152995
Trained 1951 batches 	Training Loss: 0.093879
Trained 2001 batches 	Training Loss: 0.145523
Trained 2051 batches 	Training Loss: 0.171217
Trained 2101 batches 	Training Loss: 0.092580
Trained 2151 batches 	Training Loss: 0.216161
Trained 2201 batches 	Training Loss: 0.213131
Trained 2251 batches 	Training Loss: 0.145953
Trained 2301 batches 	Training Loss: 0.191063
Trained 2351 batches 	Training Loss: 0.142767
Trained 2401 batches 	Training Loss: 0.133273
Trained 2451 batches 	Training Loss: 0.158050
Trained 2501 batches 	Training Loss: 0.202340
Trained 2551 batches 	Training Loss: 0.124530
Trained 2601 batches 	Training Loss: 0.189212
Trained 2651 batches 	Training Loss: 0.178744
Trained 2701 batches 	Training Loss: 0.222682
Trained 2751 batches 	Training Loss: 0.110299
Trained 2801 batches 	Training Loss: 0.185157
Trained 2851 batches 	Training Loss: 0.147201
Trained 2901 batches 	Training Loss: 0.190319
Trained 2951 batches 	Training Loss: 0.128148
Trained 3001 batches 	Training Loss: 0.153554
Trained 3051 batches 	Training Loss: 0.191128
Trained 3101 batches 	Training Loss: 0.150081
Trained 3151 batches 	Training Loss: 0.125418
Trained 3201 batches 	Training Loss: 0.158287
Trained 3251 batches 	Training Loss: 0.216894
Trained 3301 batches 	Training Loss: 0.182124
Trained 3351 batches 	Training Loss: 0.196266
Trained 3401 batches 	Training Loss: 0.166050
Trained 3451 batches 	Training Loss: 0.199910
Trained 3501 batches 	Training Loss: 0.103693
Trained 3551 batches 	Training Loss: 0.167340
Trained 3601 batches 	Training Loss: 0.171995
Trained 3651 batches 	Training Loss: 0.275890
Trained 3701 batches 	Training Loss: 0.195883
Trained 3751 batches 	Training Loss: 0.134151
Trained 3801 batches 	Training Loss: 0.188618
Trained 3851 batches 	Training Loss: 0.142986
Trained 3901 batches 	Training Loss: 0.260105
Trained 3951 batches 	Training Loss: 0.156547
Trained 4001 batches 	Training Loss: 0.180781
Trained 4051 batches 	Training Loss: 0.127357
Trained 4101 batches 	Training Loss: 0.127523
Trained 4151 batches 	Training Loss: 0.180101
Trained 4201 batches 	Training Loss: 0.189016
Trained 4251 batches 	Training Loss: 0.240925
Trained 4301 batches 	Training Loss: 0.147086
Trained 4351 batches 	Training Loss: 0.109912
Trained 4401 batches 	Training Loss: 0.129987
Trained 4451 batches 	Training Loss: 0.168170
Trained 4501 batches 	Training Loss: 0.139105
Trained 4551 batches 	Training Loss: 0.174170
Trained 4601 batches 	Training Loss: 0.116166
Trained 4651 batches 	Training Loss: 0.093301
Trained 4701 batches 	Training Loss: 0.089364
Trained 4751 batches 	Training Loss: 0.198276
Trained 4801 batches 	Training Loss: 0.170305
Trained 4851 batches 	Training Loss: 0.229744
Trained 4901 batches 	Training Loss: 0.154699
Epoch: 2 	Training Loss: 0.164961
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.761
The AUROC of Atelectasis is 0.7557822862560091
The AUROC of Cardiomegaly is 0.8378166333820968
The AUROC of Effusion is 0.8374363872493288
The AUROC of Infiltration is 0.6854059156093673
The AUROC of Mass is 0.7466348517647717
The AUROC of Nodule is 0.676633485995233
The AUROC of Pneumonia is 0.7133507441102606
The AUROC of Pneumothorax is 0.7697569044158188
The AUROC of Consolidation is 0.7566380805907829
The AUROC of Edema is 0.8471405862721021
The AUROC of Emphysema is 0.7559903555614053
The AUROC of Fibrosis is 0.7316195378907216
The AUROC of Pleural_Thickening is 0.7089556010209499
The AUROC of Hernia is 0.8341907400079145
Started epoch 3
Trained 1 batches 	Training Loss: 0.144513
Trained 51 batches 	Training Loss: 0.258469
Trained 101 batches 	Training Loss: 0.180326
Trained 151 batches 	Training Loss: 0.247383
Trained 201 batches 	Training Loss: 0.135302
Trained 251 batches 	Training Loss: 0.169014
Trained 301 batches 	Training Loss: 0.168057
Trained 351 batches 	Training Loss: 0.228873
Trained 401 batches 	Training Loss: 0.180837
Trained 451 batches 	Training Loss: 0.151218
Trained 501 batches 	Training Loss: 0.126401
Trained 551 batches 	Training Loss: 0.254386
Trained 601 batches 	Training Loss: 0.174547
Trained 651 batches 	Training Loss: 0.107296
Trained 701 batches 	Training Loss: 0.138289
Trained 751 batches 	Training Loss: 0.206066
Trained 801 batches 	Training Loss: 0.167859
Trained 851 batches 	Training Loss: 0.179130
Trained 901 batches 	Training Loss: 0.214498
Trained 951 batches 	Training Loss: 0.171728
Trained 1001 batches 	Training Loss: 0.090011
Trained 1051 batches 	Training Loss: 0.210956
Trained 1101 batches 	Training Loss: 0.147372
Trained 1151 batches 	Training Loss: 0.222319
Trained 1201 batches 	Training Loss: 0.151734
Trained 1251 batches 	Training Loss: 0.219612
Trained 1301 batches 	Training Loss: 0.180300
Trained 1351 batches 	Training Loss: 0.167391
Trained 1401 batches 	Training Loss: 0.138733
Trained 1451 batches 	Training Loss: 0.177309
Trained 1501 batches 	Training Loss: 0.146728
Trained 1551 batches 	Training Loss: 0.203486
Trained 1601 batches 	Training Loss: 0.195908
Trained 1651 batches 	Training Loss: 0.254054
Trained 1701 batches 	Training Loss: 0.231317
Trained 1751 batches 	Training Loss: 0.167124
Trained 1801 batches 	Training Loss: 0.110404
Trained 1851 batches 	Training Loss: 0.154148
Trained 1901 batches 	Training Loss: 0.083167
Trained 1951 batches 	Training Loss: 0.146686
Trained 2001 batches 	Training Loss: 0.156380
Trained 2051 batches 	Training Loss: 0.172479
Trained 2101 batches 	Training Loss: 0.201362
Trained 2151 batches 	Training Loss: 0.145361
Trained 2201 batches 	Training Loss: 0.116318
Trained 2251 batches 	Training Loss: 0.227234
Trained 2301 batches 	Training Loss: 0.231220
Trained 2351 batches 	Training Loss: 0.140297
Trained 2401 batches 	Training Loss: 0.130966
Trained 2451 batches 	Training Loss: 0.188230
Trained 2501 batches 	Training Loss: 0.152493
Trained 2551 batches 	Training Loss: 0.127882
Trained 2601 batches 	Training Loss: 0.181729
Trained 2651 batches 	Training Loss: 0.320830
Trained 2701 batches 	Training Loss: 0.174422
Trained 2751 batches 	Training Loss: 0.140296
Trained 2801 batches 	Training Loss: 0.177401
Trained 2851 batches 	Training Loss: 0.152834
Trained 2901 batches 	Training Loss: 0.181634
Trained 2951 batches 	Training Loss: 0.201223
Trained 3001 batches 	Training Loss: 0.186708
Trained 3051 batches 	Training Loss: 0.160104
Trained 3101 batches 	Training Loss: 0.139559
Trained 3151 batches 	Training Loss: 0.154623
Trained 3201 batches 	Training Loss: 0.150511
Trained 3251 batches 	Training Loss: 0.184769
Trained 3301 batches 	Training Loss: 0.110151
Trained 3351 batches 	Training Loss: 0.198501
Trained 3401 batches 	Training Loss: 0.190179
Trained 3451 batches 	Training Loss: 0.188943
Trained 3501 batches 	Training Loss: 0.127378
Trained 3551 batches 	Training Loss: 0.143846
Trained 3601 batches 	Training Loss: 0.143433
Trained 3651 batches 	Training Loss: 0.183624
Trained 3701 batches 	Training Loss: 0.225001
Trained 3751 batches 	Training Loss: 0.133388
Trained 3801 batches 	Training Loss: 0.141403
Trained 3851 batches 	Training Loss: 0.186951
Trained 3901 batches 	Training Loss: 0.201821
Trained 3951 batches 	Training Loss: 0.233024
Trained 4001 batches 	Training Loss: 0.121539
Trained 4051 batches 	Training Loss: 0.171808
Trained 4101 batches 	Training Loss: 0.105333
Trained 4151 batches 	Training Loss: 0.214587
Trained 4201 batches 	Training Loss: 0.162344
Trained 4251 batches 	Training Loss: 0.119960
Trained 4301 batches 	Training Loss: 0.118752
Trained 4351 batches 	Training Loss: 0.115125
Trained 4401 batches 	Training Loss: 0.143237
Trained 4451 batches 	Training Loss: 0.234405
Trained 4501 batches 	Training Loss: 0.194792
Trained 4551 batches 	Training Loss: 0.146932
Trained 4601 batches 	Training Loss: 0.262722
Trained 4651 batches 	Training Loss: 0.104856
Trained 4701 batches 	Training Loss: 0.157451
Trained 4751 batches 	Training Loss: 0.222396
Trained 4801 batches 	Training Loss: 0.208529
Trained 4851 batches 	Training Loss: 0.208691
Trained 4901 batches 	Training Loss: 0.092384
Epoch: 3 	Training Loss: 0.160687
Started epoch 4
Trained 1 batches 	Training Loss: 0.140820
Trained 51 batches 	Training Loss: 0.130593
Trained 101 batches 	Training Loss: 0.155120
Trained 151 batches 	Training Loss: 0.155619
Trained 201 batches 	Training Loss: 0.154910
Trained 251 batches 	Training Loss: 0.158392
Trained 301 batches 	Training Loss: 0.203712
Trained 351 batches 	Training Loss: 0.141287
Trained 401 batches 	Training Loss: 0.218971
Trained 451 batches 	Training Loss: 0.155354
Trained 501 batches 	Training Loss: 0.181656
Trained 551 batches 	Training Loss: 0.178131
Trained 601 batches 	Training Loss: 0.095850
Trained 651 batches 	Training Loss: 0.142592
Trained 701 batches 	Training Loss: 0.195821
Trained 751 batches 	Training Loss: 0.147673
Trained 801 batches 	Training Loss: 0.132788
Trained 851 batches 	Training Loss: 0.162290
Trained 901 batches 	Training Loss: 0.253305
Trained 951 batches 	Training Loss: 0.117464
Trained 1001 batches 	Training Loss: 0.158917
Trained 1051 batches 	Training Loss: 0.135408
Trained 1101 batches 	Training Loss: 0.127876
Trained 1151 batches 	Training Loss: 0.158917
Trained 1201 batches 	Training Loss: 0.128519
Trained 1251 batches 	Training Loss: 0.185128
Trained 1301 batches 	Training Loss: 0.171251
Trained 1351 batches 	Training Loss: 0.107515
Trained 1401 batches 	Training Loss: 0.136238
Trained 1451 batches 	Training Loss: 0.169201
Trained 1501 batches 	Training Loss: 0.181916
Trained 1551 batches 	Training Loss: 0.164809
Trained 1601 batches 	Training Loss: 0.178775
Trained 1651 batches 	Training Loss: 0.125502
Trained 1701 batches 	Training Loss: 0.105927
Trained 1751 batches 	Training Loss: 0.143525
Trained 1801 batches 	Training Loss: 0.184770
Trained 1851 batches 	Training Loss: 0.101642
Trained 1901 batches 	Training Loss: 0.130926
Trained 1951 batches 	Training Loss: 0.180703
Trained 2001 batches 	Training Loss: 0.143761
Trained 2051 batches 	Training Loss: 0.199411
Trained 2101 batches 	Training Loss: 0.114520
Trained 2151 batches 	Training Loss: 0.157884
Trained 2201 batches 	Training Loss: 0.150650
Trained 2251 batches 	Training Loss: 0.127924
Trained 2301 batches 	Training Loss: 0.211607
Trained 2351 batches 	Training Loss: 0.091050
Trained 2401 batches 	Training Loss: 0.145193
Trained 2451 batches 	Training Loss: 0.061983
Trained 2501 batches 	Training Loss: 0.134139
Trained 2551 batches 	Training Loss: 0.216757
Trained 2601 batches 	Training Loss: 0.170752
Trained 2651 batches 	Training Loss: 0.089930
Trained 2701 batches 	Training Loss: 0.106764
Trained 2751 batches 	Training Loss: 0.118634
Trained 2801 batches 	Training Loss: 0.317591
Trained 2851 batches 	Training Loss: 0.233396
Trained 2901 batches 	Training Loss: 0.148359
Trained 2951 batches 	Training Loss: 0.181287
Trained 3001 batches 	Training Loss: 0.191105
Trained 3051 batches 	Training Loss: 0.092973
Trained 3101 batches 	Training Loss: 0.161620
Trained 3151 batches 	Training Loss: 0.147487
Trained 3201 batches 	Training Loss: 0.122651
Trained 3251 batches 	Training Loss: 0.099331
Trained 3301 batches 	Training Loss: 0.124645
Trained 3351 batches 	Training Loss: 0.194039
Trained 3401 batches 	Training Loss: 0.163452
Trained 3451 batches 	Training Loss: 0.162391
Trained 3501 batches 	Training Loss: 0.201349
Trained 3551 batches 	Training Loss: 0.157542
Trained 3601 batches 	Training Loss: 0.067831
Trained 3651 batches 	Training Loss: 0.104311
Trained 3701 batches 	Training Loss: 0.090721
Trained 3751 batches 	Training Loss: 0.161264
Trained 3801 batches 	Training Loss: 0.125890
Trained 3851 batches 	Training Loss: 0.176976
Trained 3901 batches 	Training Loss: 0.237083
Trained 3951 batches 	Training Loss: 0.219639
Trained 4001 batches 	Training Loss: 0.194825
Trained 4051 batches 	Training Loss: 0.154968
Trained 4101 batches 	Training Loss: 0.140971
Trained 4151 batches 	Training Loss: 0.149338
Trained 4201 batches 	Training Loss: 0.197233
Trained 4251 batches 	Training Loss: 0.177039
Trained 4301 batches 	Training Loss: 0.279181
Trained 4351 batches 	Training Loss: 0.126146
Trained 4401 batches 	Training Loss: 0.160622
Trained 4451 batches 	Training Loss: 0.151070
Trained 4501 batches 	Training Loss: 0.175712
Trained 4551 batches 	Training Loss: 0.139848
Trained 4601 batches 	Training Loss: 0.127958
Trained 4651 batches 	Training Loss: 0.150355
Trained 4701 batches 	Training Loss: 0.174975
Trained 4751 batches 	Training Loss: 0.124708
Trained 4801 batches 	Training Loss: 0.105005
Trained 4851 batches 	Training Loss: 0.160746
Trained 4901 batches 	Training Loss: 0.098258
Epoch: 4 	Training Loss: 0.157385
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.792
The AUROC of Atelectasis is 0.7757587606617851
The AUROC of Cardiomegaly is 0.8807300550561185
The AUROC of Effusion is 0.8567287776071024
The AUROC of Infiltration is 0.6913591893648838
The AUROC of Mass is 0.792443339415137
The AUROC of Nodule is 0.7075196806204485
The AUROC of Pneumonia is 0.7493155961043386
The AUROC of Pneumothorax is 0.8219447646734499
The AUROC of Consolidation is 0.7718260335931544
The AUROC of Edema is 0.8750467089055802
The AUROC of Emphysema is 0.8201620579617841
The AUROC of Fibrosis is 0.7614696586239701
The AUROC of Pleural_Thickening is 0.7371236204781527
The AUROC of Hernia is 0.8535174949257694
Started epoch 5
Trained 1 batches 	Training Loss: 0.133873
Trained 51 batches 	Training Loss: 0.146744
Trained 101 batches 	Training Loss: 0.180735
Trained 151 batches 	Training Loss: 0.238598
Trained 201 batches 	Training Loss: 0.146986
Trained 251 batches 	Training Loss: 0.138023
Trained 301 batches 	Training Loss: 0.194644
Trained 351 batches 	Training Loss: 0.131454
Trained 401 batches 	Training Loss: 0.088627
Trained 451 batches 	Training Loss: 0.123328
Trained 501 batches 	Training Loss: 0.164296
Trained 551 batches 	Training Loss: 0.137812
Trained 601 batches 	Training Loss: 0.190937
Trained 651 batches 	Training Loss: 0.151736
Trained 701 batches 	Training Loss: 0.184196
Trained 751 batches 	Training Loss: 0.184082
Trained 801 batches 	Training Loss: 0.116086
Trained 851 batches 	Training Loss: 0.066217
Trained 901 batches 	Training Loss: 0.198113
Trained 951 batches 	Training Loss: 0.135024
Trained 1001 batches 	Training Loss: 0.160564
Trained 1051 batches 	Training Loss: 0.188308
Trained 1101 batches 	Training Loss: 0.167334
Trained 1151 batches 	Training Loss: 0.073968
Trained 1201 batches 	Training Loss: 0.202876
Trained 1251 batches 	Training Loss: 0.148462
Trained 1301 batches 	Training Loss: 0.190335
Trained 1351 batches 	Training Loss: 0.215194
Trained 1401 batches 	Training Loss: 0.109673
Trained 1451 batches 	Training Loss: 0.112331
Trained 1501 batches 	Training Loss: 0.149110
Trained 1551 batches 	Training Loss: 0.196116
Trained 1601 batches 	Training Loss: 0.137585
Trained 1651 batches 	Training Loss: 0.169438
Trained 1701 batches 	Training Loss: 0.168319
Trained 1751 batches 	Training Loss: 0.138440
Trained 1801 batches 	Training Loss: 0.083140
Trained 1851 batches 	Training Loss: 0.152210
Trained 1901 batches 	Training Loss: 0.171535
Trained 1951 batches 	Training Loss: 0.145000
Trained 2001 batches 	Training Loss: 0.110677
Trained 2051 batches 	Training Loss: 0.160740
Trained 2101 batches 	Training Loss: 0.195820
Trained 2151 batches 	Training Loss: 0.177281
Trained 2201 batches 	Training Loss: 0.108119
Trained 2251 batches 	Training Loss: 0.183220
Trained 2301 batches 	Training Loss: 0.181106
Trained 2351 batches 	Training Loss: 0.171549
Trained 2401 batches 	Training Loss: 0.106133
Trained 2451 batches 	Training Loss: 0.185283
Trained 2501 batches 	Training Loss: 0.193918
Trained 2551 batches 	Training Loss: 0.215631
Trained 2601 batches 	Training Loss: 0.157451
Trained 2651 batches 	Training Loss: 0.089754
Trained 2701 batches 	Training Loss: 0.088962
Trained 2751 batches 	Training Loss: 0.127144
Trained 2801 batches 	Training Loss: 0.087933
Trained 2851 batches 	Training Loss: 0.123697
Trained 2901 batches 	Training Loss: 0.183565
Trained 2951 batches 	Training Loss: 0.160064
Trained 3001 batches 	Training Loss: 0.049610
Trained 3051 batches 	Training Loss: 0.154670
Trained 3101 batches 	Training Loss: 0.201941
Trained 3151 batches 	Training Loss: 0.101031
Trained 3201 batches 	Training Loss: 0.102829
Trained 3251 batches 	Training Loss: 0.124526
Trained 3301 batches 	Training Loss: 0.216581
Trained 3351 batches 	Training Loss: 0.112941
Trained 3401 batches 	Training Loss: 0.131991
Trained 3451 batches 	Training Loss: 0.119221
Trained 3501 batches 	Training Loss: 0.071485
Trained 3551 batches 	Training Loss: 0.173067
Trained 3601 batches 	Training Loss: 0.185257
Trained 3651 batches 	Training Loss: 0.158691
Trained 3701 batches 	Training Loss: 0.138947
Trained 3751 batches 	Training Loss: 0.101126
Trained 3801 batches 	Training Loss: 0.196363
Trained 3851 batches 	Training Loss: 0.236365
Trained 3901 batches 	Training Loss: 0.093782
Trained 3951 batches 	Training Loss: 0.098508
Trained 4001 batches 	Training Loss: 0.171135
Trained 4051 batches 	Training Loss: 0.159899
Trained 4101 batches 	Training Loss: 0.133654
Trained 4151 batches 	Training Loss: 0.258883
Trained 4201 batches 	Training Loss: 0.099966
Trained 4251 batches 	Training Loss: 0.150726
Trained 4301 batches 	Training Loss: 0.162886
Trained 4351 batches 	Training Loss: 0.154586
Trained 4401 batches 	Training Loss: 0.159632
Trained 4451 batches 	Training Loss: 0.158793
Trained 4501 batches 	Training Loss: 0.092441
Trained 4551 batches 	Training Loss: 0.145457
Trained 4601 batches 	Training Loss: 0.108937
Trained 4651 batches 	Training Loss: 0.191646
Trained 4701 batches 	Training Loss: 0.137745
Trained 4751 batches 	Training Loss: 0.126488
Trained 4801 batches 	Training Loss: 0.140897
Trained 4851 batches 	Training Loss: 0.172224
Trained 4901 batches 	Training Loss: 0.166613
Epoch: 5 	Training Loss: 0.154765
Started epoch 6
Trained 1 batches 	Training Loss: 0.206488
Trained 51 batches 	Training Loss: 0.152492
Trained 101 batches 	Training Loss: 0.145921
Trained 151 batches 	Training Loss: 0.130305
Trained 201 batches 	Training Loss: 0.136653
Trained 251 batches 	Training Loss: 0.156039
Trained 301 batches 	Training Loss: 0.164176
Trained 351 batches 	Training Loss: 0.104724
Trained 401 batches 	Training Loss: 0.234076
Trained 451 batches 	Training Loss: 0.168004
Trained 501 batches 	Training Loss: 0.165200
Trained 551 batches 	Training Loss: 0.191802
Trained 601 batches 	Training Loss: 0.109752
Trained 651 batches 	Training Loss: 0.268980
Trained 701 batches 	Training Loss: 0.086719
Trained 751 batches 	Training Loss: 0.133681
Trained 801 batches 	Training Loss: 0.129555
Trained 851 batches 	Training Loss: 0.108377
Trained 901 batches 	Training Loss: 0.128068
Trained 951 batches 	Training Loss: 0.250908
Trained 1001 batches 	Training Loss: 0.154192
Trained 1051 batches 	Training Loss: 0.129618
Trained 1101 batches 	Training Loss: 0.181830
Trained 1151 batches 	Training Loss: 0.114970
Trained 1201 batches 	Training Loss: 0.179018
Trained 1251 batches 	Training Loss: 0.196223
Trained 1301 batches 	Training Loss: 0.177388
Trained 1351 batches 	Training Loss: 0.147278
Trained 1401 batches 	Training Loss: 0.097633
Trained 1451 batches 	Training Loss: 0.095390
Trained 1501 batches 	Training Loss: 0.133619
Trained 1551 batches 	Training Loss: 0.143468
Trained 1601 batches 	Training Loss: 0.102634
Trained 1651 batches 	Training Loss: 0.168951
Trained 1701 batches 	Training Loss: 0.268305
Trained 1751 batches 	Training Loss: 0.128497
Trained 1801 batches 	Training Loss: 0.130049
Trained 1851 batches 	Training Loss: 0.117334
Trained 1901 batches 	Training Loss: 0.122652
Trained 1951 batches 	Training Loss: 0.093185
Trained 2001 batches 	Training Loss: 0.154468
Trained 2051 batches 	Training Loss: 0.153270
Trained 2101 batches 	Training Loss: 0.150557
Trained 2151 batches 	Training Loss: 0.125474
Trained 2201 batches 	Training Loss: 0.202093
Trained 2251 batches 	Training Loss: 0.079683
Trained 2301 batches 	Training Loss: 0.199214
Trained 2351 batches 	Training Loss: 0.138016
Trained 2401 batches 	Training Loss: 0.101627
Trained 2451 batches 	Training Loss: 0.185513
Trained 2501 batches 	Training Loss: 0.158880
Trained 2551 batches 	Training Loss: 0.105247
Trained 2601 batches 	Training Loss: 0.112727
Trained 2651 batches 	Training Loss: 0.136042
Trained 2701 batches 	Training Loss: 0.175308
Trained 2751 batches 	Training Loss: 0.136719
Trained 2801 batches 	Training Loss: 0.210033
Trained 2851 batches 	Training Loss: 0.105408
Trained 2901 batches 	Training Loss: 0.122663
Trained 2951 batches 	Training Loss: 0.154722
Trained 3001 batches 	Training Loss: 0.194423
Trained 3051 batches 	Training Loss: 0.121184
Trained 3101 batches 	Training Loss: 0.159484
Trained 3151 batches 	Training Loss: 0.141836
Trained 3201 batches 	Training Loss: 0.125487
Trained 3251 batches 	Training Loss: 0.208763
Trained 3301 batches 	Training Loss: 0.085990
Trained 3351 batches 	Training Loss: 0.162281
Trained 3401 batches 	Training Loss: 0.158741
Trained 3451 batches 	Training Loss: 0.144145
Trained 3501 batches 	Training Loss: 0.121701
Trained 3551 batches 	Training Loss: 0.147110
Trained 3601 batches 	Training Loss: 0.110387
Trained 3651 batches 	Training Loss: 0.187452
Trained 3701 batches 	Training Loss: 0.234769
Trained 3751 batches 	Training Loss: 0.139835
Trained 3801 batches 	Training Loss: 0.132543
Trained 3851 batches 	Training Loss: 0.133735
Trained 3901 batches 	Training Loss: 0.129857
Trained 3951 batches 	Training Loss: 0.147186
Trained 4001 batches 	Training Loss: 0.178854
Trained 4051 batches 	Training Loss: 0.135121
Trained 4101 batches 	Training Loss: 0.177704
Trained 4151 batches 	Training Loss: 0.177153
Trained 4201 batches 	Training Loss: 0.149061
Trained 4251 batches 	Training Loss: 0.134588
Trained 4301 batches 	Training Loss: 0.094718
Trained 4351 batches 	Training Loss: 0.093092
Trained 4401 batches 	Training Loss: 0.151153
Trained 4451 batches 	Training Loss: 0.105517
Trained 4501 batches 	Training Loss: 0.189634
Trained 4551 batches 	Training Loss: 0.123423
Trained 4601 batches 	Training Loss: 0.124317
Trained 4651 batches 	Training Loss: 0.155465
Trained 4701 batches 	Training Loss: 0.156780
Trained 4751 batches 	Training Loss: 0.147877
Trained 4801 batches 	Training Loss: 0.214824
Trained 4851 batches 	Training Loss: 0.153437
Trained 4901 batches 	Training Loss: 0.140660
Epoch: 6 	Training Loss: 0.152884
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.804
The AUROC of Atelectasis is 0.7897761080296917
The AUROC of Cardiomegaly is 0.8937553493058801
The AUROC of Effusion is 0.8701254414717338
The AUROC of Infiltration is 0.7083527242368454
The AUROC of Mass is 0.8105274236531679
The AUROC of Nodule is 0.704412082958281
The AUROC of Pneumonia is 0.7385605754719953
The AUROC of Pneumothorax is 0.8325129901592235
The AUROC of Consolidation is 0.7835397155523695
The AUROC of Edema is 0.876462855119522
The AUROC of Emphysema is 0.8356224347449824
The AUROC of Fibrosis is 0.7753786796997505
The AUROC of Pleural_Thickening is 0.7631057900286092
The AUROC of Hernia is 0.8700316155414002
Started epoch 7
Trained 1 batches 	Training Loss: 0.049133
Trained 51 batches 	Training Loss: 0.157287
Trained 101 batches 	Training Loss: 0.213353
Trained 151 batches 	Training Loss: 0.117991
Trained 201 batches 	Training Loss: 0.110540
Trained 251 batches 	Training Loss: 0.156385
Trained 301 batches 	Training Loss: 0.123505
Trained 351 batches 	Training Loss: 0.178402
Trained 401 batches 	Training Loss: 0.171397
Trained 451 batches 	Training Loss: 0.173570
Trained 501 batches 	Training Loss: 0.184681
Trained 551 batches 	Training Loss: 0.160852
Trained 601 batches 	Training Loss: 0.118335
Trained 651 batches 	Training Loss: 0.179491
Trained 701 batches 	Training Loss: 0.161876
Trained 751 batches 	Training Loss: 0.139364
Trained 801 batches 	Training Loss: 0.115047
Trained 851 batches 	Training Loss: 0.168041
Trained 901 batches 	Training Loss: 0.224499
Trained 951 batches 	Training Loss: 0.145909
Trained 1001 batches 	Training Loss: 0.102151
Trained 1051 batches 	Training Loss: 0.107265
Trained 1101 batches 	Training Loss: 0.172999
Trained 1151 batches 	Training Loss: 0.137135
Trained 1201 batches 	Training Loss: 0.159980
Trained 1251 batches 	Training Loss: 0.091074
Trained 1301 batches 	Training Loss: 0.111912
Trained 1351 batches 	Training Loss: 0.144317
Trained 1401 batches 	Training Loss: 0.116889
Trained 1451 batches 	Training Loss: 0.142538
Trained 1501 batches 	Training Loss: 0.087089
Trained 1551 batches 	Training Loss: 0.179125
Trained 1601 batches 	Training Loss: 0.142910
Trained 1651 batches 	Training Loss: 0.101570
Trained 1701 batches 	Training Loss: 0.111962
Trained 1751 batches 	Training Loss: 0.297119
Trained 1801 batches 	Training Loss: 0.134274
Trained 1851 batches 	Training Loss: 0.142340
Trained 1901 batches 	Training Loss: 0.134975
Trained 1951 batches 	Training Loss: 0.183630
Trained 2001 batches 	Training Loss: 0.175246
Trained 2051 batches 	Training Loss: 0.148640
Trained 2101 batches 	Training Loss: 0.141601
Trained 2151 batches 	Training Loss: 0.286825
Trained 2201 batches 	Training Loss: 0.151962
Trained 2251 batches 	Training Loss: 0.093151
Trained 2301 batches 	Training Loss: 0.170455
Trained 2351 batches 	Training Loss: 0.125450
Trained 2401 batches 	Training Loss: 0.174184
Trained 2451 batches 	Training Loss: 0.252034
Trained 2501 batches 	Training Loss: 0.140172
Trained 2551 batches 	Training Loss: 0.080143
Trained 2601 batches 	Training Loss: 0.091055
Trained 2651 batches 	Training Loss: 0.185839
Trained 2701 batches 	Training Loss: 0.096516
Trained 2751 batches 	Training Loss: 0.170804
Trained 2801 batches 	Training Loss: 0.094359
Trained 2851 batches 	Training Loss: 0.262172
Trained 2901 batches 	Training Loss: 0.219970
Trained 2951 batches 	Training Loss: 0.111738
Trained 3001 batches 	Training Loss: 0.213850
Trained 3051 batches 	Training Loss: 0.164010
Trained 3101 batches 	Training Loss: 0.135397
Trained 3151 batches 	Training Loss: 0.090931
Trained 3201 batches 	Training Loss: 0.158710
Trained 3251 batches 	Training Loss: 0.216833
Trained 3301 batches 	Training Loss: 0.098450
Trained 3351 batches 	Training Loss: 0.149645
Trained 3401 batches 	Training Loss: 0.200980
Trained 3451 batches 	Training Loss: 0.147951
Trained 3501 batches 	Training Loss: 0.158139
Trained 3551 batches 	Training Loss: 0.161180
Trained 3601 batches 	Training Loss: 0.177793
Trained 3651 batches 	Training Loss: 0.056953
Trained 3701 batches 	Training Loss: 0.170712
Trained 3751 batches 	Training Loss: 0.200686
Trained 3801 batches 	Training Loss: 0.126716
Trained 3851 batches 	Training Loss: 0.121972
Trained 3901 batches 	Training Loss: 0.158092
Trained 3951 batches 	Training Loss: 0.126053
Trained 4001 batches 	Training Loss: 0.147153
Trained 4051 batches 	Training Loss: 0.119062
Trained 4101 batches 	Training Loss: 0.193245
Trained 4151 batches 	Training Loss: 0.156718
Trained 4201 batches 	Training Loss: 0.112199
Trained 4251 batches 	Training Loss: 0.189954
Trained 4301 batches 	Training Loss: 0.169004
Trained 4351 batches 	Training Loss: 0.221342
Trained 4401 batches 	Training Loss: 0.108139
Trained 4451 batches 	Training Loss: 0.169015
Trained 4501 batches 	Training Loss: 0.099291
Trained 4551 batches 	Training Loss: 0.131661
Trained 4601 batches 	Training Loss: 0.158682
Trained 4651 batches 	Training Loss: 0.136708
Trained 4701 batches 	Training Loss: 0.127621
Trained 4751 batches 	Training Loss: 0.119785
Trained 4801 batches 	Training Loss: 0.148599
Trained 4851 batches 	Training Loss: 0.148665
Trained 4901 batches 	Training Loss: 0.132437
Epoch: 7 	Training Loss: 0.150761
Started epoch 8
Trained 1 batches 	Training Loss: 0.187514
Trained 51 batches 	Training Loss: 0.111552
Trained 101 batches 	Training Loss: 0.120367
Trained 151 batches 	Training Loss: 0.137139
Trained 201 batches 	Training Loss: 0.151083
Trained 251 batches 	Training Loss: 0.174221
Trained 301 batches 	Training Loss: 0.222005
Trained 351 batches 	Training Loss: 0.163023
Trained 401 batches 	Training Loss: 0.233443
Trained 451 batches 	Training Loss: 0.191979
Trained 501 batches 	Training Loss: 0.152864
Trained 551 batches 	Training Loss: 0.095791
Trained 601 batches 	Training Loss: 0.120483
Trained 651 batches 	Training Loss: 0.180206
Trained 701 batches 	Training Loss: 0.179647
Trained 751 batches 	Training Loss: 0.134787
Trained 801 batches 	Training Loss: 0.059636
Trained 851 batches 	Training Loss: 0.177427
Trained 901 batches 	Training Loss: 0.122012
Trained 951 batches 	Training Loss: 0.121484
Trained 1001 batches 	Training Loss: 0.130681
Trained 1051 batches 	Training Loss: 0.178657
Trained 1101 batches 	Training Loss: 0.108326
Trained 1151 batches 	Training Loss: 0.164016
Trained 1201 batches 	Training Loss: 0.139922
Trained 1251 batches 	Training Loss: 0.099996
Trained 1301 batches 	Training Loss: 0.155849
Trained 1351 batches 	Training Loss: 0.113518
Trained 1401 batches 	Training Loss: 0.131151
Trained 1451 batches 	Training Loss: 0.120019
Trained 1501 batches 	Training Loss: 0.148844
Trained 1551 batches 	Training Loss: 0.133960
Trained 1601 batches 	Training Loss: 0.161765
Trained 1651 batches 	Training Loss: 0.123874
Trained 1701 batches 	Training Loss: 0.160474
Trained 1751 batches 	Training Loss: 0.096790
Trained 1801 batches 	Training Loss: 0.119629
Trained 1851 batches 	Training Loss: 0.174314
Trained 1901 batches 	Training Loss: 0.105868
Trained 1951 batches 	Training Loss: 0.223049
Trained 2001 batches 	Training Loss: 0.169162
Trained 2051 batches 	Training Loss: 0.153751
Trained 2101 batches 	Training Loss: 0.142334
Trained 2151 batches 	Training Loss: 0.197340
Trained 2201 batches 	Training Loss: 0.148883
Trained 2251 batches 	Training Loss: 0.156100
Trained 2301 batches 	Training Loss: 0.265256
Trained 2351 batches 	Training Loss: 0.166474
Trained 2401 batches 	Training Loss: 0.152217
Trained 2451 batches 	Training Loss: 0.122578
Trained 2501 batches 	Training Loss: 0.188827
Trained 2551 batches 	Training Loss: 0.137822
Trained 2601 batches 	Training Loss: 0.143117
Trained 2651 batches 	Training Loss: 0.147835
Trained 2701 batches 	Training Loss: 0.123618
Trained 2751 batches 	Training Loss: 0.188010
Trained 2801 batches 	Training Loss: 0.134856
Trained 2851 batches 	Training Loss: 0.130471
Trained 2901 batches 	Training Loss: 0.131239
Trained 2951 batches 	Training Loss: 0.132613
Trained 3001 batches 	Training Loss: 0.117465
Trained 3051 batches 	Training Loss: 0.172930
Trained 3101 batches 	Training Loss: 0.156177
Trained 3151 batches 	Training Loss: 0.112106
Trained 3201 batches 	Training Loss: 0.178220
Trained 3251 batches 	Training Loss: 0.175524
Trained 3301 batches 	Training Loss: 0.185383
Trained 3351 batches 	Training Loss: 0.139147
Trained 3401 batches 	Training Loss: 0.199823
Trained 3451 batches 	Training Loss: 0.102533
Trained 3501 batches 	Training Loss: 0.135718
Trained 3551 batches 	Training Loss: 0.108313
Trained 3601 batches 	Training Loss: 0.135990
Trained 3651 batches 	Training Loss: 0.128197
Trained 3701 batches 	Training Loss: 0.113870
Trained 3751 batches 	Training Loss: 0.148832
Trained 3801 batches 	Training Loss: 0.166175
Trained 3851 batches 	Training Loss: 0.181548
Trained 3901 batches 	Training Loss: 0.074555
Trained 3951 batches 	Training Loss: 0.187013
Trained 4001 batches 	Training Loss: 0.136561
Trained 4051 batches 	Training Loss: 0.144140
Trained 4101 batches 	Training Loss: 0.146216
Trained 4151 batches 	Training Loss: 0.124370
Trained 4201 batches 	Training Loss: 0.180357
Trained 4251 batches 	Training Loss: 0.118354
Trained 4301 batches 	Training Loss: 0.115090
Trained 4351 batches 	Training Loss: 0.135384
Trained 4401 batches 	Training Loss: 0.128340
Trained 4451 batches 	Training Loss: 0.195221
Trained 4501 batches 	Training Loss: 0.108347
Trained 4551 batches 	Training Loss: 0.118867
Trained 4601 batches 	Training Loss: 0.093876
Trained 4651 batches 	Training Loss: 0.122656
Trained 4701 batches 	Training Loss: 0.116598
Trained 4751 batches 	Training Loss: 0.207877
Trained 4801 batches 	Training Loss: 0.122567
Trained 4851 batches 	Training Loss: 0.188682
Trained 4901 batches 	Training Loss: 0.151247
Epoch: 8 	Training Loss: 0.148928
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.784389931548022
The AUROC of Cardiomegaly is 0.8920799922302282
The AUROC of Effusion is 0.8722265899029972
The AUROC of Infiltration is 0.7040587216937205
The AUROC of Mass is 0.8139200293293627
The AUROC of Nodule is 0.7148503128431867
The AUROC of Pneumonia is 0.7622040096331996
The AUROC of Pneumothorax is 0.8464192271064106
The AUROC of Consolidation is 0.7870600323471406
The AUROC of Edema is 0.8889974159428041
The AUROC of Emphysema is 0.8494883120391918
The AUROC of Fibrosis is 0.7898313446624408
The AUROC of Pleural_Thickening is 0.7463539023458654
The AUROC of Hernia is 0.8712996412933862
Started epoch 9
Trained 1 batches 	Training Loss: 0.147237
Trained 51 batches 	Training Loss: 0.137852
Trained 101 batches 	Training Loss: 0.176010
Trained 151 batches 	Training Loss: 0.164153
Trained 201 batches 	Training Loss: 0.172395
Trained 251 batches 	Training Loss: 0.190979
Trained 301 batches 	Training Loss: 0.210243
Trained 351 batches 	Training Loss: 0.101149
Trained 401 batches 	Training Loss: 0.285053
Trained 451 batches 	Training Loss: 0.096012
Trained 501 batches 	Training Loss: 0.081335
Trained 551 batches 	Training Loss: 0.122426
Trained 601 batches 	Training Loss: 0.106602
Trained 651 batches 	Training Loss: 0.136675
Trained 701 batches 	Training Loss: 0.110938
Trained 751 batches 	Training Loss: 0.049917
Trained 801 batches 	Training Loss: 0.220550
Trained 851 batches 	Training Loss: 0.149827
Trained 901 batches 	Training Loss: 0.141885
Trained 951 batches 	Training Loss: 0.160539
Trained 1001 batches 	Training Loss: 0.174148
Trained 1051 batches 	Training Loss: 0.142914
Trained 1101 batches 	Training Loss: 0.153581
Trained 1151 batches 	Training Loss: 0.131000
Trained 1201 batches 	Training Loss: 0.215589
Trained 1251 batches 	Training Loss: 0.069913
Trained 1301 batches 	Training Loss: 0.144677
Trained 1351 batches 	Training Loss: 0.169252
Trained 1401 batches 	Training Loss: 0.160406
Trained 1451 batches 	Training Loss: 0.147222
Trained 1501 batches 	Training Loss: 0.136906
Trained 1551 batches 	Training Loss: 0.160172
Trained 1601 batches 	Training Loss: 0.125394
Trained 1651 batches 	Training Loss: 0.087400
Trained 1701 batches 	Training Loss: 0.238764
Trained 1751 batches 	Training Loss: 0.105976
Trained 1801 batches 	Training Loss: 0.224504
Trained 1851 batches 	Training Loss: 0.262897
Trained 1901 batches 	Training Loss: 0.146528
Trained 1951 batches 	Training Loss: 0.151465
Trained 2001 batches 	Training Loss: 0.364472
Trained 2051 batches 	Training Loss: 0.074239
Trained 2101 batches 	Training Loss: 0.158522
Trained 2151 batches 	Training Loss: 0.142422
Trained 2201 batches 	Training Loss: 0.153104
Trained 2251 batches 	Training Loss: 0.168045
Trained 2301 batches 	Training Loss: 0.131684
Trained 2351 batches 	Training Loss: 0.133532
Trained 2401 batches 	Training Loss: 0.171657
Trained 2451 batches 	Training Loss: 0.167990
Trained 2501 batches 	Training Loss: 0.181810
Trained 2551 batches 	Training Loss: 0.171608
Trained 2601 batches 	Training Loss: 0.130243
Trained 2651 batches 	Training Loss: 0.111720
Trained 2701 batches 	Training Loss: 0.146887
Trained 2751 batches 	Training Loss: 0.092123
Trained 2801 batches 	Training Loss: 0.278515
Trained 2851 batches 	Training Loss: 0.192926
Trained 2901 batches 	Training Loss: 0.219224
Trained 2951 batches 	Training Loss: 0.128832
Trained 3001 batches 	Training Loss: 0.160176
Trained 3051 batches 	Training Loss: 0.098033
Trained 3101 batches 	Training Loss: 0.134983
Trained 3151 batches 	Training Loss: 0.091145
Trained 3201 batches 	Training Loss: 0.057400
Trained 3251 batches 	Training Loss: 0.163566
Trained 3301 batches 	Training Loss: 0.094756
Trained 3351 batches 	Training Loss: 0.119903
Trained 3401 batches 	Training Loss: 0.118533
Trained 3451 batches 	Training Loss: 0.137181
Trained 3501 batches 	Training Loss: 0.201925
Trained 3551 batches 	Training Loss: 0.158360
Trained 3601 batches 	Training Loss: 0.198074
Trained 3651 batches 	Training Loss: 0.145626
Trained 3701 batches 	Training Loss: 0.121974
Trained 3751 batches 	Training Loss: 0.071337
Trained 3801 batches 	Training Loss: 0.136824
Trained 3851 batches 	Training Loss: 0.124933
Trained 3901 batches 	Training Loss: 0.135792
Trained 3951 batches 	Training Loss: 0.138717
Trained 4001 batches 	Training Loss: 0.138846
Trained 4051 batches 	Training Loss: 0.165994
Trained 4101 batches 	Training Loss: 0.128547
Trained 4151 batches 	Training Loss: 0.108965
Trained 4201 batches 	Training Loss: 0.127906
Trained 4251 batches 	Training Loss: 0.177602
Trained 4301 batches 	Training Loss: 0.151423
Trained 4351 batches 	Training Loss: 0.106158
Trained 4401 batches 	Training Loss: 0.099594
Trained 4451 batches 	Training Loss: 0.214787
Trained 4501 batches 	Training Loss: 0.081879
Trained 4551 batches 	Training Loss: 0.169263
Trained 4601 batches 	Training Loss: 0.140366
Trained 4651 batches 	Training Loss: 0.153058
Trained 4701 batches 	Training Loss: 0.165908
Trained 4751 batches 	Training Loss: 0.138262
Trained 4801 batches 	Training Loss: 0.210710
Trained 4851 batches 	Training Loss: 0.159913
Trained 4901 batches 	Training Loss: 0.171210
Epoch: 9 	Training Loss: 0.147236
Started epoch 10
Trained 1 batches 	Training Loss: 0.151150
Trained 51 batches 	Training Loss: 0.167245
Trained 101 batches 	Training Loss: 0.114184
Trained 151 batches 	Training Loss: 0.167094
Trained 201 batches 	Training Loss: 0.104250
Trained 251 batches 	Training Loss: 0.109925
Trained 301 batches 	Training Loss: 0.165774
Trained 351 batches 	Training Loss: 0.086137
Trained 401 batches 	Training Loss: 0.199050
Trained 451 batches 	Training Loss: 0.135695
Trained 501 batches 	Training Loss: 0.145284
Trained 551 batches 	Training Loss: 0.106916
Trained 601 batches 	Training Loss: 0.108587
Trained 651 batches 	Training Loss: 0.164903
Trained 701 batches 	Training Loss: 0.117824
Trained 751 batches 	Training Loss: 0.130335
Trained 801 batches 	Training Loss: 0.160698
Trained 851 batches 	Training Loss: 0.085820
Trained 901 batches 	Training Loss: 0.128448
Trained 951 batches 	Training Loss: 0.103085
Trained 1001 batches 	Training Loss: 0.113149
Trained 1051 batches 	Training Loss: 0.241627
Trained 1101 batches 	Training Loss: 0.121105
Trained 1151 batches 	Training Loss: 0.210971
Trained 1201 batches 	Training Loss: 0.126196
Trained 1251 batches 	Training Loss: 0.114927
Trained 1301 batches 	Training Loss: 0.171027
Trained 1351 batches 	Training Loss: 0.241231
Trained 1401 batches 	Training Loss: 0.155304
Trained 1451 batches 	Training Loss: 0.117034
Trained 1501 batches 	Training Loss: 0.158120
Trained 1551 batches 	Training Loss: 0.146759
Trained 1601 batches 	Training Loss: 0.099648
Trained 1651 batches 	Training Loss: 0.179381
Trained 1701 batches 	Training Loss: 0.115963
Trained 1751 batches 	Training Loss: 0.091248
Trained 1801 batches 	Training Loss: 0.122078
Trained 1851 batches 	Training Loss: 0.176889
Trained 1901 batches 	Training Loss: 0.074637
Trained 1951 batches 	Training Loss: 0.136508
Trained 2001 batches 	Training Loss: 0.133230
Trained 2051 batches 	Training Loss: 0.164908
Trained 2101 batches 	Training Loss: 0.139574
Trained 2151 batches 	Training Loss: 0.197758
Trained 2201 batches 	Training Loss: 0.116869
Trained 2251 batches 	Training Loss: 0.123405
Trained 2301 batches 	Training Loss: 0.160004
Trained 2351 batches 	Training Loss: 0.144073
Trained 2401 batches 	Training Loss: 0.147641
Trained 2451 batches 	Training Loss: 0.236959
Trained 2501 batches 	Training Loss: 0.124372
Trained 2551 batches 	Training Loss: 0.168955
Trained 2601 batches 	Training Loss: 0.115257
Trained 2651 batches 	Training Loss: 0.153404
Trained 2701 batches 	Training Loss: 0.181991
Trained 2751 batches 	Training Loss: 0.129559
Trained 2801 batches 	Training Loss: 0.193182
Trained 2851 batches 	Training Loss: 0.149870
Trained 2901 batches 	Training Loss: 0.141334
Trained 2951 batches 	Training Loss: 0.125078
Trained 3001 batches 	Training Loss: 0.111314
Trained 3051 batches 	Training Loss: 0.118463
Trained 3101 batches 	Training Loss: 0.113026
Trained 3151 batches 	Training Loss: 0.135398
Trained 3201 batches 	Training Loss: 0.099099
Trained 3251 batches 	Training Loss: 0.104292
Trained 3301 batches 	Training Loss: 0.177445
Trained 3351 batches 	Training Loss: 0.129422
Trained 3401 batches 	Training Loss: 0.217911
Trained 3451 batches 	Training Loss: 0.155032
Trained 3501 batches 	Training Loss: 0.198723
Trained 3551 batches 	Training Loss: 0.165602
Trained 3601 batches 	Training Loss: 0.184544
Trained 3651 batches 	Training Loss: 0.204497
Trained 3701 batches 	Training Loss: 0.124495
Trained 3751 batches 	Training Loss: 0.116928
Trained 3801 batches 	Training Loss: 0.095216
Trained 3851 batches 	Training Loss: 0.110897
Trained 3901 batches 	Training Loss: 0.122745
Trained 3951 batches 	Training Loss: 0.195711
Trained 4001 batches 	Training Loss: 0.176069
Trained 4051 batches 	Training Loss: 0.105740
Trained 4101 batches 	Training Loss: 0.265762
Trained 4151 batches 	Training Loss: 0.142477
Trained 4201 batches 	Training Loss: 0.129085
Trained 4251 batches 	Training Loss: 0.147536
Trained 4301 batches 	Training Loss: 0.130426
Trained 4351 batches 	Training Loss: 0.150098
Trained 4401 batches 	Training Loss: 0.140741
Trained 4451 batches 	Training Loss: 0.130959
Trained 4501 batches 	Training Loss: 0.138952
Trained 4551 batches 	Training Loss: 0.130719
Trained 4601 batches 	Training Loss: 0.165115
Trained 4651 batches 	Training Loss: 0.106509
Trained 4701 batches 	Training Loss: 0.171123
Trained 4751 batches 	Training Loss: 0.166147
Trained 4801 batches 	Training Loss: 0.171681
Trained 4851 batches 	Training Loss: 0.184734
Trained 4901 batches 	Training Loss: 0.157365
Epoch: 10 	Training Loss: 0.145335
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.819
The AUROC of Atelectasis is 0.7931435078218952
The AUROC of Cardiomegaly is 0.9003887921039693
The AUROC of Effusion is 0.8760233980018868
The AUROC of Infiltration is 0.7085651704019871
The AUROC of Mass is 0.8176353667374268
The AUROC of Nodule is 0.7338285858490269
The AUROC of Pneumonia is 0.7686385516982668
The AUROC of Pneumothorax is 0.8520526957713065
The AUROC of Consolidation is 0.7864859489999637
The AUROC of Edema is 0.8908544716243398
The AUROC of Emphysema is 0.8686252577192494
The AUROC of Fibrosis is 0.7726365111952921
The AUROC of Pleural_Thickening is 0.7724769099987338
The AUROC of Hernia is 0.9259992085476849
Training time lapse: 107.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.813
The AUROC of Atelectasis is 0.7918263211610711
The AUROC of Cardiomegaly is 0.8966455630141744
The AUROC of Effusion is 0.8773496003517194
The AUROC of Infiltration is 0.7007212433533367
The AUROC of Mass is 0.8156071371925888
The AUROC of Nodule is 0.7212641357103471
The AUROC of Pneumonia is 0.7406003515268047
The AUROC of Pneumothorax is 0.856946009163153
The AUROC of Consolidation is 0.7968246138347919
The AUROC of Edema is 0.900835705266545
The AUROC of Emphysema is 0.8721067380160721
The AUROC of Fibrosis is 0.7781172685890422
The AUROC of Pleural_Thickening is 0.7751315041997483
The AUROC of Hernia is 0.8612106265459347
