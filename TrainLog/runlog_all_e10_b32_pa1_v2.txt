Started training, total epoch : 10
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.679089
Trained 51 batches 	Training Loss: 0.166987
Trained 101 batches 	Training Loss: 0.186483
Trained 151 batches 	Training Loss: 0.126978
Trained 201 batches 	Training Loss: 0.227989
Trained 251 batches 	Training Loss: 0.127242
Trained 301 batches 	Training Loss: 0.150240
Trained 351 batches 	Training Loss: 0.188550
Trained 401 batches 	Training Loss: 0.168014
Trained 451 batches 	Training Loss: 0.186111
Trained 501 batches 	Training Loss: 0.165661
Trained 551 batches 	Training Loss: 0.160364
Trained 601 batches 	Training Loss: 0.134202
Trained 651 batches 	Training Loss: 0.156180
Trained 701 batches 	Training Loss: 0.157852
Trained 751 batches 	Training Loss: 0.172950
Trained 801 batches 	Training Loss: 0.164846
Trained 851 batches 	Training Loss: 0.147247
Trained 901 batches 	Training Loss: 0.155106
Trained 951 batches 	Training Loss: 0.188830
Trained 1001 batches 	Training Loss: 0.124932
Trained 1051 batches 	Training Loss: 0.153001
Trained 1101 batches 	Training Loss: 0.127937
Trained 1151 batches 	Training Loss: 0.114672
Trained 1201 batches 	Training Loss: 0.168765
Trained 1251 batches 	Training Loss: 0.209735
Trained 1301 batches 	Training Loss: 0.131870
Trained 1351 batches 	Training Loss: 0.172682
Trained 1401 batches 	Training Loss: 0.154790
Trained 1451 batches 	Training Loss: 0.140330
Trained 1501 batches 	Training Loss: 0.157626
Trained 1551 batches 	Training Loss: 0.155360
Trained 1601 batches 	Training Loss: 0.177124
Trained 1651 batches 	Training Loss: 0.164010
Trained 1701 batches 	Training Loss: 0.158066
Trained 1751 batches 	Training Loss: 0.123655
Trained 1801 batches 	Training Loss: 0.188112
Trained 1851 batches 	Training Loss: 0.121465
Trained 1901 batches 	Training Loss: 0.120764
Trained 1951 batches 	Training Loss: 0.217935
Trained 2001 batches 	Training Loss: 0.175350
Trained 2051 batches 	Training Loss: 0.130699
Trained 2101 batches 	Training Loss: 0.121797
Trained 2151 batches 	Training Loss: 0.173783
Trained 2201 batches 	Training Loss: 0.172281
Trained 2251 batches 	Training Loss: 0.203040
Trained 2301 batches 	Training Loss: 0.163449
Trained 2351 batches 	Training Loss: 0.159031
Trained 2401 batches 	Training Loss: 0.192190
Trained 2451 batches 	Training Loss: 0.130414
Epoch: 1 	Training Loss: 0.167470
Started epoch 2
Trained 1 batches 	Training Loss: 0.164421
Trained 51 batches 	Training Loss: 0.129303
Trained 101 batches 	Training Loss: 0.202371
Trained 151 batches 	Training Loss: 0.130972
Trained 201 batches 	Training Loss: 0.194675
Trained 251 batches 	Training Loss: 0.189592
Trained 301 batches 	Training Loss: 0.127706
Trained 351 batches 	Training Loss: 0.119572
Trained 401 batches 	Training Loss: 0.176682
Trained 451 batches 	Training Loss: 0.147912
Trained 501 batches 	Training Loss: 0.143264
Trained 551 batches 	Training Loss: 0.158731
Trained 601 batches 	Training Loss: 0.143452
Trained 651 batches 	Training Loss: 0.111754
Trained 701 batches 	Training Loss: 0.124393
Trained 751 batches 	Training Loss: 0.132678
Trained 801 batches 	Training Loss: 0.140534
Trained 851 batches 	Training Loss: 0.134209
Trained 901 batches 	Training Loss: 0.113983
Trained 951 batches 	Training Loss: 0.175003
Trained 1001 batches 	Training Loss: 0.172457
Trained 1051 batches 	Training Loss: 0.158411
Trained 1101 batches 	Training Loss: 0.176785
Trained 1151 batches 	Training Loss: 0.133451
Trained 1201 batches 	Training Loss: 0.126903
Trained 1251 batches 	Training Loss: 0.201890
Trained 1301 batches 	Training Loss: 0.231669
Trained 1351 batches 	Training Loss: 0.158375
Trained 1401 batches 	Training Loss: 0.150921
Trained 1451 batches 	Training Loss: 0.147488
Trained 1501 batches 	Training Loss: 0.131470
Trained 1551 batches 	Training Loss: 0.201108
Trained 1601 batches 	Training Loss: 0.183285
Trained 1651 batches 	Training Loss: 0.155805
Trained 1701 batches 	Training Loss: 0.161480
Trained 1751 batches 	Training Loss: 0.164744
Trained 1801 batches 	Training Loss: 0.157371
Trained 1851 batches 	Training Loss: 0.149459
Trained 1901 batches 	Training Loss: 0.204128
Trained 1951 batches 	Training Loss: 0.169058
Trained 2001 batches 	Training Loss: 0.152009
Trained 2051 batches 	Training Loss: 0.121988
Trained 2101 batches 	Training Loss: 0.164653
Trained 2151 batches 	Training Loss: 0.140317
Trained 2201 batches 	Training Loss: 0.168111
Trained 2251 batches 	Training Loss: 0.124913
Trained 2301 batches 	Training Loss: 0.131800
Trained 2351 batches 	Training Loss: 0.151134
Trained 2401 batches 	Training Loss: 0.193167
Trained 2451 batches 	Training Loss: 0.140243
Epoch: 2 	Training Loss: 0.158749
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.794
The AUROC of Atelectasis is 0.7841926403686436
The AUROC of Cardiomegaly is 0.891805925665135
The AUROC of Effusion is 0.8673082999709718
The AUROC of Infiltration is 0.6944643020774646
The AUROC of Mass is 0.7886806990889722
The AUROC of Nodule is 0.7157042217874428
The AUROC of Pneumonia is 0.7500372269901406
The AUROC of Pneumothorax is 0.8261428833976576
The AUROC of Consolidation is 0.7797241700699835
The AUROC of Edema is 0.8688274557960477
The AUROC of Emphysema is 0.8223804316382871
The AUROC of Fibrosis is 0.7511619789355957
The AUROC of Pleural_Thickening is 0.7352241014821648
The AUROC of Hernia is 0.8361353298356247
Started epoch 3
Trained 1 batches 	Training Loss: 0.187522
Trained 51 batches 	Training Loss: 0.148973
Trained 101 batches 	Training Loss: 0.169706
Trained 151 batches 	Training Loss: 0.230717
Trained 201 batches 	Training Loss: 0.166556
Trained 251 batches 	Training Loss: 0.161015
Trained 301 batches 	Training Loss: 0.145549
Trained 351 batches 	Training Loss: 0.133068
Trained 401 batches 	Training Loss: 0.150447
Trained 451 batches 	Training Loss: 0.163111
Trained 501 batches 	Training Loss: 0.116502
Trained 551 batches 	Training Loss: 0.155312
Trained 601 batches 	Training Loss: 0.192075
Trained 651 batches 	Training Loss: 0.184827
Trained 701 batches 	Training Loss: 0.150204
Trained 751 batches 	Training Loss: 0.155286
Trained 801 batches 	Training Loss: 0.135583
Trained 851 batches 	Training Loss: 0.150279
Trained 901 batches 	Training Loss: 0.142863
Trained 951 batches 	Training Loss: 0.134978
Trained 1001 batches 	Training Loss: 0.152292
Trained 1051 batches 	Training Loss: 0.154401
Trained 1101 batches 	Training Loss: 0.109406
Trained 1151 batches 	Training Loss: 0.109363
Trained 1201 batches 	Training Loss: 0.134856
Trained 1251 batches 	Training Loss: 0.166552
Trained 1301 batches 	Training Loss: 0.177452
Trained 1351 batches 	Training Loss: 0.137352
Trained 1401 batches 	Training Loss: 0.156777
Trained 1451 batches 	Training Loss: 0.128736
Trained 1501 batches 	Training Loss: 0.155876
Trained 1551 batches 	Training Loss: 0.183536
Trained 1601 batches 	Training Loss: 0.117108
Trained 1651 batches 	Training Loss: 0.137076
Trained 1701 batches 	Training Loss: 0.133334
Trained 1751 batches 	Training Loss: 0.099262
Trained 1801 batches 	Training Loss: 0.177098
Trained 1851 batches 	Training Loss: 0.154862
Trained 1901 batches 	Training Loss: 0.109591
Trained 1951 batches 	Training Loss: 0.137636
Trained 2001 batches 	Training Loss: 0.139147
Trained 2051 batches 	Training Loss: 0.153241
Trained 2101 batches 	Training Loss: 0.205035
Trained 2151 batches 	Training Loss: 0.150375
Trained 2201 batches 	Training Loss: 0.189122
Trained 2251 batches 	Training Loss: 0.154737
Trained 2301 batches 	Training Loss: 0.167216
Trained 2351 batches 	Training Loss: 0.145105
Trained 2401 batches 	Training Loss: 0.147486
Trained 2451 batches 	Training Loss: 0.123748
Epoch: 3 	Training Loss: 0.154693
Started epoch 4
Trained 1 batches 	Training Loss: 0.179065
Trained 51 batches 	Training Loss: 0.132848
Trained 101 batches 	Training Loss: 0.161229
Trained 151 batches 	Training Loss: 0.138276
Trained 201 batches 	Training Loss: 0.155277
Trained 251 batches 	Training Loss: 0.164614
Trained 301 batches 	Training Loss: 0.186359
Trained 351 batches 	Training Loss: 0.196919
Trained 401 batches 	Training Loss: 0.154608
Trained 451 batches 	Training Loss: 0.132804
Trained 501 batches 	Training Loss: 0.124379
Trained 551 batches 	Training Loss: 0.164491
Trained 601 batches 	Training Loss: 0.110877
Trained 651 batches 	Training Loss: 0.189206
Trained 701 batches 	Training Loss: 0.131932
Trained 751 batches 	Training Loss: 0.139645
Trained 801 batches 	Training Loss: 0.167697
Trained 851 batches 	Training Loss: 0.179965
Trained 901 batches 	Training Loss: 0.157521
Trained 951 batches 	Training Loss: 0.132802
Trained 1001 batches 	Training Loss: 0.107230
Trained 1051 batches 	Training Loss: 0.105421
Trained 1101 batches 	Training Loss: 0.147989
Trained 1151 batches 	Training Loss: 0.117508
Trained 1201 batches 	Training Loss: 0.166579
Trained 1251 batches 	Training Loss: 0.119271
Trained 1301 batches 	Training Loss: 0.164223
Trained 1351 batches 	Training Loss: 0.148882
Trained 1401 batches 	Training Loss: 0.117994
Trained 1451 batches 	Training Loss: 0.157112
Trained 1501 batches 	Training Loss: 0.155043
Trained 1551 batches 	Training Loss: 0.145536
Trained 1601 batches 	Training Loss: 0.195704
Trained 1651 batches 	Training Loss: 0.189056
Trained 1701 batches 	Training Loss: 0.199268
Trained 1751 batches 	Training Loss: 0.126468
Trained 1801 batches 	Training Loss: 0.132298
Trained 1851 batches 	Training Loss: 0.144491
Trained 1901 batches 	Training Loss: 0.133040
Trained 1951 batches 	Training Loss: 0.151356
Trained 2001 batches 	Training Loss: 0.145795
Trained 2051 batches 	Training Loss: 0.144737
Trained 2101 batches 	Training Loss: 0.122844
Trained 2151 batches 	Training Loss: 0.150683
Trained 2201 batches 	Training Loss: 0.142657
Trained 2251 batches 	Training Loss: 0.141708
Trained 2301 batches 	Training Loss: 0.125276
Trained 2351 batches 	Training Loss: 0.178005
Trained 2401 batches 	Training Loss: 0.130404
Trained 2451 batches 	Training Loss: 0.202314
Epoch: 4 	Training Loss: 0.151726
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.806
The AUROC of Atelectasis is 0.7963843018336115
The AUROC of Cardiomegaly is 0.9007007970086376
The AUROC of Effusion is 0.8728744345533275
The AUROC of Infiltration is 0.706516605798764
The AUROC of Mass is 0.8175047982557728
The AUROC of Nodule is 0.7256315001434186
The AUROC of Pneumonia is 0.7487138790810088
The AUROC of Pneumothorax is 0.8370866957378912
The AUROC of Consolidation is 0.7882478936549548
The AUROC of Edema is 0.8791230026292595
The AUROC of Emphysema is 0.8462640649509809
The AUROC of Fibrosis is 0.7769814712033389
The AUROC of Pleural_Thickening is 0.7298924683134219
The AUROC of Hernia is 0.8566194773861648
Started epoch 5
Trained 1 batches 	Training Loss: 0.140758
Trained 51 batches 	Training Loss: 0.178705
Trained 101 batches 	Training Loss: 0.173066
Trained 151 batches 	Training Loss: 0.140231
Trained 201 batches 	Training Loss: 0.188095
Trained 251 batches 	Training Loss: 0.117150
Trained 301 batches 	Training Loss: 0.157233
Trained 351 batches 	Training Loss: 0.157170
Trained 401 batches 	Training Loss: 0.132126
Trained 451 batches 	Training Loss: 0.185754
Trained 501 batches 	Training Loss: 0.159076
Trained 551 batches 	Training Loss: 0.213960
Trained 601 batches 	Training Loss: 0.134897
Trained 651 batches 	Training Loss: 0.108503
Trained 701 batches 	Training Loss: 0.123294
Trained 751 batches 	Training Loss: 0.103104
Trained 801 batches 	Training Loss: 0.129868
Trained 851 batches 	Training Loss: 0.175897
Trained 901 batches 	Training Loss: 0.150317
Trained 951 batches 	Training Loss: 0.138405
Trained 1001 batches 	Training Loss: 0.132937
Trained 1051 batches 	Training Loss: 0.140642
Trained 1101 batches 	Training Loss: 0.137069
Trained 1151 batches 	Training Loss: 0.154851
Trained 1201 batches 	Training Loss: 0.123461
Trained 1251 batches 	Training Loss: 0.155121
Trained 1301 batches 	Training Loss: 0.153603
Trained 1351 batches 	Training Loss: 0.128224
Trained 1401 batches 	Training Loss: 0.162174
Trained 1451 batches 	Training Loss: 0.186049
Trained 1501 batches 	Training Loss: 0.116724
Trained 1551 batches 	Training Loss: 0.183571
Trained 1601 batches 	Training Loss: 0.190029
Trained 1651 batches 	Training Loss: 0.158661
Trained 1701 batches 	Training Loss: 0.140378
Trained 1751 batches 	Training Loss: 0.155485
Trained 1801 batches 	Training Loss: 0.175134
Trained 1851 batches 	Training Loss: 0.140233
Trained 1901 batches 	Training Loss: 0.167294
Trained 1951 batches 	Training Loss: 0.198918
Trained 2001 batches 	Training Loss: 0.167960
Trained 2051 batches 	Training Loss: 0.146785
Trained 2101 batches 	Training Loss: 0.168557
Trained 2151 batches 	Training Loss: 0.137427
Trained 2201 batches 	Training Loss: 0.146757
Trained 2251 batches 	Training Loss: 0.108807
Trained 2301 batches 	Training Loss: 0.163507
Trained 2351 batches 	Training Loss: 0.135231
Trained 2401 batches 	Training Loss: 0.151737
Trained 2451 batches 	Training Loss: 0.149557
Epoch: 5 	Training Loss: 0.149094
Started epoch 6
Trained 1 batches 	Training Loss: 0.159649
Trained 51 batches 	Training Loss: 0.162388
Trained 101 batches 	Training Loss: 0.142579
Trained 151 batches 	Training Loss: 0.153303
Trained 201 batches 	Training Loss: 0.105596
Trained 251 batches 	Training Loss: 0.137067
Trained 301 batches 	Training Loss: 0.124705
Trained 351 batches 	Training Loss: 0.141083
Trained 401 batches 	Training Loss: 0.132065
Trained 451 batches 	Training Loss: 0.155284
Trained 501 batches 	Training Loss: 0.152922
Trained 551 batches 	Training Loss: 0.173146
Trained 601 batches 	Training Loss: 0.179057
Trained 651 batches 	Training Loss: 0.131558
Trained 701 batches 	Training Loss: 0.144314
Trained 751 batches 	Training Loss: 0.176466
Trained 801 batches 	Training Loss: 0.149203
Trained 851 batches 	Training Loss: 0.203194
Trained 901 batches 	Training Loss: 0.110519
Trained 951 batches 	Training Loss: 0.130127
Trained 1001 batches 	Training Loss: 0.174120
Trained 1051 batches 	Training Loss: 0.176960
Trained 1101 batches 	Training Loss: 0.119081
Trained 1151 batches 	Training Loss: 0.159446
Trained 1201 batches 	Training Loss: 0.183997
Trained 1251 batches 	Training Loss: 0.167550
Trained 1301 batches 	Training Loss: 0.128544
Trained 1351 batches 	Training Loss: 0.157089
Trained 1401 batches 	Training Loss: 0.154981
Trained 1451 batches 	Training Loss: 0.113434
Trained 1501 batches 	Training Loss: 0.147366
Trained 1551 batches 	Training Loss: 0.129158
Trained 1601 batches 	Training Loss: 0.117188
Trained 1651 batches 	Training Loss: 0.151653
Trained 1701 batches 	Training Loss: 0.178242
Trained 1751 batches 	Training Loss: 0.136667
Trained 1801 batches 	Training Loss: 0.152859
Trained 1851 batches 	Training Loss: 0.155554
Trained 1901 batches 	Training Loss: 0.226007
Trained 1951 batches 	Training Loss: 0.096986
Trained 2001 batches 	Training Loss: 0.151460
Trained 2051 batches 	Training Loss: 0.138061
Trained 2101 batches 	Training Loss: 0.150722
Trained 2151 batches 	Training Loss: 0.152916
Trained 2201 batches 	Training Loss: 0.155680
Trained 2251 batches 	Training Loss: 0.127855
Trained 2301 batches 	Training Loss: 0.182573
Trained 2351 batches 	Training Loss: 0.196083
Trained 2401 batches 	Training Loss: 0.175163
Trained 2451 batches 	Training Loss: 0.123597
Epoch: 6 	Training Loss: 0.146593
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.821
The AUROC of Atelectasis is 0.8011875995577962
The AUROC of Cardiomegaly is 0.8957876302802582
The AUROC of Effusion is 0.8738210285686641
The AUROC of Infiltration is 0.710720695748077
The AUROC of Mass is 0.8318350969010763
The AUROC of Nodule is 0.7444123617258057
The AUROC of Pneumonia is 0.7612676076504328
The AUROC of Pneumothorax is 0.8505881075301155
The AUROC of Consolidation is 0.7907261276098465
The AUROC of Edema is 0.8874580938246308
The AUROC of Emphysema is 0.8787138572302209
The AUROC of Fibrosis is 0.8040002131467221
The AUROC of Pleural_Thickening is 0.7528013350228586
The AUROC of Hernia is 0.9063192786720622
Started epoch 7
Trained 1 batches 	Training Loss: 0.108907
Trained 51 batches 	Training Loss: 0.135989
Trained 101 batches 	Training Loss: 0.132848
Trained 151 batches 	Training Loss: 0.138371
Trained 201 batches 	Training Loss: 0.107515
Trained 251 batches 	Training Loss: 0.148616
Trained 301 batches 	Training Loss: 0.101121
Trained 351 batches 	Training Loss: 0.228843
Trained 401 batches 	Training Loss: 0.182537
Trained 451 batches 	Training Loss: 0.184330
Trained 501 batches 	Training Loss: 0.187089
Trained 551 batches 	Training Loss: 0.115329
Trained 601 batches 	Training Loss: 0.191324
Trained 651 batches 	Training Loss: 0.100762
Trained 701 batches 	Training Loss: 0.097056
Trained 751 batches 	Training Loss: 0.137348
Trained 801 batches 	Training Loss: 0.179610
Trained 851 batches 	Training Loss: 0.177535
Trained 901 batches 	Training Loss: 0.128047
Trained 951 batches 	Training Loss: 0.138288
Trained 1001 batches 	Training Loss: 0.148614
Trained 1051 batches 	Training Loss: 0.131459
Trained 1101 batches 	Training Loss: 0.139891
Trained 1151 batches 	Training Loss: 0.116630
Trained 1201 batches 	Training Loss: 0.168763
Trained 1251 batches 	Training Loss: 0.183758
Trained 1301 batches 	Training Loss: 0.123699
Trained 1351 batches 	Training Loss: 0.140845
Trained 1401 batches 	Training Loss: 0.127648
Trained 1451 batches 	Training Loss: 0.125627
Trained 1501 batches 	Training Loss: 0.125743
Trained 1551 batches 	Training Loss: 0.140016
Trained 1601 batches 	Training Loss: 0.140812
Trained 1651 batches 	Training Loss: 0.106852
Trained 1701 batches 	Training Loss: 0.122931
Trained 1751 batches 	Training Loss: 0.148818
Trained 1801 batches 	Training Loss: 0.121258
Trained 1851 batches 	Training Loss: 0.150041
Trained 1901 batches 	Training Loss: 0.128339
Trained 1951 batches 	Training Loss: 0.136364
Trained 2001 batches 	Training Loss: 0.111671
Trained 2051 batches 	Training Loss: 0.132935
Trained 2101 batches 	Training Loss: 0.117291
Trained 2151 batches 	Training Loss: 0.109575
Trained 2201 batches 	Training Loss: 0.126965
Trained 2251 batches 	Training Loss: 0.117591
Trained 2301 batches 	Training Loss: 0.139468
Trained 2351 batches 	Training Loss: 0.173955
Trained 2401 batches 	Training Loss: 0.108768
Trained 2451 batches 	Training Loss: 0.184840
Epoch: 7 	Training Loss: 0.143994
Started epoch 8
Trained 1 batches 	Training Loss: 0.191813
Trained 51 batches 	Training Loss: 0.097912
Trained 101 batches 	Training Loss: 0.116229
Trained 151 batches 	Training Loss: 0.124043
Trained 201 batches 	Training Loss: 0.114246
Trained 251 batches 	Training Loss: 0.150095
Trained 301 batches 	Training Loss: 0.136758
Trained 351 batches 	Training Loss: 0.153741
Trained 401 batches 	Training Loss: 0.121894
Trained 451 batches 	Training Loss: 0.165406
Trained 501 batches 	Training Loss: 0.089213
Trained 551 batches 	Training Loss: 0.191690
Trained 601 batches 	Training Loss: 0.147021
Trained 651 batches 	Training Loss: 0.126033
Trained 701 batches 	Training Loss: 0.149999
Trained 751 batches 	Training Loss: 0.176827
Trained 801 batches 	Training Loss: 0.174598
Trained 851 batches 	Training Loss: 0.129044
Trained 901 batches 	Training Loss: 0.134810
Trained 951 batches 	Training Loss: 0.167255
Trained 1001 batches 	Training Loss: 0.133614
Trained 1051 batches 	Training Loss: 0.162772
Trained 1101 batches 	Training Loss: 0.133283
Trained 1151 batches 	Training Loss: 0.146593
Trained 1201 batches 	Training Loss: 0.105624
Trained 1251 batches 	Training Loss: 0.165357
Trained 1301 batches 	Training Loss: 0.127620
Trained 1351 batches 	Training Loss: 0.107623
Trained 1401 batches 	Training Loss: 0.158907
Trained 1451 batches 	Training Loss: 0.107145
Trained 1501 batches 	Training Loss: 0.128002
Trained 1551 batches 	Training Loss: 0.136149
Trained 1601 batches 	Training Loss: 0.138564
Trained 1651 batches 	Training Loss: 0.105140
Trained 1701 batches 	Training Loss: 0.194601
Trained 1751 batches 	Training Loss: 0.116782
Trained 1801 batches 	Training Loss: 0.110442
Trained 1851 batches 	Training Loss: 0.186163
Trained 1901 batches 	Training Loss: 0.127999
Trained 1951 batches 	Training Loss: 0.164497
Trained 2001 batches 	Training Loss: 0.171958
Trained 2051 batches 	Training Loss: 0.124411
Trained 2101 batches 	Training Loss: 0.119150
Trained 2151 batches 	Training Loss: 0.116651
Trained 2201 batches 	Training Loss: 0.151834
Trained 2251 batches 	Training Loss: 0.111846
Trained 2301 batches 	Training Loss: 0.141557
Trained 2351 batches 	Training Loss: 0.142069
Trained 2401 batches 	Training Loss: 0.081720
Trained 2451 batches 	Training Loss: 0.177622
Epoch: 8 	Training Loss: 0.141309
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.824
The AUROC of Atelectasis is 0.8059250486991002
The AUROC of Cardiomegaly is 0.8866359922545086
The AUROC of Effusion is 0.88089500834563
The AUROC of Infiltration is 0.7126451968178756
The AUROC of Mass is 0.8383447240934967
The AUROC of Nodule is 0.7475966938171563
The AUROC of Pneumonia is 0.7465937304021373
The AUROC of Pneumothorax is 0.8671356490067332
The AUROC of Consolidation is 0.7943588793972454
The AUROC of Edema is 0.8789297633666573
The AUROC of Emphysema is 0.8896421604903758
The AUROC of Fibrosis is 0.8037092319473407
The AUROC of Pleural_Thickening is 0.7738499152577537
The AUROC of Hernia is 0.9149784478173362
Started epoch 9
Trained 1 batches 	Training Loss: 0.139627
Trained 51 batches 	Training Loss: 0.172993
Trained 101 batches 	Training Loss: 0.178808
Trained 151 batches 	Training Loss: 0.142707
Trained 201 batches 	Training Loss: 0.132242
Trained 251 batches 	Training Loss: 0.164519
Trained 301 batches 	Training Loss: 0.154758
Trained 351 batches 	Training Loss: 0.163703
Trained 401 batches 	Training Loss: 0.112928
Trained 451 batches 	Training Loss: 0.141262
Trained 501 batches 	Training Loss: 0.154106
Trained 551 batches 	Training Loss: 0.111487
Trained 601 batches 	Training Loss: 0.107509
Trained 651 batches 	Training Loss: 0.121615
Trained 701 batches 	Training Loss: 0.169756
Trained 751 batches 	Training Loss: 0.109464
Trained 801 batches 	Training Loss: 0.144359
Trained 851 batches 	Training Loss: 0.150294
Trained 901 batches 	Training Loss: 0.133421
Trained 951 batches 	Training Loss: 0.117524
Trained 1001 batches 	Training Loss: 0.137429
Trained 1051 batches 	Training Loss: 0.142972
Trained 1101 batches 	Training Loss: 0.120574
Trained 1151 batches 	Training Loss: 0.132945
Trained 1201 batches 	Training Loss: 0.127201
Trained 1251 batches 	Training Loss: 0.120901
Trained 1301 batches 	Training Loss: 0.145912
Trained 1351 batches 	Training Loss: 0.134576
Trained 1401 batches 	Training Loss: 0.117721
Trained 1451 batches 	Training Loss: 0.163021
Trained 1501 batches 	Training Loss: 0.101638
Trained 1551 batches 	Training Loss: 0.087816
Trained 1601 batches 	Training Loss: 0.106122
Trained 1651 batches 	Training Loss: 0.148420
Trained 1701 batches 	Training Loss: 0.120549
Trained 1751 batches 	Training Loss: 0.143742
Trained 1801 batches 	Training Loss: 0.119238
Trained 1851 batches 	Training Loss: 0.147279
Trained 1901 batches 	Training Loss: 0.126864
Trained 1951 batches 	Training Loss: 0.101937
Trained 2001 batches 	Training Loss: 0.094746
Trained 2051 batches 	Training Loss: 0.138678
Trained 2101 batches 	Training Loss: 0.166679
Trained 2151 batches 	Training Loss: 0.188381
Trained 2201 batches 	Training Loss: 0.174949
Trained 2251 batches 	Training Loss: 0.199385
Trained 2301 batches 	Training Loss: 0.163558
Trained 2351 batches 	Training Loss: 0.169035
Trained 2401 batches 	Training Loss: 0.107799
Trained 2451 batches 	Training Loss: 0.129416
Epoch: 9 	Training Loss: 0.138051
Started epoch 10
Trained 1 batches 	Training Loss: 0.144200
Trained 51 batches 	Training Loss: 0.156781
Trained 101 batches 	Training Loss: 0.165503
Trained 151 batches 	Training Loss: 0.066703
Trained 201 batches 	Training Loss: 0.100516
Trained 251 batches 	Training Loss: 0.088623
Trained 301 batches 	Training Loss: 0.165129
Trained 351 batches 	Training Loss: 0.099850
Trained 401 batches 	Training Loss: 0.153136
Trained 451 batches 	Training Loss: 0.129014
Trained 501 batches 	Training Loss: 0.125583
Trained 551 batches 	Training Loss: 0.171854
Trained 601 batches 	Training Loss: 0.149549
Trained 651 batches 	Training Loss: 0.191562
Trained 701 batches 	Training Loss: 0.140265
Trained 751 batches 	Training Loss: 0.162589
Trained 801 batches 	Training Loss: 0.143326
Trained 851 batches 	Training Loss: 0.150887
Trained 901 batches 	Training Loss: 0.140769
Trained 951 batches 	Training Loss: 0.186734
Trained 1001 batches 	Training Loss: 0.134054
Trained 1051 batches 	Training Loss: 0.137551
Trained 1101 batches 	Training Loss: 0.144904
Trained 1151 batches 	Training Loss: 0.162923
Trained 1201 batches 	Training Loss: 0.161675
Trained 1251 batches 	Training Loss: 0.114074
Trained 1301 batches 	Training Loss: 0.124237
Trained 1351 batches 	Training Loss: 0.114287
Trained 1401 batches 	Training Loss: 0.158609
Trained 1451 batches 	Training Loss: 0.088789
Trained 1501 batches 	Training Loss: 0.146705
Trained 1551 batches 	Training Loss: 0.156001
Trained 1601 batches 	Training Loss: 0.083578
Trained 1651 batches 	Training Loss: 0.184278
Trained 1701 batches 	Training Loss: 0.135973
Trained 1751 batches 	Training Loss: 0.141731
Trained 1801 batches 	Training Loss: 0.167615
Trained 1851 batches 	Training Loss: 0.127863
Trained 1901 batches 	Training Loss: 0.116651
Trained 1951 batches 	Training Loss: 0.189848
Trained 2001 batches 	Training Loss: 0.121392
Trained 2051 batches 	Training Loss: 0.133865
Trained 2101 batches 	Training Loss: 0.105949
Trained 2151 batches 	Training Loss: 0.097143
Trained 2201 batches 	Training Loss: 0.116913
Trained 2251 batches 	Training Loss: 0.148416
Trained 2301 batches 	Training Loss: 0.140679
Trained 2351 batches 	Training Loss: 0.099721
Trained 2401 batches 	Training Loss: 0.128781
Trained 2451 batches 	Training Loss: 0.114416
Epoch: 10 	Training Loss: 0.134247
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.822
The AUROC of Atelectasis is 0.8057179141748578
The AUROC of Cardiomegaly is 0.9036690926970213
The AUROC of Effusion is 0.8760329985002057
The AUROC of Infiltration is 0.7171232319443845
The AUROC of Mass is 0.8385372757092138
The AUROC of Nodule is 0.7436816240115673
The AUROC of Pneumonia is 0.7349169838119867
The AUROC of Pneumothorax is 0.8608896797153025
The AUROC of Consolidation is 0.7917894470679276
The AUROC of Edema is 0.8882327459562894
The AUROC of Emphysema is 0.8809857107981108
The AUROC of Fibrosis is 0.7815455652010433
The AUROC of Pleural_Thickening is 0.7715338665764452
The AUROC of Hernia is 0.9180846853977048
Training time lapse: 112.0 min
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.824
The AUROC of Atelectasis is 0.7998601597974975
The AUROC of Cardiomegaly is 0.9022459434806208
The AUROC of Effusion is 0.8774201240115935
The AUROC of Infiltration is 0.7087582126029023
The AUROC of Mass is 0.8334921565308161
The AUROC of Nodule is 0.7314557358661716
The AUROC of Pneumonia is 0.7369156772502351
The AUROC of Pneumothorax is 0.8710466082765345
The AUROC of Consolidation is 0.8007699170590881
The AUROC of Edema is 0.9023506311793501
The AUROC of Emphysema is 0.8859579546528935
The AUROC of Fibrosis is 0.8038857523345405
The AUROC of Pleural_Thickening is 0.7903498775918738
The AUROC of Hernia is 0.8926117998146703