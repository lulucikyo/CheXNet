Started training, total epoch : 20
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.763216
Trained 51 batches 	Training Loss: 0.196178
Trained 101 batches 	Training Loss: 0.206836
Trained 151 batches 	Training Loss: 0.178950
Trained 201 batches 	Training Loss: 0.158831
Trained 251 batches 	Training Loss: 0.167766
Trained 301 batches 	Training Loss: 0.142558
Trained 351 batches 	Training Loss: 0.190925
Trained 401 batches 	Training Loss: 0.187239
Trained 451 batches 	Training Loss: 0.141332
Trained 501 batches 	Training Loss: 0.122070
Trained 551 batches 	Training Loss: 0.197765
Trained 601 batches 	Training Loss: 0.226403
Trained 651 batches 	Training Loss: 0.172232
Trained 701 batches 	Training Loss: 0.171351
Trained 751 batches 	Training Loss: 0.197828
Trained 801 batches 	Training Loss: 0.151307
Trained 851 batches 	Training Loss: 0.213706
Trained 901 batches 	Training Loss: 0.152926
Trained 951 batches 	Training Loss: 0.182680
Trained 1001 batches 	Training Loss: 0.182602
Trained 1051 batches 	Training Loss: 0.162223
Trained 1101 batches 	Training Loss: 0.201249
Trained 1151 batches 	Training Loss: 0.187014
Trained 1201 batches 	Training Loss: 0.145243
Trained 1251 batches 	Training Loss: 0.168479
Trained 1301 batches 	Training Loss: 0.156317
Trained 1351 batches 	Training Loss: 0.145115
Trained 1401 batches 	Training Loss: 0.178426
Trained 1451 batches 	Training Loss: 0.107086
Trained 1501 batches 	Training Loss: 0.201779
Trained 1551 batches 	Training Loss: 0.139602
Trained 1601 batches 	Training Loss: 0.178043
Trained 1651 batches 	Training Loss: 0.167342
Trained 1701 batches 	Training Loss: 0.139805
Trained 1751 batches 	Training Loss: 0.181463
Trained 1801 batches 	Training Loss: 0.165660
Trained 1851 batches 	Training Loss: 0.203129
Trained 1901 batches 	Training Loss: 0.159084
Trained 1951 batches 	Training Loss: 0.209105
Trained 2001 batches 	Training Loss: 0.147030
Trained 2051 batches 	Training Loss: 0.153910
Trained 2101 batches 	Training Loss: 0.160183
Trained 2151 batches 	Training Loss: 0.188715
Trained 2201 batches 	Training Loss: 0.210772
Trained 2251 batches 	Training Loss: 0.151383
Trained 2301 batches 	Training Loss: 0.116533
Trained 2351 batches 	Training Loss: 0.124542
Trained 2401 batches 	Training Loss: 0.171510
Trained 2451 batches 	Training Loss: 0.191074
Epoch: 1 	Training Loss: 0.168120
Started epoch 2
Trained 1 batches 	Training Loss: 0.164425
Trained 51 batches 	Training Loss: 0.170058
Trained 101 batches 	Training Loss: 0.149840
Trained 151 batches 	Training Loss: 0.146009
Trained 201 batches 	Training Loss: 0.110833
Trained 251 batches 	Training Loss: 0.151680
Trained 301 batches 	Training Loss: 0.225620
Trained 351 batches 	Training Loss: 0.148703
Trained 401 batches 	Training Loss: 0.135547
Trained 451 batches 	Training Loss: 0.182175
Trained 501 batches 	Training Loss: 0.123101
Trained 551 batches 	Training Loss: 0.225887
Trained 601 batches 	Training Loss: 0.170214
Trained 651 batches 	Training Loss: 0.112231
Trained 701 batches 	Training Loss: 0.155008
Trained 751 batches 	Training Loss: 0.137478
Trained 801 batches 	Training Loss: 0.181461
Trained 851 batches 	Training Loss: 0.142071
Trained 901 batches 	Training Loss: 0.170130
Trained 951 batches 	Training Loss: 0.183449
Trained 1001 batches 	Training Loss: 0.137631
Trained 1051 batches 	Training Loss: 0.213292
Trained 1101 batches 	Training Loss: 0.097064
Trained 1151 batches 	Training Loss: 0.141647
Trained 1201 batches 	Training Loss: 0.249240
Trained 1251 batches 	Training Loss: 0.127467
Trained 1301 batches 	Training Loss: 0.108729
Trained 1351 batches 	Training Loss: 0.153547
Trained 1401 batches 	Training Loss: 0.145709
Trained 1451 batches 	Training Loss: 0.164102
Trained 1501 batches 	Training Loss: 0.145217
Trained 1551 batches 	Training Loss: 0.126243
Trained 1601 batches 	Training Loss: 0.176557
Trained 1651 batches 	Training Loss: 0.202967
Trained 1701 batches 	Training Loss: 0.146061
Trained 1751 batches 	Training Loss: 0.130658
Trained 1801 batches 	Training Loss: 0.186130
Trained 1851 batches 	Training Loss: 0.121773
Trained 1901 batches 	Training Loss: 0.175495
Trained 1951 batches 	Training Loss: 0.125076
Trained 2001 batches 	Training Loss: 0.171279
Trained 2051 batches 	Training Loss: 0.171894
Trained 2101 batches 	Training Loss: 0.181535
Trained 2151 batches 	Training Loss: 0.156347
Trained 2201 batches 	Training Loss: 0.102256
Trained 2251 batches 	Training Loss: 0.139405
Trained 2301 batches 	Training Loss: 0.183951
Trained 2351 batches 	Training Loss: 0.184732
Trained 2401 batches 	Training Loss: 0.159551
Trained 2451 batches 	Training Loss: 0.133920
Epoch: 2 	Training Loss: 0.158991
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.789
The AUROC of Atelectasis is 0.7766115252670176
The AUROC of Cardiomegaly is 0.8679551538475546
The AUROC of Effusion is 0.8647303393889548
The AUROC of Infiltration is 0.701068356138238
The AUROC of Mass is 0.7999864848805426
The AUROC of Nodule is 0.7028646297563792
The AUROC of Pneumonia is 0.7548896219742331
The AUROC of Pneumothorax is 0.8187876129851469
The AUROC of Consolidation is 0.7797816280229682
The AUROC of Edema is 0.8668061655755366
The AUROC of Emphysema is 0.8187691498815628
The AUROC of Fibrosis is 0.7596393509562565
The AUROC of Pleural_Thickening is 0.737962132504317
The AUROC of Hernia is 0.7978052091178711
Started epoch 3
Trained 1 batches 	Training Loss: 0.165750
Trained 51 batches 	Training Loss: 0.133945
Trained 101 batches 	Training Loss: 0.133280
Trained 151 batches 	Training Loss: 0.130585
Trained 201 batches 	Training Loss: 0.181905
Trained 251 batches 	Training Loss: 0.212737
Trained 301 batches 	Training Loss: 0.187971
Trained 351 batches 	Training Loss: 0.129940
Trained 401 batches 	Training Loss: 0.177126
Trained 451 batches 	Training Loss: 0.184900
Trained 501 batches 	Training Loss: 0.099917
Trained 551 batches 	Training Loss: 0.154070
Trained 601 batches 	Training Loss: 0.136821
Trained 651 batches 	Training Loss: 0.153008
Trained 701 batches 	Training Loss: 0.138034
Trained 751 batches 	Training Loss: 0.140108
Trained 801 batches 	Training Loss: 0.146058
Trained 851 batches 	Training Loss: 0.100768
Trained 901 batches 	Training Loss: 0.182325
Trained 951 batches 	Training Loss: 0.193997
Trained 1001 batches 	Training Loss: 0.177363
Trained 1051 batches 	Training Loss: 0.127987
Trained 1101 batches 	Training Loss: 0.150524
Trained 1151 batches 	Training Loss: 0.161437
Trained 1201 batches 	Training Loss: 0.138058
Trained 1251 batches 	Training Loss: 0.085006
Trained 1301 batches 	Training Loss: 0.129327
Trained 1351 batches 	Training Loss: 0.137076
Trained 1401 batches 	Training Loss: 0.210087
Trained 1451 batches 	Training Loss: 0.129860
Trained 1501 batches 	Training Loss: 0.157942
Trained 1551 batches 	Training Loss: 0.177660
Trained 1601 batches 	Training Loss: 0.212813
Trained 1651 batches 	Training Loss: 0.180503
Trained 1701 batches 	Training Loss: 0.229349
Trained 1751 batches 	Training Loss: 0.188630
Trained 1801 batches 	Training Loss: 0.129736
Trained 1851 batches 	Training Loss: 0.160726
Trained 1901 batches 	Training Loss: 0.213118
Trained 1951 batches 	Training Loss: 0.141527
Trained 2001 batches 	Training Loss: 0.125896
Trained 2051 batches 	Training Loss: 0.137318
Trained 2101 batches 	Training Loss: 0.135792
Trained 2151 batches 	Training Loss: 0.151139
Trained 2201 batches 	Training Loss: 0.125293
Trained 2251 batches 	Training Loss: 0.147669
Trained 2301 batches 	Training Loss: 0.141454
Trained 2351 batches 	Training Loss: 0.117307
Trained 2401 batches 	Training Loss: 0.171405
Trained 2451 batches 	Training Loss: 0.119121
Epoch: 3 	Training Loss: 0.154859
Started epoch 4
Trained 1 batches 	Training Loss: 0.211508
Trained 51 batches 	Training Loss: 0.172930
Trained 101 batches 	Training Loss: 0.118587
Trained 151 batches 	Training Loss: 0.146846
Trained 201 batches 	Training Loss: 0.173688
Trained 251 batches 	Training Loss: 0.199579
Trained 301 batches 	Training Loss: 0.167303
Trained 351 batches 	Training Loss: 0.149451
Trained 401 batches 	Training Loss: 0.141451
Trained 451 batches 	Training Loss: 0.128932
Trained 501 batches 	Training Loss: 0.190846
Trained 551 batches 	Training Loss: 0.137327
Trained 601 batches 	Training Loss: 0.108582
Trained 651 batches 	Training Loss: 0.135058
Trained 701 batches 	Training Loss: 0.189104
Trained 751 batches 	Training Loss: 0.156674
Trained 801 batches 	Training Loss: 0.204435
Trained 851 batches 	Training Loss: 0.155162
Trained 901 batches 	Training Loss: 0.215674
Trained 951 batches 	Training Loss: 0.157202
Trained 1001 batches 	Training Loss: 0.156772
Trained 1051 batches 	Training Loss: 0.146248
Trained 1101 batches 	Training Loss: 0.153448
Trained 1151 batches 	Training Loss: 0.238045
Trained 1201 batches 	Training Loss: 0.172805
Trained 1251 batches 	Training Loss: 0.190034
Trained 1301 batches 	Training Loss: 0.170755
Trained 1351 batches 	Training Loss: 0.143234
Trained 1401 batches 	Training Loss: 0.189286
Trained 1451 batches 	Training Loss: 0.146898
Trained 1501 batches 	Training Loss: 0.148268
Trained 1551 batches 	Training Loss: 0.153030
Trained 1601 batches 	Training Loss: 0.178807
Trained 1651 batches 	Training Loss: 0.147594
Trained 1701 batches 	Training Loss: 0.132762
Trained 1751 batches 	Training Loss: 0.155757
Trained 1801 batches 	Training Loss: 0.119449
Trained 1851 batches 	Training Loss: 0.135383
Trained 1901 batches 	Training Loss: 0.126366
Trained 1951 batches 	Training Loss: 0.153907
Trained 2001 batches 	Training Loss: 0.182089
Trained 2051 batches 	Training Loss: 0.157624
Trained 2101 batches 	Training Loss: 0.188584
Trained 2151 batches 	Training Loss: 0.144166
Trained 2201 batches 	Training Loss: 0.175059
Trained 2251 batches 	Training Loss: 0.175173
Trained 2301 batches 	Training Loss: 0.180627
Trained 2351 batches 	Training Loss: 0.157692
Trained 2401 batches 	Training Loss: 0.136465
Trained 2451 batches 	Training Loss: 0.148474
Epoch: 4 	Training Loss: 0.152050
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.808
The AUROC of Atelectasis is 0.7900922406270143
The AUROC of Cardiomegaly is 0.8984776103095161
The AUROC of Effusion is 0.8711780945354266
The AUROC of Infiltration is 0.7131827872488806
The AUROC of Mass is 0.8213082169596108
The AUROC of Nodule is 0.7266126151108138
The AUROC of Pneumonia is 0.7492948348982987
The AUROC of Pneumothorax is 0.8319313985932201
The AUROC of Consolidation is 0.7905773720459021
The AUROC of Edema is 0.8807250427537161
The AUROC of Emphysema is 0.8667737769339839
The AUROC of Fibrosis is 0.7601111476334428
The AUROC of Pleural_Thickening is 0.7477904775887879
The AUROC of Hernia is 0.8667934692418653
Started epoch 5
Trained 1 batches 	Training Loss: 0.152367
Trained 51 batches 	Training Loss: 0.147137
Trained 101 batches 	Training Loss: 0.175855
Trained 151 batches 	Training Loss: 0.137440
Trained 201 batches 	Training Loss: 0.156869
Trained 251 batches 	Training Loss: 0.133060
Trained 301 batches 	Training Loss: 0.154147
Trained 351 batches 	Training Loss: 0.153429
Trained 401 batches 	Training Loss: 0.158723
Trained 451 batches 	Training Loss: 0.146211
Trained 501 batches 	Training Loss: 0.124103
Trained 551 batches 	Training Loss: 0.155386
Trained 601 batches 	Training Loss: 0.148382
Trained 651 batches 	Training Loss: 0.136880
Trained 701 batches 	Training Loss: 0.161252
Trained 751 batches 	Training Loss: 0.223502
Trained 801 batches 	Training Loss: 0.131133
Trained 851 batches 	Training Loss: 0.109155
Trained 901 batches 	Training Loss: 0.152295
Trained 951 batches 	Training Loss: 0.171643
Trained 1001 batches 	Training Loss: 0.127926
Trained 1051 batches 	Training Loss: 0.187411
Trained 1101 batches 	Training Loss: 0.138175
Trained 1151 batches 	Training Loss: 0.122613
Trained 1201 batches 	Training Loss: 0.140554
Trained 1251 batches 	Training Loss: 0.179150
Trained 1301 batches 	Training Loss: 0.131880
Trained 1351 batches 	Training Loss: 0.151203
Trained 1401 batches 	Training Loss: 0.136526
Trained 1451 batches 	Training Loss: 0.147063
Trained 1501 batches 	Training Loss: 0.118094
Trained 1551 batches 	Training Loss: 0.177602
Trained 1601 batches 	Training Loss: 0.142430
Trained 1651 batches 	Training Loss: 0.118227
Trained 1701 batches 	Training Loss: 0.192913
Trained 1751 batches 	Training Loss: 0.187974
Trained 1801 batches 	Training Loss: 0.184826
Trained 1851 batches 	Training Loss: 0.117883
Trained 1901 batches 	Training Loss: 0.156983
Trained 1951 batches 	Training Loss: 0.085795
Trained 2001 batches 	Training Loss: 0.152371
Trained 2051 batches 	Training Loss: 0.130797
Trained 2101 batches 	Training Loss: 0.109002
Trained 2151 batches 	Training Loss: 0.158017
Trained 2201 batches 	Training Loss: 0.145704
Trained 2251 batches 	Training Loss: 0.139530
Trained 2301 batches 	Training Loss: 0.191814
Trained 2351 batches 	Training Loss: 0.221626
Trained 2401 batches 	Training Loss: 0.139376
Trained 2451 batches 	Training Loss: 0.119141
Epoch: 5 	Training Loss: 0.149283
Started epoch 6
Trained 1 batches 	Training Loss: 0.153762
Trained 51 batches 	Training Loss: 0.200555
Trained 101 batches 	Training Loss: 0.154919
Trained 151 batches 	Training Loss: 0.143199
Trained 201 batches 	Training Loss: 0.199532
Trained 251 batches 	Training Loss: 0.180724
Trained 301 batches 	Training Loss: 0.139791
Trained 351 batches 	Training Loss: 0.099893
Trained 401 batches 	Training Loss: 0.114775
Trained 451 batches 	Training Loss: 0.172029
Trained 501 batches 	Training Loss: 0.131262
Trained 551 batches 	Training Loss: 0.124190
Trained 601 batches 	Training Loss: 0.165182
Trained 651 batches 	Training Loss: 0.168291
Trained 701 batches 	Training Loss: 0.192654
Trained 751 batches 	Training Loss: 0.147916
Trained 801 batches 	Training Loss: 0.099541
Trained 851 batches 	Training Loss: 0.151410
Trained 901 batches 	Training Loss: 0.161979
Trained 951 batches 	Training Loss: 0.102048
Trained 1001 batches 	Training Loss: 0.129714
Trained 1051 batches 	Training Loss: 0.142669
Trained 1101 batches 	Training Loss: 0.116812
Trained 1151 batches 	Training Loss: 0.151874
Trained 1201 batches 	Training Loss: 0.124911
Trained 1251 batches 	Training Loss: 0.122438
Trained 1301 batches 	Training Loss: 0.166948
Trained 1351 batches 	Training Loss: 0.151219
Trained 1401 batches 	Training Loss: 0.128827
Trained 1451 batches 	Training Loss: 0.146069
Trained 1501 batches 	Training Loss: 0.176132
Trained 1551 batches 	Training Loss: 0.148550
Trained 1601 batches 	Training Loss: 0.150126
Trained 1651 batches 	Training Loss: 0.159858
Trained 1701 batches 	Training Loss: 0.155529
Trained 1751 batches 	Training Loss: 0.101262
Trained 1801 batches 	Training Loss: 0.169890
Trained 1851 batches 	Training Loss: 0.113177
Trained 1901 batches 	Training Loss: 0.142608
Trained 1951 batches 	Training Loss: 0.137463
Trained 2001 batches 	Training Loss: 0.127951
Trained 2051 batches 	Training Loss: 0.144514
Trained 2101 batches 	Training Loss: 0.120518
Trained 2151 batches 	Training Loss: 0.134994
Trained 2201 batches 	Training Loss: 0.142412
Trained 2251 batches 	Training Loss: 0.188499
Trained 2301 batches 	Training Loss: 0.141572
Trained 2351 batches 	Training Loss: 0.133722
Trained 2401 batches 	Training Loss: 0.163084
Trained 2451 batches 	Training Loss: 0.125597
Epoch: 6 	Training Loss: 0.146863
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.819
The AUROC of Atelectasis is 0.7917754950269046
The AUROC of Cardiomegaly is 0.8978960914405035
The AUROC of Effusion is 0.8756425530370836
The AUROC of Infiltration is 0.709502436109466
The AUROC of Mass is 0.8337873326047598
The AUROC of Nodule is 0.7417814565309729
The AUROC of Pneumonia is 0.7626850968904009
The AUROC of Pneumothorax is 0.8489846791305364
The AUROC of Consolidation is 0.7871953909790418
The AUROC of Edema is 0.8880282061520149
The AUROC of Emphysema is 0.8807714439605402
The AUROC of Fibrosis is 0.7707451333993129
The AUROC of Pleural_Thickening is 0.7553436024125739
The AUROC of Hernia is 0.9271948972601283
Started epoch 7
Trained 1 batches 	Training Loss: 0.117528
Trained 51 batches 	Training Loss: 0.140027
Trained 101 batches 	Training Loss: 0.164967
Trained 151 batches 	Training Loss: 0.119584
Trained 201 batches 	Training Loss: 0.175532
Trained 251 batches 	Training Loss: 0.148400
Trained 301 batches 	Training Loss: 0.158360
Trained 351 batches 	Training Loss: 0.172272
Trained 401 batches 	Training Loss: 0.143190
Trained 451 batches 	Training Loss: 0.126979
Trained 501 batches 	Training Loss: 0.106163
Trained 551 batches 	Training Loss: 0.130550
Trained 601 batches 	Training Loss: 0.141734
Trained 651 batches 	Training Loss: 0.194072
Trained 701 batches 	Training Loss: 0.203468
Trained 751 batches 	Training Loss: 0.128698
Trained 801 batches 	Training Loss: 0.224885
Trained 851 batches 	Training Loss: 0.141583
Trained 901 batches 	Training Loss: 0.173513
Trained 951 batches 	Training Loss: 0.147432
Trained 1001 batches 	Training Loss: 0.120331
Trained 1051 batches 	Training Loss: 0.141606
Trained 1101 batches 	Training Loss: 0.108632
Trained 1151 batches 	Training Loss: 0.140451
Trained 1201 batches 	Training Loss: 0.141918
Trained 1251 batches 	Training Loss: 0.112907
Trained 1301 batches 	Training Loss: 0.229506
Trained 1351 batches 	Training Loss: 0.102224
Trained 1401 batches 	Training Loss: 0.176271
Trained 1451 batches 	Training Loss: 0.205430
Trained 1501 batches 	Training Loss: 0.097671
Trained 1551 batches 	Training Loss: 0.126356
Trained 1601 batches 	Training Loss: 0.146467
Trained 1651 batches 	Training Loss: 0.129275
Trained 1701 batches 	Training Loss: 0.154324
Trained 1751 batches 	Training Loss: 0.171166
Trained 1801 batches 	Training Loss: 0.177938
Trained 1851 batches 	Training Loss: 0.137038
Trained 1901 batches 	Training Loss: 0.181115
Trained 1951 batches 	Training Loss: 0.139559
Trained 2001 batches 	Training Loss: 0.115855
Trained 2051 batches 	Training Loss: 0.115838
Trained 2101 batches 	Training Loss: 0.148591
Trained 2151 batches 	Training Loss: 0.134599
Trained 2201 batches 	Training Loss: 0.155301
Trained 2251 batches 	Training Loss: 0.127820
Trained 2301 batches 	Training Loss: 0.161353
Trained 2351 batches 	Training Loss: 0.140800
Trained 2401 batches 	Training Loss: 0.128591
Trained 2451 batches 	Training Loss: 0.130060
Epoch: 7 	Training Loss: 0.144313
Started epoch 8
Trained 1 batches 	Training Loss: 0.153011
Trained 51 batches 	Training Loss: 0.147300
Trained 101 batches 	Training Loss: 0.132581
Trained 151 batches 	Training Loss: 0.138676
Trained 201 batches 	Training Loss: 0.157141
Trained 251 batches 	Training Loss: 0.141220
Trained 301 batches 	Training Loss: 0.158972
Trained 351 batches 	Training Loss: 0.180856
Trained 401 batches 	Training Loss: 0.221805
Trained 451 batches 	Training Loss: 0.122271
Trained 501 batches 	Training Loss: 0.166402
Trained 551 batches 	Training Loss: 0.106538
Trained 601 batches 	Training Loss: 0.112712
Trained 651 batches 	Training Loss: 0.167193
Trained 701 batches 	Training Loss: 0.165190
Trained 751 batches 	Training Loss: 0.122669
Trained 801 batches 	Training Loss: 0.128398
Trained 851 batches 	Training Loss: 0.133537
Trained 901 batches 	Training Loss: 0.105070
Trained 951 batches 	Training Loss: 0.164860
Trained 1001 batches 	Training Loss: 0.103764
Trained 1051 batches 	Training Loss: 0.105661
Trained 1101 batches 	Training Loss: 0.118166
Trained 1151 batches 	Training Loss: 0.156257
Trained 1201 batches 	Training Loss: 0.110864
Trained 1251 batches 	Training Loss: 0.189614
Trained 1301 batches 	Training Loss: 0.175370
Trained 1351 batches 	Training Loss: 0.184582
Trained 1401 batches 	Training Loss: 0.150269
Trained 1451 batches 	Training Loss: 0.151135
Trained 1501 batches 	Training Loss: 0.159762
Trained 1551 batches 	Training Loss: 0.127614
Trained 1601 batches 	Training Loss: 0.121686
Trained 1651 batches 	Training Loss: 0.109315
Trained 1701 batches 	Training Loss: 0.122047
Trained 1751 batches 	Training Loss: 0.135776
Trained 1801 batches 	Training Loss: 0.115510
Trained 1851 batches 	Training Loss: 0.134779
Trained 1901 batches 	Training Loss: 0.125851
Trained 1951 batches 	Training Loss: 0.130833
Trained 2001 batches 	Training Loss: 0.137873
Trained 2051 batches 	Training Loss: 0.165341
Trained 2101 batches 	Training Loss: 0.102860
Trained 2151 batches 	Training Loss: 0.123924
Trained 2201 batches 	Training Loss: 0.161657
Trained 2251 batches 	Training Loss: 0.136212
Trained 2301 batches 	Training Loss: 0.130851
Trained 2351 batches 	Training Loss: 0.130186
Trained 2401 batches 	Training Loss: 0.135985
Trained 2451 batches 	Training Loss: 0.112770
Epoch: 8 	Training Loss: 0.141420
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.827
The AUROC of Atelectasis is 0.7936344446469683
The AUROC of Cardiomegaly is 0.9138119836591984
The AUROC of Effusion is 0.8782833704250224
The AUROC of Infiltration is 0.7182821782310764
The AUROC of Mass is 0.8391873218858719
The AUROC of Nodule is 0.7385371894217995
The AUROC of Pneumonia is 0.7523155903771095
The AUROC of Pneumothorax is 0.8640221041551801
The AUROC of Consolidation is 0.7869600019212194
The AUROC of Edema is 0.8895251512389161
The AUROC of Emphysema is 0.8944692415961039
The AUROC of Fibrosis is 0.7886416746147642
The AUROC of Pleural_Thickening is 0.7632359888031715
The AUROC of Hernia is 0.9531298534962193
Started epoch 9
Trained 1 batches 	Training Loss: 0.136855
Trained 51 batches 	Training Loss: 0.103599
Trained 101 batches 	Training Loss: 0.135533
Trained 151 batches 	Training Loss: 0.146191
Trained 201 batches 	Training Loss: 0.140874
Trained 251 batches 	Training Loss: 0.097380
Trained 301 batches 	Training Loss: 0.100108
Trained 351 batches 	Training Loss: 0.125666
Trained 401 batches 	Training Loss: 0.154753
Trained 451 batches 	Training Loss: 0.144825
Trained 501 batches 	Training Loss: 0.099520
Trained 551 batches 	Training Loss: 0.160411
Trained 601 batches 	Training Loss: 0.189098
Trained 651 batches 	Training Loss: 0.132968
Trained 701 batches 	Training Loss: 0.150280
Trained 751 batches 	Training Loss: 0.162256
Trained 801 batches 	Training Loss: 0.106540
Trained 851 batches 	Training Loss: 0.080330
Trained 901 batches 	Training Loss: 0.106860
Trained 951 batches 	Training Loss: 0.121921
Trained 1001 batches 	Training Loss: 0.175734
Trained 1051 batches 	Training Loss: 0.095195
Trained 1101 batches 	Training Loss: 0.138186
Trained 1151 batches 	Training Loss: 0.162322
Trained 1201 batches 	Training Loss: 0.146993
Trained 1251 batches 	Training Loss: 0.130368
Trained 1301 batches 	Training Loss: 0.155040
Trained 1351 batches 	Training Loss: 0.145593
Trained 1401 batches 	Training Loss: 0.136259
Trained 1451 batches 	Training Loss: 0.131685
Trained 1501 batches 	Training Loss: 0.096756
Trained 1551 batches 	Training Loss: 0.103279
Trained 1601 batches 	Training Loss: 0.174080
Trained 1651 batches 	Training Loss: 0.088087
Trained 1701 batches 	Training Loss: 0.116419
Trained 1751 batches 	Training Loss: 0.177567
Trained 1801 batches 	Training Loss: 0.128752
Trained 1851 batches 	Training Loss: 0.127762
Trained 1901 batches 	Training Loss: 0.130623
Trained 1951 batches 	Training Loss: 0.190021
Trained 2001 batches 	Training Loss: 0.108749
Trained 2051 batches 	Training Loss: 0.119813
Trained 2101 batches 	Training Loss: 0.086055
Trained 2151 batches 	Training Loss: 0.116203
Trained 2201 batches 	Training Loss: 0.192570
Trained 2251 batches 	Training Loss: 0.146646
Trained 2301 batches 	Training Loss: 0.165854
Trained 2351 batches 	Training Loss: 0.144634
Trained 2401 batches 	Training Loss: 0.105575
Trained 2451 batches 	Training Loss: 0.135257
Epoch: 9 	Training Loss: 0.138129
Started epoch 10
Trained 1 batches 	Training Loss: 0.147647
Trained 51 batches 	Training Loss: 0.103563
Trained 101 batches 	Training Loss: 0.144681
Trained 151 batches 	Training Loss: 0.114752
Trained 201 batches 	Training Loss: 0.127579
Trained 251 batches 	Training Loss: 0.152242
Trained 301 batches 	Training Loss: 0.120798
Trained 351 batches 	Training Loss: 0.174853
Trained 401 batches 	Training Loss: 0.160758
Trained 451 batches 	Training Loss: 0.129284
Trained 501 batches 	Training Loss: 0.143454
Trained 551 batches 	Training Loss: 0.106920
Trained 601 batches 	Training Loss: 0.156429
Trained 651 batches 	Training Loss: 0.187942
Trained 701 batches 	Training Loss: 0.134591
Trained 751 batches 	Training Loss: 0.167653
Trained 801 batches 	Training Loss: 0.167275
Trained 851 batches 	Training Loss: 0.190363
Trained 901 batches 	Training Loss: 0.143680
Trained 951 batches 	Training Loss: 0.136629
Trained 1001 batches 	Training Loss: 0.110944
Trained 1051 batches 	Training Loss: 0.099698
Trained 1101 batches 	Training Loss: 0.153510
Trained 1151 batches 	Training Loss: 0.135649
Trained 1201 batches 	Training Loss: 0.101221
Trained 1251 batches 	Training Loss: 0.143405
Trained 1301 batches 	Training Loss: 0.157927
Trained 1351 batches 	Training Loss: 0.154008
Trained 1401 batches 	Training Loss: 0.134049
Trained 1451 batches 	Training Loss: 0.160801
Trained 1501 batches 	Training Loss: 0.147791
Trained 1551 batches 	Training Loss: 0.132367
Trained 1601 batches 	Training Loss: 0.158743
Trained 1651 batches 	Training Loss: 0.109723
Trained 1701 batches 	Training Loss: 0.124607
Trained 1751 batches 	Training Loss: 0.113386
Trained 1801 batches 	Training Loss: 0.151045
Trained 1851 batches 	Training Loss: 0.120668
Trained 1901 batches 	Training Loss: 0.176483
Trained 1951 batches 	Training Loss: 0.148671
Trained 2001 batches 	Training Loss: 0.173969
Trained 2051 batches 	Training Loss: 0.138574
Trained 2101 batches 	Training Loss: 0.159303
Trained 2151 batches 	Training Loss: 0.121678
Trained 2201 batches 	Training Loss: 0.119392
Trained 2251 batches 	Training Loss: 0.212023
Trained 2301 batches 	Training Loss: 0.138593
Trained 2351 batches 	Training Loss: 0.107352
Trained 2401 batches 	Training Loss: 0.209436
Trained 2451 batches 	Training Loss: 0.111950
Epoch: 10 	Training Loss: 0.134070
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.824
The AUROC of Atelectasis is 0.7917808409814425
The AUROC of Cardiomegaly is 0.8977979070176824
The AUROC of Effusion is 0.8767636946950822
The AUROC of Infiltration is 0.7127390709206827
The AUROC of Mass is 0.8287731456129845
The AUROC of Nodule is 0.7347218890461236
The AUROC of Pneumonia is 0.7633372851215176
The AUROC of Pneumothorax is 0.8623873490050623
The AUROC of Consolidation is 0.7865140329389871
The AUROC of Edema is 0.8870846109223502
The AUROC of Emphysema is 0.8917254453503227
The AUROC of Fibrosis is 0.7809414499126157
The AUROC of Pleural_Thickening is 0.7575071097308357
The AUROC of Hernia is 0.9592359506576288
Started epoch 11
Trained 1 batches 	Training Loss: 0.191809
Trained 51 batches 	Training Loss: 0.133574
Trained 101 batches 	Training Loss: 0.131736
Trained 151 batches 	Training Loss: 0.154396
Trained 201 batches 	Training Loss: 0.121765
Trained 251 batches 	Training Loss: 0.123016
Trained 301 batches 	Training Loss: 0.129166
Trained 351 batches 	Training Loss: 0.104532
Trained 401 batches 	Training Loss: 0.159415
Trained 451 batches 	Training Loss: 0.129251
Trained 501 batches 	Training Loss: 0.125943
Trained 551 batches 	Training Loss: 0.102784
Trained 601 batches 	Training Loss: 0.126751
Trained 651 batches 	Training Loss: 0.080256
Trained 701 batches 	Training Loss: 0.113674
Trained 751 batches 	Training Loss: 0.145585
Trained 801 batches 	Training Loss: 0.093257
Trained 851 batches 	Training Loss: 0.129825
Trained 901 batches 	Training Loss: 0.095071
Trained 951 batches 	Training Loss: 0.104950
Trained 1001 batches 	Training Loss: 0.148371
Trained 1051 batches 	Training Loss: 0.141465
Trained 1101 batches 	Training Loss: 0.138398
Trained 1151 batches 	Training Loss: 0.107218
Trained 1201 batches 	Training Loss: 0.174909
Trained 1251 batches 	Training Loss: 0.144594
Trained 1301 batches 	Training Loss: 0.101437
Trained 1351 batches 	Training Loss: 0.131985
Trained 1401 batches 	Training Loss: 0.145651
Trained 1451 batches 	Training Loss: 0.131049
Trained 1501 batches 	Training Loss: 0.132248
Trained 1551 batches 	Training Loss: 0.102264
Trained 1601 batches 	Training Loss: 0.071483
Trained 1651 batches 	Training Loss: 0.094586
Trained 1701 batches 	Training Loss: 0.121876
Trained 1751 batches 	Training Loss: 0.117872
Trained 1801 batches 	Training Loss: 0.148653
Trained 1851 batches 	Training Loss: 0.142107
Trained 1901 batches 	Training Loss: 0.098137
Trained 1951 batches 	Training Loss: 0.116137
Trained 2001 batches 	Training Loss: 0.182374
Trained 2051 batches 	Training Loss: 0.131241
Trained 2101 batches 	Training Loss: 0.150390
Trained 2151 batches 	Training Loss: 0.138236
Trained 2201 batches 	Training Loss: 0.167454
Trained 2251 batches 	Training Loss: 0.152613
Trained 2301 batches 	Training Loss: 0.098275
Trained 2351 batches 	Training Loss: 0.122810
Trained 2401 batches 	Training Loss: 0.119119
Trained 2451 batches 	Training Loss: 0.179859
Epoch: 11 	Training Loss: 0.129123
Started epoch 12
Trained 1 batches 	Training Loss: 0.078202
Trained 51 batches 	Training Loss: 0.116468
Trained 101 batches 	Training Loss: 0.114610
Trained 151 batches 	Training Loss: 0.102860
Trained 201 batches 	Training Loss: 0.105686
Trained 251 batches 	Training Loss: 0.111622
Trained 301 batches 	Training Loss: 0.133887
Trained 351 batches 	Training Loss: 0.119250
Trained 401 batches 	Training Loss: 0.090979
Trained 451 batches 	Training Loss: 0.102196
Trained 501 batches 	Training Loss: 0.110978
Trained 551 batches 	Training Loss: 0.134671
Trained 601 batches 	Training Loss: 0.094881
Trained 651 batches 	Training Loss: 0.116369
Trained 701 batches 	Training Loss: 0.115198
Trained 751 batches 	Training Loss: 0.111859
Trained 801 batches 	Training Loss: 0.103289
Trained 851 batches 	Training Loss: 0.141405
Trained 901 batches 	Training Loss: 0.106248
Trained 951 batches 	Training Loss: 0.079115
Trained 1001 batches 	Training Loss: 0.095652
Trained 1051 batches 	Training Loss: 0.107142
Trained 1101 batches 	Training Loss: 0.133070
Trained 1151 batches 	Training Loss: 0.107975
Trained 1201 batches 	Training Loss: 0.098582
Trained 1251 batches 	Training Loss: 0.109881
Trained 1301 batches 	Training Loss: 0.142928
Trained 1351 batches 	Training Loss: 0.130607
Trained 1401 batches 	Training Loss: 0.147232
Trained 1451 batches 	Training Loss: 0.102651
Trained 1501 batches 	Training Loss: 0.125393
Trained 1551 batches 	Training Loss: 0.127691
Trained 1601 batches 	Training Loss: 0.087192
Trained 1651 batches 	Training Loss: 0.147397
Trained 1701 batches 	Training Loss: 0.107445
Trained 1751 batches 	Training Loss: 0.139854
Trained 1801 batches 	Training Loss: 0.141951
Trained 1851 batches 	Training Loss: 0.125722
Trained 1901 batches 	Training Loss: 0.164446
Trained 1951 batches 	Training Loss: 0.152832
Trained 2001 batches 	Training Loss: 0.122992
Trained 2051 batches 	Training Loss: 0.157586
Trained 2101 batches 	Training Loss: 0.126768
Trained 2151 batches 	Training Loss: 0.136161
Trained 2201 batches 	Training Loss: 0.130998
Trained 2251 batches 	Training Loss: 0.115216
Trained 2301 batches 	Training Loss: 0.126974
Trained 2351 batches 	Training Loss: 0.155139
Trained 2401 batches 	Training Loss: 0.180568
Trained 2451 batches 	Training Loss: 0.123427
Epoch: 12 	Training Loss: 0.123031
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.813
The AUROC of Atelectasis is 0.7811235957535132
The AUROC of Cardiomegaly is 0.8850116243072459
The AUROC of Effusion is 0.8579013613053048
The AUROC of Infiltration is 0.6998848758165284
The AUROC of Mass is 0.813046906008558
The AUROC of Nodule is 0.7356261962244607
The AUROC of Pneumonia is 0.7409746026018802
The AUROC of Pneumothorax is 0.8514153008203433
The AUROC of Consolidation is 0.7706682409550365
The AUROC of Edema is 0.8711809819417344
The AUROC of Emphysema is 0.8884107339003824
The AUROC of Fibrosis is 0.7762965278039721
The AUROC of Pleural_Thickening is 0.7558995072927273
The AUROC of Hernia is 0.9548999834050322
Started epoch 13
Trained 1 batches 	Training Loss: 0.088570
Trained 51 batches 	Training Loss: 0.116053
Trained 101 batches 	Training Loss: 0.143236
Trained 151 batches 	Training Loss: 0.082480
Trained 201 batches 	Training Loss: 0.123217
Trained 251 batches 	Training Loss: 0.088890
Trained 301 batches 	Training Loss: 0.111098
Trained 351 batches 	Training Loss: 0.127245
Trained 401 batches 	Training Loss: 0.123199
Trained 451 batches 	Training Loss: 0.108421
Trained 501 batches 	Training Loss: 0.139807
Trained 551 batches 	Training Loss: 0.130065
Trained 601 batches 	Training Loss: 0.115496
Trained 651 batches 	Training Loss: 0.094304
Trained 701 batches 	Training Loss: 0.181158
Trained 751 batches 	Training Loss: 0.113243
Trained 801 batches 	Training Loss: 0.129436
Trained 851 batches 	Training Loss: 0.107538
Trained 901 batches 	Training Loss: 0.106765
Trained 951 batches 	Training Loss: 0.107367
Trained 1001 batches 	Training Loss: 0.085537
Trained 1051 batches 	Training Loss: 0.100891
Trained 1101 batches 	Training Loss: 0.127331
Trained 1151 batches 	Training Loss: 0.115027
Trained 1201 batches 	Training Loss: 0.130595
Trained 1251 batches 	Training Loss: 0.098232
Trained 1301 batches 	Training Loss: 0.099488
Trained 1351 batches 	Training Loss: 0.140227
Trained 1401 batches 	Training Loss: 0.097712
Trained 1451 batches 	Training Loss: 0.112684
Trained 1501 batches 	Training Loss: 0.092610
Trained 1551 batches 	Training Loss: 0.142737
Trained 1601 batches 	Training Loss: 0.142981
Trained 1651 batches 	Training Loss: 0.115163
Trained 1701 batches 	Training Loss: 0.113613
Trained 1751 batches 	Training Loss: 0.149097
Trained 1801 batches 	Training Loss: 0.123575
Trained 1851 batches 	Training Loss: 0.128295
Trained 1901 batches 	Training Loss: 0.115277
Trained 1951 batches 	Training Loss: 0.111252
Trained 2001 batches 	Training Loss: 0.165939
Trained 2051 batches 	Training Loss: 0.129031
Trained 2101 batches 	Training Loss: 0.135591
Trained 2151 batches 	Training Loss: 0.127524
Trained 2201 batches 	Training Loss: 0.079919
Trained 2251 batches 	Training Loss: 0.131318
Trained 2301 batches 	Training Loss: 0.112731
Trained 2351 batches 	Training Loss: 0.071480
Trained 2401 batches 	Training Loss: 0.089472
Trained 2451 batches 	Training Loss: 0.092635
Epoch: 13 	Training Loss: 0.115304
Started epoch 14
Trained 1 batches 	Training Loss: 0.125289
Trained 51 batches 	Training Loss: 0.093806
Trained 101 batches 	Training Loss: 0.125787
Trained 151 batches 	Training Loss: 0.086665
Trained 201 batches 	Training Loss: 0.112701
Trained 251 batches 	Training Loss: 0.098341
Trained 301 batches 	Training Loss: 0.063322
Trained 351 batches 	Training Loss: 0.101853
Trained 401 batches 	Training Loss: 0.089251
Trained 451 batches 	Training Loss: 0.099574
Trained 501 batches 	Training Loss: 0.072632
Trained 551 batches 	Training Loss: 0.098968
Trained 601 batches 	Training Loss: 0.078475
Trained 651 batches 	Training Loss: 0.117895
Trained 701 batches 	Training Loss: 0.107841
Trained 751 batches 	Training Loss: 0.086746
Trained 801 batches 	Training Loss: 0.142935
Trained 851 batches 	Training Loss: 0.108249
Trained 901 batches 	Training Loss: 0.093079
Trained 951 batches 	Training Loss: 0.132055
Trained 1001 batches 	Training Loss: 0.092851
Trained 1051 batches 	Training Loss: 0.127724
Trained 1101 batches 	Training Loss: 0.087918
Trained 1151 batches 	Training Loss: 0.109692
Trained 1201 batches 	Training Loss: 0.082019
Trained 1251 batches 	Training Loss: 0.121417
Trained 1301 batches 	Training Loss: 0.140378
Trained 1351 batches 	Training Loss: 0.142062
Trained 1401 batches 	Training Loss: 0.086905
Trained 1451 batches 	Training Loss: 0.107844
Trained 1501 batches 	Training Loss: 0.132486
Trained 1551 batches 	Training Loss: 0.132275
Trained 1601 batches 	Training Loss: 0.127187
Trained 1651 batches 	Training Loss: 0.127189
Trained 1701 batches 	Training Loss: 0.121556
Trained 1751 batches 	Training Loss: 0.105746
Trained 1801 batches 	Training Loss: 0.080592
Trained 1851 batches 	Training Loss: 0.126940
Trained 1901 batches 	Training Loss: 0.091341
Trained 1951 batches 	Training Loss: 0.076133
Trained 2001 batches 	Training Loss: 0.133413
Trained 2051 batches 	Training Loss: 0.126515
Trained 2101 batches 	Training Loss: 0.106382
Trained 2151 batches 	Training Loss: 0.094633
Trained 2201 batches 	Training Loss: 0.104186
Trained 2251 batches 	Training Loss: 0.087266
Trained 2301 batches 	Training Loss: 0.115034
Trained 2351 batches 	Training Loss: 0.108579
Trained 2401 batches 	Training Loss: 0.064446
Trained 2451 batches 	Training Loss: 0.119084
Epoch: 14 	Training Loss: 0.106399
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.803
The AUROC of Atelectasis is 0.760162320151058
The AUROC of Cardiomegaly is 0.8745278953023232
The AUROC of Effusion is 0.85448282795907
The AUROC of Infiltration is 0.6947740101186228
The AUROC of Mass is 0.8199345681802823
The AUROC of Nodule is 0.7213376733035345
The AUROC of Pneumonia is 0.7312819829958563
The AUROC of Pneumothorax is 0.846871167694184
The AUROC of Consolidation is 0.7555746618961681
The AUROC of Edema is 0.8646598160271816
The AUROC of Emphysema is 0.8783704052004678
The AUROC of Fibrosis is 0.7422128102377484
The AUROC of Pleural_Thickening is 0.7447697064290267
The AUROC of Hernia is 0.955687180600057
Started epoch 15
Trained 1 batches 	Training Loss: 0.057896
Trained 51 batches 	Training Loss: 0.117013
Trained 101 batches 	Training Loss: 0.114533
Trained 151 batches 	Training Loss: 0.072404
Trained 201 batches 	Training Loss: 0.107955
Trained 251 batches 	Training Loss: 0.081517
Trained 301 batches 	Training Loss: 0.126881
Trained 351 batches 	Training Loss: 0.105280
Trained 401 batches 	Training Loss: 0.119555
Trained 451 batches 	Training Loss: 0.107870
Trained 501 batches 	Training Loss: 0.081185
Trained 551 batches 	Training Loss: 0.113431
Trained 601 batches 	Training Loss: 0.089777
Trained 651 batches 	Training Loss: 0.095969
Trained 701 batches 	Training Loss: 0.095262
Trained 751 batches 	Training Loss: 0.110792
Trained 801 batches 	Training Loss: 0.056382
Trained 851 batches 	Training Loss: 0.109046
Trained 901 batches 	Training Loss: 0.088931
Trained 951 batches 	Training Loss: 0.087231
Trained 1001 batches 	Training Loss: 0.104889
Trained 1051 batches 	Training Loss: 0.123652
Trained 1101 batches 	Training Loss: 0.114260
Trained 1151 batches 	Training Loss: 0.067829
Trained 1201 batches 	Training Loss: 0.092622
Trained 1251 batches 	Training Loss: 0.097332
Trained 1301 batches 	Training Loss: 0.104405
Trained 1351 batches 	Training Loss: 0.077730
Trained 1401 batches 	Training Loss: 0.143692
Trained 1451 batches 	Training Loss: 0.093323
Trained 1501 batches 	Training Loss: 0.114608
Trained 1551 batches 	Training Loss: 0.087992
Trained 1601 batches 	Training Loss: 0.121052
Trained 1651 batches 	Training Loss: 0.133420
Trained 1701 batches 	Training Loss: 0.114758
Trained 1751 batches 	Training Loss: 0.099153
Trained 1801 batches 	Training Loss: 0.065621
Trained 1851 batches 	Training Loss: 0.124064
Trained 1901 batches 	Training Loss: 0.078170
Trained 1951 batches 	Training Loss: 0.079829
Trained 2001 batches 	Training Loss: 0.129639
Trained 2051 batches 	Training Loss: 0.068906
Trained 2101 batches 	Training Loss: 0.101515
Trained 2151 batches 	Training Loss: 0.110456
Trained 2201 batches 	Training Loss: 0.100353
Trained 2251 batches 	Training Loss: 0.114515
Trained 2301 batches 	Training Loss: 0.088154
Trained 2351 batches 	Training Loss: 0.098319
Trained 2401 batches 	Training Loss: 0.099873
Trained 2451 batches 	Training Loss: 0.099743
Epoch: 15 	Training Loss: 0.096724
Started epoch 16
Trained 1 batches 	Training Loss: 0.066571
Trained 51 batches 	Training Loss: 0.064434
Trained 101 batches 	Training Loss: 0.073845
Trained 151 batches 	Training Loss: 0.074152
Trained 201 batches 	Training Loss: 0.083556
Trained 251 batches 	Training Loss: 0.071083
Trained 301 batches 	Training Loss: 0.061557
Trained 351 batches 	Training Loss: 0.068862
Trained 401 batches 	Training Loss: 0.065204
Trained 451 batches 	Training Loss: 0.043290
Trained 501 batches 	Training Loss: 0.110546
Trained 551 batches 	Training Loss: 0.057102
Trained 601 batches 	Training Loss: 0.076265
Trained 651 batches 	Training Loss: 0.070353
Trained 701 batches 	Training Loss: 0.056732
Trained 751 batches 	Training Loss: 0.048305
Trained 801 batches 	Training Loss: 0.069926
Trained 851 batches 	Training Loss: 0.092145
Trained 901 batches 	Training Loss: 0.091678
Trained 951 batches 	Training Loss: 0.100781
Trained 1001 batches 	Training Loss: 0.088627
Trained 1051 batches 	Training Loss: 0.088027
Trained 1101 batches 	Training Loss: 0.083179
Trained 1151 batches 	Training Loss: 0.083294
Trained 1201 batches 	Training Loss: 0.129810
Trained 1251 batches 	Training Loss: 0.089585
Trained 1301 batches 	Training Loss: 0.076771
Trained 1351 batches 	Training Loss: 0.089844
Trained 1401 batches 	Training Loss: 0.075697
Trained 1451 batches 	Training Loss: 0.128127
Trained 1501 batches 	Training Loss: 0.089238
Trained 1551 batches 	Training Loss: 0.118910
Trained 1601 batches 	Training Loss: 0.056267
Trained 1651 batches 	Training Loss: 0.107316
Trained 1701 batches 	Training Loss: 0.097074
Trained 1751 batches 	Training Loss: 0.144110
Trained 1801 batches 	Training Loss: 0.109496
Trained 1851 batches 	Training Loss: 0.095812
Trained 1901 batches 	Training Loss: 0.066080
Trained 1951 batches 	Training Loss: 0.111743
Trained 2001 batches 	Training Loss: 0.104736
Trained 2051 batches 	Training Loss: 0.130805
Trained 2101 batches 	Training Loss: 0.090584
Trained 2151 batches 	Training Loss: 0.100924
Trained 2201 batches 	Training Loss: 0.103864
Trained 2251 batches 	Training Loss: 0.091855
Trained 2301 batches 	Training Loss: 0.104123
Trained 2351 batches 	Training Loss: 0.061315
Trained 2401 batches 	Training Loss: 0.106507
Trained 2451 batches 	Training Loss: 0.091755
Epoch: 16 	Training Loss: 0.086812
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.786
The AUROC of Atelectasis is 0.7593453225273773
The AUROC of Cardiomegaly is 0.8649962668673857
The AUROC of Effusion is 0.8528225087084835
The AUROC of Infiltration is 0.6658050247998658
The AUROC of Mass is 0.7964924381353173
The AUROC of Nodule is 0.711158697216653
The AUROC of Pneumonia is 0.688019925030569
The AUROC of Pneumothorax is 0.8234509548393564
The AUROC of Consolidation is 0.7324053129652333
The AUROC of Edema is 0.8326016860408176
The AUROC of Emphysema is 0.8607425300919888
The AUROC of Fibrosis is 0.7611020404008836
The AUROC of Pleural_Thickening is 0.7349445531429106
The AUROC of Hernia is 0.9139955151035484
Started epoch 17
Trained 1 batches 	Training Loss: 0.118036
Trained 51 batches 	Training Loss: 0.079519
Trained 101 batches 	Training Loss: 0.048944
Trained 151 batches 	Training Loss: 0.075956
Trained 201 batches 	Training Loss: 0.075092
Trained 251 batches 	Training Loss: 0.072877
Trained 301 batches 	Training Loss: 0.042226
Trained 351 batches 	Training Loss: 0.100492
Trained 401 batches 	Training Loss: 0.102041
Trained 451 batches 	Training Loss: 0.129398
Trained 501 batches 	Training Loss: 0.059378
Trained 551 batches 	Training Loss: 0.069324
Trained 601 batches 	Training Loss: 0.092372
Trained 651 batches 	Training Loss: 0.051863
Trained 701 batches 	Training Loss: 0.084980
Trained 751 batches 	Training Loss: 0.070988
Trained 801 batches 	Training Loss: 0.066468
Trained 851 batches 	Training Loss: 0.080002
Trained 901 batches 	Training Loss: 0.051254
Trained 951 batches 	Training Loss: 0.062837
Trained 1001 batches 	Training Loss: 0.061173
Trained 1051 batches 	Training Loss: 0.062132
Trained 1101 batches 	Training Loss: 0.047255
Trained 1151 batches 	Training Loss: 0.069496
Trained 1201 batches 	Training Loss: 0.067240
Trained 1251 batches 	Training Loss: 0.077325
Trained 1301 batches 	Training Loss: 0.103908
Trained 1351 batches 	Training Loss: 0.115297
Trained 1401 batches 	Training Loss: 0.089215
Trained 1451 batches 	Training Loss: 0.069319
Trained 1501 batches 	Training Loss: 0.082767
Trained 1551 batches 	Training Loss: 0.091206
Trained 1601 batches 	Training Loss: 0.062134
Trained 1651 batches 	Training Loss: 0.088526
Trained 1701 batches 	Training Loss: 0.105392
Trained 1751 batches 	Training Loss: 0.070574
Trained 1801 batches 	Training Loss: 0.122545
Trained 1851 batches 	Training Loss: 0.130542
Trained 1901 batches 	Training Loss: 0.100758
Trained 1951 batches 	Training Loss: 0.078913
Trained 2001 batches 	Training Loss: 0.061353
Trained 2051 batches 	Training Loss: 0.101749
Trained 2101 batches 	Training Loss: 0.128955
Trained 2151 batches 	Training Loss: 0.072818
Trained 2201 batches 	Training Loss: 0.100987
Trained 2251 batches 	Training Loss: 0.102871
Trained 2301 batches 	Training Loss: 0.072134
Trained 2351 batches 	Training Loss: 0.128501
Trained 2401 batches 	Training Loss: 0.062061
Trained 2451 batches 	Training Loss: 0.051055
Epoch: 17 	Training Loss: 0.076975
Started epoch 18
Trained 1 batches 	Training Loss: 0.063949
Trained 51 batches 	Training Loss: 0.049664
Trained 101 batches 	Training Loss: 0.040098
Trained 151 batches 	Training Loss: 0.035648
Trained 201 batches 	Training Loss: 0.049662
Trained 251 batches 	Training Loss: 0.072162
Trained 301 batches 	Training Loss: 0.067425
Trained 351 batches 	Training Loss: 0.072594
Trained 401 batches 	Training Loss: 0.054087
Trained 451 batches 	Training Loss: 0.078822
Trained 501 batches 	Training Loss: 0.055081
Trained 551 batches 	Training Loss: 0.066170
Trained 601 batches 	Training Loss: 0.055698
Trained 651 batches 	Training Loss: 0.081474
Trained 701 batches 	Training Loss: 0.062257
Trained 751 batches 	Training Loss: 0.095032
Trained 801 batches 	Training Loss: 0.086126
Trained 851 batches 	Training Loss: 0.064168
Trained 901 batches 	Training Loss: 0.088694
Trained 951 batches 	Training Loss: 0.052448
Trained 1001 batches 	Training Loss: 0.070709
Trained 1051 batches 	Training Loss: 0.069424
Trained 1101 batches 	Training Loss: 0.103088
Trained 1151 batches 	Training Loss: 0.098041
Trained 1201 batches 	Training Loss: 0.073150
Trained 1251 batches 	Training Loss: 0.052440
Trained 1301 batches 	Training Loss: 0.068338
Trained 1351 batches 	Training Loss: 0.064124
Trained 1401 batches 	Training Loss: 0.072059
Trained 1451 batches 	Training Loss: 0.107788
Trained 1501 batches 	Training Loss: 0.070126
Trained 1551 batches 	Training Loss: 0.094143
Trained 1601 batches 	Training Loss: 0.084068
Trained 1651 batches 	Training Loss: 0.082462
Trained 1701 batches 	Training Loss: 0.052038
Trained 1751 batches 	Training Loss: 0.076787
Trained 1801 batches 	Training Loss: 0.067519
Trained 1851 batches 	Training Loss: 0.060295
Trained 1901 batches 	Training Loss: 0.063289
Trained 1951 batches 	Training Loss: 0.074797
Trained 2001 batches 	Training Loss: 0.036022
Trained 2051 batches 	Training Loss: 0.065092
Trained 2101 batches 	Training Loss: 0.103741
Trained 2151 batches 	Training Loss: 0.066321
Trained 2201 batches 	Training Loss: 0.093162
Trained 2251 batches 	Training Loss: 0.116403
Trained 2301 batches 	Training Loss: 0.071577
Trained 2351 batches 	Training Loss: 0.077025
Trained 2401 batches 	Training Loss: 0.076103
Trained 2451 batches 	Training Loss: 0.068329
Epoch: 18 	Training Loss: 0.067940
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.779
The AUROC of Atelectasis is 0.7385292786898712
The AUROC of Cardiomegaly is 0.8614889432503142
The AUROC of Effusion is 0.8259883221655094
The AUROC of Infiltration is 0.6509868501397211
The AUROC of Mass is 0.7939775384928466
The AUROC of Nodule is 0.7041600184280005
The AUROC of Pneumonia is 0.6998187331941617
The AUROC of Pneumothorax is 0.8099533857951982
The AUROC of Consolidation is 0.7303324602503103
The AUROC of Edema is 0.8445832736915856
The AUROC of Emphysema is 0.857176567334135
The AUROC of Fibrosis is 0.734518572803077
The AUROC of Pleural_Thickening is 0.7314658919108127
The AUROC of Hernia is 0.9167741084459876
Started epoch 19
Trained 1 batches 	Training Loss: 0.061912
Trained 51 batches 	Training Loss: 0.049674
Trained 101 batches 	Training Loss: 0.049029
Trained 151 batches 	Training Loss: 0.050414
Trained 201 batches 	Training Loss: 0.037170
Trained 251 batches 	Training Loss: 0.051498
Trained 301 batches 	Training Loss: 0.037808
Trained 351 batches 	Training Loss: 0.086570
Trained 401 batches 	Training Loss: 0.043342
Trained 451 batches 	Training Loss: 0.035674
Trained 501 batches 	Training Loss: 0.086619
Trained 551 batches 	Training Loss: 0.056835
Trained 601 batches 	Training Loss: 0.037304
Trained 651 batches 	Training Loss: 0.052092
Trained 701 batches 	Training Loss: 0.062246
Trained 751 batches 	Training Loss: 0.064002
Trained 801 batches 	Training Loss: 0.080717
Trained 851 batches 	Training Loss: 0.066721
Trained 901 batches 	Training Loss: 0.069355
Trained 951 batches 	Training Loss: 0.055775
Trained 1001 batches 	Training Loss: 0.052696
Trained 1051 batches 	Training Loss: 0.055253
Trained 1101 batches 	Training Loss: 0.076598
Trained 1151 batches 	Training Loss: 0.052996
Trained 1201 batches 	Training Loss: 0.060803
Trained 1251 batches 	Training Loss: 0.046599
Trained 1301 batches 	Training Loss: 0.047551
Trained 1351 batches 	Training Loss: 0.068047
Trained 1401 batches 	Training Loss: 0.072319
Trained 1451 batches 	Training Loss: 0.044693
Trained 1501 batches 	Training Loss: 0.064605
Trained 1551 batches 	Training Loss: 0.054732
Trained 1601 batches 	Training Loss: 0.077335
Trained 1651 batches 	Training Loss: 0.071252
Trained 1701 batches 	Training Loss: 0.060110
Trained 1751 batches 	Training Loss: 0.035191
Trained 1801 batches 	Training Loss: 0.044474
Trained 1851 batches 	Training Loss: 0.072710
Trained 1901 batches 	Training Loss: 0.130208
Trained 1951 batches 	Training Loss: 0.043762
Trained 2001 batches 	Training Loss: 0.046471
Trained 2051 batches 	Training Loss: 0.060853
Trained 2101 batches 	Training Loss: 0.062221
Trained 2151 batches 	Training Loss: 0.079735
Trained 2201 batches 	Training Loss: 0.067271
Trained 2251 batches 	Training Loss: 0.086580
Trained 2301 batches 	Training Loss: 0.055023
Trained 2351 batches 	Training Loss: 0.061421
Trained 2401 batches 	Training Loss: 0.127975
Trained 2451 batches 	Training Loss: 0.083420
Epoch: 19 	Training Loss: 0.059338
Started epoch 20
Trained 1 batches 	Training Loss: 0.046418
Trained 51 batches 	Training Loss: 0.041423
Trained 101 batches 	Training Loss: 0.045522
Trained 151 batches 	Training Loss: 0.038099
Trained 201 batches 	Training Loss: 0.040034
Trained 251 batches 	Training Loss: 0.027300
Trained 301 batches 	Training Loss: 0.032539
Trained 351 batches 	Training Loss: 0.057236
Trained 401 batches 	Training Loss: 0.039836
Trained 451 batches 	Training Loss: 0.047726
Trained 501 batches 	Training Loss: 0.030526
Trained 551 batches 	Training Loss: 0.038465
Trained 601 batches 	Training Loss: 0.060249
Trained 651 batches 	Training Loss: 0.040182
Trained 701 batches 	Training Loss: 0.039815
Trained 751 batches 	Training Loss: 0.057784
Trained 801 batches 	Training Loss: 0.051912
Trained 851 batches 	Training Loss: 0.069866
Trained 901 batches 	Training Loss: 0.050098
Trained 951 batches 	Training Loss: 0.064375
Trained 1001 batches 	Training Loss: 0.038369
Trained 1051 batches 	Training Loss: 0.090867
Trained 1101 batches 	Training Loss: 0.046938
Trained 1151 batches 	Training Loss: 0.032906
Trained 1201 batches 	Training Loss: 0.057054
Trained 1251 batches 	Training Loss: 0.042635
Trained 1301 batches 	Training Loss: 0.061628
Trained 1351 batches 	Training Loss: 0.062341
Trained 1401 batches 	Training Loss: 0.033719
Trained 1451 batches 	Training Loss: 0.055274
Trained 1501 batches 	Training Loss: 0.041101
Trained 1551 batches 	Training Loss: 0.035645
Trained 1601 batches 	Training Loss: 0.068527
Trained 1651 batches 	Training Loss: 0.051898
Trained 1701 batches 	Training Loss: 0.054727
Trained 1751 batches 	Training Loss: 0.041176
Trained 1801 batches 	Training Loss: 0.037278
Trained 1851 batches 	Training Loss: 0.050920
Trained 1901 batches 	Training Loss: 0.035148
Trained 1951 batches 	Training Loss: 0.063741
Trained 2001 batches 	Training Loss: 0.064421
Trained 2051 batches 	Training Loss: 0.049737
Trained 2101 batches 	Training Loss: 0.047055
Trained 2151 batches 	Training Loss: 0.055385
Trained 2201 batches 	Training Loss: 0.070821
Trained 2251 batches 	Training Loss: 0.032587
Trained 2301 batches 	Training Loss: 0.060273
Trained 2351 batches 	Training Loss: 0.080873
Trained 2401 batches 	Training Loss: 0.054354
Trained 2451 batches 	Training Loss: 0.061676
Epoch: 20 	Training Loss: 0.052015
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 0.0 min
The average AUROC is 0.768
The AUROC of Atelectasis is 0.7283266517387252
The AUROC of Cardiomegaly is 0.8564143109487012
The AUROC of Effusion is 0.8286575252787924
The AUROC of Infiltration is 0.6427012038124251
The AUROC of Mass is 0.7830871484418388
The AUROC of Nodule is 0.6820690125687474
The AUROC of Pneumonia is 0.6938237559742161
The AUROC of Pneumothorax is 0.8132686749202211
The AUROC of Consolidation is 0.7066875698501153
The AUROC of Edema is 0.8460184424840097
The AUROC of Emphysema is 0.863210245080284
The AUROC of Fibrosis is 0.7198713216473847
The AUROC of Pleural_Thickening is 0.7023739798214508
The AUROC of Hernia is 0.8903881094927472
Training time lapse: 222.0 min
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.770
The AUROC of Atelectasis is 0.7322632993020508
The AUROC of Cardiomegaly is 0.85897829791379
The AUROC of Effusion is 0.8313560552815595
The AUROC of Infiltration is 0.6360111922327215
The AUROC of Mass is 0.7804381247496229
The AUROC of Nodule is 0.685430258183139
The AUROC of Pneumonia is 0.6555570037755926
The AUROC of Pneumothorax is 0.8296425401005026
The AUROC of Consolidation is 0.7306739327935645
The AUROC of Edema is 0.853588004342202
The AUROC of Emphysema is 0.8338189916461487
The AUROC of Fibrosis is 0.7383913274691218
The AUROC of Pleural_Thickening is 0.7272040901897664
The AUROC of Hernia is 0.8800938323130468
