Started training, total epoch : 1
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.640719
Trained 51 batches 	Training Loss: 0.168505
Trained 101 batches 	Training Loss: 0.200565
Trained 151 batches 	Training Loss: 0.196193
Trained 201 batches 	Training Loss: 0.168262
Trained 251 batches 	Training Loss: 0.165049
Trained 301 batches 	Training Loss: 0.178422
Trained 351 batches 	Training Loss: 0.133652
Trained 401 batches 	Training Loss: 0.233188
Trained 451 batches 	Training Loss: 0.185645
Trained 501 batches 	Training Loss: 0.173209
Trained 551 batches 	Training Loss: 0.153550
Trained 601 batches 	Training Loss: 0.125792
Trained 651 batches 	Training Loss: 0.215315
Trained 701 batches 	Training Loss: 0.134343
Trained 751 batches 	Training Loss: 0.142860
Trained 801 batches 	Training Loss: 0.157523
Trained 851 batches 	Training Loss: 0.113579
Trained 901 batches 	Training Loss: 0.169891
Trained 951 batches 	Training Loss: 0.196039
Trained 1001 batches 	Training Loss: 0.187517
Trained 1051 batches 	Training Loss: 0.189261
Trained 1101 batches 	Training Loss: 0.135193
Trained 1151 batches 	Training Loss: 0.214595
Trained 1201 batches 	Training Loss: 0.159974
Trained 1251 batches 	Training Loss: 0.184880
Trained 1301 batches 	Training Loss: 0.160902
Trained 1351 batches 	Training Loss: 0.203734
Trained 1401 batches 	Training Loss: 0.166819
Trained 1451 batches 	Training Loss: 0.132363
Trained 1501 batches 	Training Loss: 0.136427
Trained 1551 batches 	Training Loss: 0.167915
Trained 1601 batches 	Training Loss: 0.164162
Trained 1651 batches 	Training Loss: 0.125966
Trained 1701 batches 	Training Loss: 0.149854
Trained 1751 batches 	Training Loss: 0.192873
Trained 1801 batches 	Training Loss: 0.209129
Trained 1851 batches 	Training Loss: 0.187065
Trained 1901 batches 	Training Loss: 0.157496
Trained 1951 batches 	Training Loss: 0.183956
Trained 2001 batches 	Training Loss: 0.164214
Trained 2051 batches 	Training Loss: 0.186659
Trained 2101 batches 	Training Loss: 0.121794
Trained 2151 batches 	Training Loss: 0.231760
Trained 2201 batches 	Training Loss: 0.215263
Trained 2251 batches 	Training Loss: 0.144895
Trained 2301 batches 	Training Loss: 0.178224
Trained 2351 batches 	Training Loss: 0.163053
Trained 2401 batches 	Training Loss: 0.208026
Trained 2451 batches 	Training Loss: 0.183893
Epoch: 1 	Training Loss: 0.167753
Training time lapse: 11.0 min
Started training, total epoch : 1
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.740737
Trained 51 batches 	Training Loss: 0.177907
Trained 101 batches 	Training Loss: 0.171941
Trained 151 batches 	Training Loss: 0.140375
Trained 201 batches 	Training Loss: 0.122260
Trained 251 batches 	Training Loss: 0.181593
Trained 301 batches 	Training Loss: 0.214485
Trained 351 batches 	Training Loss: 0.170564
Trained 401 batches 	Training Loss: 0.241938
Trained 451 batches 	Training Loss: 0.194221
Trained 501 batches 	Training Loss: 0.129009
Trained 551 batches 	Training Loss: 0.146206
Trained 601 batches 	Training Loss: 0.182446
Trained 651 batches 	Training Loss: 0.166202
Trained 701 batches 	Training Loss: 0.169862
Trained 751 batches 	Training Loss: 0.178426
Trained 801 batches 	Training Loss: 0.171226
Trained 851 batches 	Training Loss: 0.173264
Trained 901 batches 	Training Loss: 0.186966
Trained 951 batches 	Training Loss: 0.155957
Trained 1001 batches 	Training Loss: 0.164725
Trained 1051 batches 	Training Loss: 0.170739
Trained 1101 batches 	Training Loss: 0.234563
Trained 1151 batches 	Training Loss: 0.204859
Trained 1201 batches 	Training Loss: 0.148752
Trained 1251 batches 	Training Loss: 0.165064
Trained 1301 batches 	Training Loss: 0.171705
Trained 1351 batches 	Training Loss: 0.132264
Trained 1401 batches 	Training Loss: 0.137597
Trained 1451 batches 	Training Loss: 0.162082
Trained 1501 batches 	Training Loss: 0.135666
Trained 1551 batches 	Training Loss: 0.142696
Trained 1601 batches 	Training Loss: 0.131398
Trained 1651 batches 	Training Loss: 0.169580
Trained 1701 batches 	Training Loss: 0.162946
Trained 1751 batches 	Training Loss: 0.160087
Trained 1801 batches 	Training Loss: 0.148674
Trained 1851 batches 	Training Loss: 0.187277
Trained 1901 batches 	Training Loss: 0.208067
Trained 1951 batches 	Training Loss: 0.172026
Trained 2001 batches 	Training Loss: 0.210002
Trained 2051 batches 	Training Loss: 0.172125
Trained 2101 batches 	Training Loss: 0.126030
Trained 2151 batches 	Training Loss: 0.151287
Trained 2201 batches 	Training Loss: 0.191323
Trained 2251 batches 	Training Loss: 0.167920
Trained 2301 batches 	Training Loss: 0.191093
Trained 2351 batches 	Training Loss: 0.157486
Trained 2401 batches 	Training Loss: 0.164423
Trained 2451 batches 	Training Loss: 0.176949
Epoch: 1 	Training Loss: 0.168074
Training time lapse: 11.0 min
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.766
The AUROC of Atelectasis is 0.7542166243197372
The AUROC of Cardiomegaly is 0.862933054798087
The AUROC of Effusion is 0.8542361198781321
The AUROC of Infiltration is 0.6617756929997841
The AUROC of Mass is 0.7622089165893513
The AUROC of Nodule is 0.6755269521818253
The AUROC of Pneumonia is 0.717489009016176
The AUROC of Pneumothorax is 0.7978441095347301
The AUROC of Consolidation is 0.7664351918700578
The AUROC of Edema is 0.8684347962805272
The AUROC of Emphysema is 0.7762270586128346
The AUROC of Fibrosis is 0.7129047286279991
The AUROC of Pleural_Thickening is 0.731439986203725
The AUROC of Hernia is 0.7792167993404011
Started training, total epoch : 1
Training data size: 1227
Started epoch 1
Trained 1 batches 	Training Loss: 0.757165
Trained 51 batches 	Training Loss: 0.183687
Trained 101 batches 	Training Loss: 0.191457
Trained 151 batches 	Training Loss: 0.182961
Trained 201 batches 	Training Loss: 0.138486
Trained 251 batches 	Training Loss: 0.140678
Trained 301 batches 	Training Loss: 0.192720
Trained 351 batches 	Training Loss: 0.149569
Trained 401 batches 	Training Loss: 0.152596
Trained 451 batches 	Training Loss: 0.138858
Trained 501 batches 	Training Loss: 0.170456
Trained 551 batches 	Training Loss: 0.200706
Trained 601 batches 	Training Loss: 0.144852
Trained 651 batches 	Training Loss: 0.187112
Trained 701 batches 	Training Loss: 0.164596
Trained 751 batches 	Training Loss: 0.143455
Trained 801 batches 	Training Loss: 0.160721
Trained 851 batches 	Training Loss: 0.146036
Trained 901 batches 	Training Loss: 0.149389
Trained 951 batches 	Training Loss: 0.120611
Trained 1001 batches 	Training Loss: 0.151053
Trained 1051 batches 	Training Loss: 0.164025
Trained 1101 batches 	Training Loss: 0.167851
Trained 1151 batches 	Training Loss: 0.138252
Trained 1201 batches 	Training Loss: 0.221710
Epoch: 1 	Training Loss: 0.164556
Training time lapse: 14.0 min
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 2.0 min
The average AUROC is 0.780
The AUROC of Atelectasis is 0.7661686275257532
The AUROC of Cardiomegaly is 0.8657217122251231
The AUROC of Effusion is 0.8448327682528212
The AUROC of Infiltration is 0.6733550252334459
The AUROC of Mass is 0.7723375755114525
The AUROC of Nodule is 0.6922096968761703
The AUROC of Pneumonia is 0.7203908453868056
The AUROC of Pneumothorax is 0.8254565529588886
The AUROC of Consolidation is 0.7752577215438914
The AUROC of Edema is 0.8790475553988261
The AUROC of Emphysema is 0.8354460932482831
The AUROC of Fibrosis is 0.7158227296569453
The AUROC of Pleural_Thickening is 0.7416254919995131
The AUROC of Hernia is 0.8160465294448134
Started training, total epoch : 2
Training data size: 1227
Started epoch 1
Trained 1 batches 	Training Loss: 0.726925
Trained 51 batches 	Training Loss: 0.212240
Trained 101 batches 	Training Loss: 0.159287
Trained 151 batches 	Training Loss: 0.167297
Trained 201 batches 	Training Loss: 0.190111
Trained 251 batches 	Training Loss: 0.128173
Trained 301 batches 	Training Loss: 0.187263
Trained 351 batches 	Training Loss: 0.181091
Trained 401 batches 	Training Loss: 0.188493
Trained 451 batches 	Training Loss: 0.203161
Trained 501 batches 	Training Loss: 0.154881
Trained 551 batches 	Training Loss: 0.174254
Trained 601 batches 	Training Loss: 0.163213
Trained 651 batches 	Training Loss: 0.159005
Trained 701 batches 	Training Loss: 0.160681
Trained 751 batches 	Training Loss: 0.203259
Trained 801 batches 	Training Loss: 0.166272
Trained 851 batches 	Training Loss: 0.130867
Trained 901 batches 	Training Loss: 0.165524
Trained 951 batches 	Training Loss: 0.153018
Trained 1001 batches 	Training Loss: 0.180571
Trained 1051 batches 	Training Loss: 0.166690
Trained 1101 batches 	Training Loss: 0.166023
Trained 1151 batches 	Training Loss: 0.126228
Trained 1201 batches 	Training Loss: 0.202163
Epoch: 1 	Training Loss: 0.164996
Started epoch 2
Trained 1 batches 	Training Loss: 0.159039
Trained 51 batches 	Training Loss: 0.159556
Trained 101 batches 	Training Loss: 0.170990
Trained 151 batches 	Training Loss: 0.214793
Trained 201 batches 	Training Loss: 0.182852
Trained 251 batches 	Training Loss: 0.153120
Trained 301 batches 	Training Loss: 0.127507
Trained 351 batches 	Training Loss: 0.132789
Trained 401 batches 	Training Loss: 0.175933
Trained 451 batches 	Training Loss: 0.146446
Trained 501 batches 	Training Loss: 0.143305
Trained 551 batches 	Training Loss: 0.187744
Trained 601 batches 	Training Loss: 0.149865
Trained 651 batches 	Training Loss: 0.164231
Trained 701 batches 	Training Loss: 0.151508
Trained 751 batches 	Training Loss: 0.175974
Trained 801 batches 	Training Loss: 0.152541
Trained 851 batches 	Training Loss: 0.196460
Trained 901 batches 	Training Loss: 0.133179
Trained 951 batches 	Training Loss: 0.135511
Trained 1001 batches 	Training Loss: 0.178874
Trained 1051 batches 	Training Loss: 0.179650
Trained 1101 batches 	Training Loss: 0.126165
Trained 1151 batches 	Training Loss: 0.130029
Trained 1201 batches 	Training Loss: 0.138309
Epoch: 2 	Training Loss: 0.156070
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 176
Started training, total epoch : 2
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.792461
Trained 51 batches 	Training Loss: 0.235552
Trained 101 batches 	Training Loss: 0.163405
Trained 151 batches 	Training Loss: 0.159793
Trained 201 batches 	Training Loss: 0.142122
Trained 251 batches 	Training Loss: 0.224651
Trained 301 batches 	Training Loss: 0.205842
Trained 351 batches 	Training Loss: 0.161487
Trained 401 batches 	Training Loss: 0.148134
Trained 451 batches 	Training Loss: 0.222352
Trained 501 batches 	Training Loss: 0.162042
Trained 551 batches 	Training Loss: 0.146923
Trained 601 batches 	Training Loss: 0.160855
Trained 651 batches 	Training Loss: 0.148935
Trained 701 batches 	Training Loss: 0.147224
Trained 751 batches 	Training Loss: 0.158004
Trained 801 batches 	Training Loss: 0.157354
Trained 851 batches 	Training Loss: 0.137631
Trained 901 batches 	Training Loss: 0.150173
Trained 951 batches 	Training Loss: 0.175981
Trained 1001 batches 	Training Loss: 0.176319
Trained 1051 batches 	Training Loss: 0.199681
Trained 1101 batches 	Training Loss: 0.146949
Trained 1151 batches 	Training Loss: 0.150112
Trained 1201 batches 	Training Loss: 0.102108
Trained 1251 batches 	Training Loss: 0.168056
Trained 1301 batches 	Training Loss: 0.163576
Trained 1351 batches 	Training Loss: 0.136927
Trained 1401 batches 	Training Loss: 0.164745
Trained 1451 batches 	Training Loss: 0.128149
Trained 1501 batches 	Training Loss: 0.166395
Trained 1551 batches 	Training Loss: 0.192745
Trained 1601 batches 	Training Loss: 0.184132
Trained 1651 batches 	Training Loss: 0.157465
Trained 1701 batches 	Training Loss: 0.217207
Trained 1751 batches 	Training Loss: 0.150576
Trained 1801 batches 	Training Loss: 0.200465
Trained 1851 batches 	Training Loss: 0.166860
Trained 1901 batches 	Training Loss: 0.218120
Trained 1951 batches 	Training Loss: 0.122932
Trained 2001 batches 	Training Loss: 0.169191
Trained 2051 batches 	Training Loss: 0.192793
Trained 2101 batches 	Training Loss: 0.124382
Trained 2151 batches 	Training Loss: 0.167989
Trained 2201 batches 	Training Loss: 0.165389
Trained 2251 batches 	Training Loss: 0.145283
Trained 2301 batches 	Training Loss: 0.159538
Trained 2351 batches 	Training Loss: 0.184487
Trained 2401 batches 	Training Loss: 0.175875
Trained 2451 batches 	Training Loss: 0.192246
Epoch: 1 	Training Loss: 0.167198
Started epoch 2
Trained 1 batches 	Training Loss: 0.180439
Trained 51 batches 	Training Loss: 0.165748
Trained 101 batches 	Training Loss: 0.202028
Trained 151 batches 	Training Loss: 0.168869
Trained 201 batches 	Training Loss: 0.194629
Trained 251 batches 	Training Loss: 0.170890
Trained 301 batches 	Training Loss: 0.182499
Trained 351 batches 	Training Loss: 0.098252
Trained 401 batches 	Training Loss: 0.101567
Trained 451 batches 	Training Loss: 0.164990
Trained 501 batches 	Training Loss: 0.200409
Trained 551 batches 	Training Loss: 0.117339
Trained 601 batches 	Training Loss: 0.160968
Trained 651 batches 	Training Loss: 0.138310
Trained 701 batches 	Training Loss: 0.171803
Trained 751 batches 	Training Loss: 0.141643
Trained 801 batches 	Training Loss: 0.171669
Trained 851 batches 	Training Loss: 0.136731
Trained 901 batches 	Training Loss: 0.197713
Trained 951 batches 	Training Loss: 0.185251
Trained 1001 batches 	Training Loss: 0.180711
Trained 1051 batches 	Training Loss: 0.153996
Trained 1101 batches 	Training Loss: 0.121550
Trained 1151 batches 	Training Loss: 0.189070
Trained 1201 batches 	Training Loss: 0.148427
Trained 1251 batches 	Training Loss: 0.176432
Trained 1301 batches 	Training Loss: 0.123988
Trained 1351 batches 	Training Loss: 0.183700
Trained 1401 batches 	Training Loss: 0.141288
Trained 1451 batches 	Training Loss: 0.112213
Trained 1501 batches 	Training Loss: 0.156641
Trained 1551 batches 	Training Loss: 0.144097
Trained 1601 batches 	Training Loss: 0.198381
Trained 1651 batches 	Training Loss: 0.130996
Trained 1701 batches 	Training Loss: 0.118332
Trained 1751 batches 	Training Loss: 0.136039
Trained 1801 batches 	Training Loss: 0.151506
Trained 1851 batches 	Training Loss: 0.127667
Trained 1901 batches 	Training Loss: 0.143654
Trained 1951 batches 	Training Loss: 0.186569
Trained 2001 batches 	Training Loss: 0.139145
Trained 2051 batches 	Training Loss: 0.122301
Trained 2101 batches 	Training Loss: 0.159155
Trained 2151 batches 	Training Loss: 0.113731
Trained 2201 batches 	Training Loss: 0.174558
Trained 2251 batches 	Training Loss: 0.137134
Trained 2301 batches 	Training Loss: 0.232770
Trained 2351 batches 	Training Loss: 0.197365
Trained 2401 batches 	Training Loss: 0.172668
Trained 2451 batches 	Training Loss: 0.173609
Epoch: 2 	Training Loss: 0.158487
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Started training, total epoch : 3
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.733485
Trained 51 batches 	Training Loss: 0.201319
Trained 101 batches 	Training Loss: 0.269197
Trained 151 batches 	Training Loss: 0.151542
Trained 201 batches 	Training Loss: 0.110879
Trained 251 batches 	Training Loss: 0.126564
Trained 301 batches 	Training Loss: 0.151641
Trained 351 batches 	Training Loss: 0.128052
Trained 401 batches 	Training Loss: 0.255284
Trained 451 batches 	Training Loss: 0.131866
Trained 501 batches 	Training Loss: 0.125025
Trained 551 batches 	Training Loss: 0.116736
Trained 601 batches 	Training Loss: 0.183359
Trained 651 batches 	Training Loss: 0.115047
Trained 701 batches 	Training Loss: 0.105235
Trained 751 batches 	Training Loss: 0.112238
Trained 801 batches 	Training Loss: 0.150425
Trained 851 batches 	Training Loss: 0.115035
Trained 901 batches 	Training Loss: 0.146872
Trained 951 batches 	Training Loss: 0.143451
Trained 1001 batches 	Training Loss: 0.164661
Trained 1051 batches 	Training Loss: 0.152800
Trained 1101 batches 	Training Loss: 0.124324
Trained 1151 batches 	Training Loss: 0.201049
Trained 1201 batches 	Training Loss: 0.203482
Trained 1251 batches 	Training Loss: 0.105718
Trained 1301 batches 	Training Loss: 0.257697
Trained 1351 batches 	Training Loss: 0.230041
Trained 1401 batches 	Training Loss: 0.195332
Trained 1451 batches 	Training Loss: 0.135371
Trained 1501 batches 	Training Loss: 0.172761
Trained 1551 batches 	Training Loss: 0.212577
Trained 1601 batches 	Training Loss: 0.240275
Trained 1651 batches 	Training Loss: 0.223609
Trained 1701 batches 	Training Loss: 0.232027
Trained 1751 batches 	Training Loss: 0.160136
Trained 1801 batches 	Training Loss: 0.250068
Trained 1851 batches 	Training Loss: 0.198066
Trained 1901 batches 	Training Loss: 0.184684
Trained 1951 batches 	Training Loss: 0.202797
Trained 2001 batches 	Training Loss: 0.114296
Trained 2051 batches 	Training Loss: 0.178656
Trained 2101 batches 	Training Loss: 0.198908
Trained 2151 batches 	Training Loss: 0.139725
Trained 2201 batches 	Training Loss: 0.179924
Trained 2251 batches 	Training Loss: 0.195918
Trained 2301 batches 	Training Loss: 0.104015
Trained 2351 batches 	Training Loss: 0.130358
Trained 2401 batches 	Training Loss: 0.227102
Trained 2451 batches 	Training Loss: 0.107225
Trained 2501 batches 	Training Loss: 0.179428
Trained 2551 batches 	Training Loss: 0.145043
Trained 2601 batches 	Training Loss: 0.284968
Trained 2651 batches 	Training Loss: 0.143918
Trained 2701 batches 	Training Loss: 0.239119
Trained 2751 batches 	Training Loss: 0.161919
Trained 2801 batches 	Training Loss: 0.136706
Trained 2851 batches 	Training Loss: 0.164842
Trained 2901 batches 	Training Loss: 0.149963
Trained 2951 batches 	Training Loss: 0.208942
Trained 3001 batches 	Training Loss: 0.117030
Trained 3051 batches 	Training Loss: 0.117440
Trained 3101 batches 	Training Loss: 0.101001
Trained 3151 batches 	Training Loss: 0.126766
Trained 3201 batches 	Training Loss: 0.123462
Trained 3251 batches 	Training Loss: 0.118834
Trained 3301 batches 	Training Loss: 0.136811
Trained 3351 batches 	Training Loss: 0.117017
Trained 3401 batches 	Training Loss: 0.164465
Trained 3451 batches 	Training Loss: 0.208457
Trained 3501 batches 	Training Loss: 0.164959
Trained 3551 batches 	Training Loss: 0.185143
Trained 3601 batches 	Training Loss: 0.212892
Trained 3651 batches 	Training Loss: 0.232150
Trained 3701 batches 	Training Loss: 0.293718
Trained 3751 batches 	Training Loss: 0.136975
Trained 3801 batches 	Training Loss: 0.165782
Trained 3851 batches 	Training Loss: 0.131003
Trained 3901 batches 	Training Loss: 0.132963
Trained 3951 batches 	Training Loss: 0.147666
Trained 4001 batches 	Training Loss: 0.113404
Trained 4051 batches 	Training Loss: 0.096108
Trained 4101 batches 	Training Loss: 0.145794
Trained 4151 batches 	Training Loss: 0.107879
Trained 4201 batches 	Training Loss: 0.134906
Trained 4251 batches 	Training Loss: 0.151135
Trained 4301 batches 	Training Loss: 0.134899
Trained 4351 batches 	Training Loss: 0.100010
Trained 4401 batches 	Training Loss: 0.153778
Trained 4451 batches 	Training Loss: 0.115184
Trained 4501 batches 	Training Loss: 0.138636
Trained 4551 batches 	Training Loss: 0.109534
Trained 4601 batches 	Training Loss: 0.181471
Trained 4651 batches 	Training Loss: 0.155943
Trained 4701 batches 	Training Loss: 0.159915
Trained 4751 batches 	Training Loss: 0.132698
Trained 4801 batches 	Training Loss: 0.124037
Trained 4851 batches 	Training Loss: 0.176633
Trained 4901 batches 	Training Loss: 0.099839
Epoch: 1 	Training Loss: 0.171531
Started epoch 2
Trained 1 batches 	Training Loss: 0.155667
Trained 51 batches 	Training Loss: 0.193861
Trained 101 batches 	Training Loss: 0.114567
Trained 151 batches 	Training Loss: 0.136075
Trained 201 batches 	Training Loss: 0.183324
Trained 251 batches 	Training Loss: 0.172213
Trained 301 batches 	Training Loss: 0.189812
Trained 351 batches 	Training Loss: 0.149109
Trained 401 batches 	Training Loss: 0.193750
Trained 451 batches 	Training Loss: 0.144656
Trained 501 batches 	Training Loss: 0.209390
Trained 551 batches 	Training Loss: 0.171811
Trained 601 batches 	Training Loss: 0.209311
Trained 651 batches 	Training Loss: 0.242470
Trained 701 batches 	Training Loss: 0.118749
Trained 751 batches 	Training Loss: 0.150185
Trained 801 batches 	Training Loss: 0.137418
Trained 851 batches 	Training Loss: 0.153136
Trained 901 batches 	Training Loss: 0.137173
Trained 951 batches 	Training Loss: 0.187110
Trained 1001 batches 	Training Loss: 0.207098
Trained 1051 batches 	Training Loss: 0.235278
Trained 1101 batches 	Training Loss: 0.169815
Trained 1151 batches 	Training Loss: 0.203830
Trained 1201 batches 	Training Loss: 0.197616
Trained 1251 batches 	Training Loss: 0.144788
Trained 1301 batches 	Training Loss: 0.157531
Trained 1351 batches 	Training Loss: 0.217751
Trained 1401 batches 	Training Loss: 0.193723
Trained 1451 batches 	Training Loss: 0.241938
Trained 1501 batches 	Training Loss: 0.147702
Trained 1551 batches 	Training Loss: 0.176122
Trained 1601 batches 	Training Loss: 0.126811
Trained 1651 batches 	Training Loss: 0.091301
Trained 1701 batches 	Training Loss: 0.092467
Trained 1751 batches 	Training Loss: 0.196781
Trained 1801 batches 	Training Loss: 0.170610
Trained 1851 batches 	Training Loss: 0.138485
Trained 1901 batches 	Training Loss: 0.176728
Trained 1951 batches 	Training Loss: 0.226498
Trained 2001 batches 	Training Loss: 0.161728
Trained 2051 batches 	Training Loss: 0.259957
Trained 2101 batches 	Training Loss: 0.219843
Trained 2151 batches 	Training Loss: 0.165950
Trained 2201 batches 	Training Loss: 0.190010
Trained 2251 batches 	Training Loss: 0.161714
Trained 2301 batches 	Training Loss: 0.158277
Trained 2351 batches 	Training Loss: 0.102058
Trained 2401 batches 	Training Loss: 0.234017
Trained 2451 batches 	Training Loss: 0.077735
Trained 2501 batches 	Training Loss: 0.072192
Trained 2551 batches 	Training Loss: 0.101649
Trained 2601 batches 	Training Loss: 0.157139
Trained 2651 batches 	Training Loss: 0.173039
Trained 2701 batches 	Training Loss: 0.164372
Trained 2751 batches 	Training Loss: 0.156411
Trained 2801 batches 	Training Loss: 0.131563
Trained 2851 batches 	Training Loss: 0.160150
Trained 2901 batches 	Training Loss: 0.092691
Trained 2951 batches 	Training Loss: 0.226523
Trained 3001 batches 	Training Loss: 0.141041
Trained 3051 batches 	Training Loss: 0.169464
Trained 3101 batches 	Training Loss: 0.142389
Trained 3151 batches 	Training Loss: 0.098872
Trained 3201 batches 	Training Loss: 0.209602
Trained 3251 batches 	Training Loss: 0.200879
Trained 3301 batches 	Training Loss: 0.146898
Trained 3351 batches 	Training Loss: 0.144644
Trained 3401 batches 	Training Loss: 0.143492
Trained 3451 batches 	Training Loss: 0.170393
Trained 3501 batches 	Training Loss: 0.128979
Trained 3551 batches 	Training Loss: 0.165127
Trained 3601 batches 	Training Loss: 0.206765
Trained 3651 batches 	Training Loss: 0.191358
Trained 3701 batches 	Training Loss: 0.158589
Trained 3751 batches 	Training Loss: 0.228385
Trained 3801 batches 	Training Loss: 0.219324
Trained 3851 batches 	Training Loss: 0.094410
Trained 3901 batches 	Training Loss: 0.112539
Trained 3951 batches 	Training Loss: 0.286242
Trained 4001 batches 	Training Loss: 0.138402
Trained 4051 batches 	Training Loss: 0.228086
Trained 4101 batches 	Training Loss: 0.136163
Trained 4151 batches 	Training Loss: 0.159930
Trained 4201 batches 	Training Loss: 0.154571
Trained 4251 batches 	Training Loss: 0.190832
Trained 4301 batches 	Training Loss: 0.143066
Trained 4351 batches 	Training Loss: 0.164293
Trained 4401 batches 	Training Loss: 0.129063
Trained 4451 batches 	Training Loss: 0.099694
Trained 4501 batches 	Training Loss: 0.170825
Trained 4551 batches 	Training Loss: 0.145860
Trained 4601 batches 	Training Loss: 0.208325
Trained 4651 batches 	Training Loss: 0.175213
Trained 4701 batches 	Training Loss: 0.197321
Trained 4751 batches 	Training Loss: 0.154922
Trained 4801 batches 	Training Loss: 0.202461
Trained 4851 batches 	Training Loss: 0.115331
Trained 4901 batches 	Training Loss: 0.142261
Epoch: 2 	Training Loss: 0.162464
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 3
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.759608
Trained 51 batches 	Training Loss: 0.099575
Trained 101 batches 	Training Loss: 0.171390
Trained 151 batches 	Training Loss: 0.266545
Trained 201 batches 	Training Loss: 0.161798
Trained 251 batches 	Training Loss: 0.173090
Trained 301 batches 	Training Loss: 0.206330
Trained 351 batches 	Training Loss: 0.187675
Trained 401 batches 	Training Loss: 0.181546
Trained 451 batches 	Training Loss: 0.227786
Trained 501 batches 	Training Loss: 0.169467
Trained 551 batches 	Training Loss: 0.202103
Trained 601 batches 	Training Loss: 0.113011
Trained 651 batches 	Training Loss: 0.126712
Trained 701 batches 	Training Loss: 0.192224
Trained 751 batches 	Training Loss: 0.129309
Trained 801 batches 	Training Loss: 0.168257
Trained 851 batches 	Training Loss: 0.196454
Trained 901 batches 	Training Loss: 0.095674
Trained 951 batches 	Training Loss: 0.179057
Trained 1001 batches 	Training Loss: 0.166532
Trained 1051 batches 	Training Loss: 0.180555
Trained 1101 batches 	Training Loss: 0.157218
Trained 1151 batches 	Training Loss: 0.215831
Trained 1201 batches 	Training Loss: 0.241884
Trained 1251 batches 	Training Loss: 0.134628
Trained 1301 batches 	Training Loss: 0.188601
Trained 1351 batches 	Training Loss: 0.185735
Trained 1401 batches 	Training Loss: 0.184611
Trained 1451 batches 	Training Loss: 0.256518
Trained 1501 batches 	Training Loss: 0.199389
Trained 1551 batches 	Training Loss: 0.206289
Trained 1601 batches 	Training Loss: 0.151998
Trained 1651 batches 	Training Loss: 0.117262
Trained 1701 batches 	Training Loss: 0.120248
Trained 1751 batches 	Training Loss: 0.151118
Trained 1801 batches 	Training Loss: 0.166938
Trained 1851 batches 	Training Loss: 0.153357
Trained 1901 batches 	Training Loss: 0.149133
Trained 1951 batches 	Training Loss: 0.165100
Trained 2001 batches 	Training Loss: 0.179668
Trained 2051 batches 	Training Loss: 0.201637
Trained 2101 batches 	Training Loss: 0.139413
Trained 2151 batches 	Training Loss: 0.131291
Trained 2201 batches 	Training Loss: 0.137715
Trained 2251 batches 	Training Loss: 0.149167
Trained 2301 batches 	Training Loss: 0.165699
Trained 2351 batches 	Training Loss: 0.181208
Trained 2401 batches 	Training Loss: 0.211226
Trained 2451 batches 	Training Loss: 0.099397
Trained 2501 batches 	Training Loss: 0.165621
Trained 2551 batches 	Training Loss: 0.189343
Trained 2601 batches 	Training Loss: 0.160349
Trained 2651 batches 	Training Loss: 0.168839
Trained 2701 batches 	Training Loss: 0.154289
Trained 2751 batches 	Training Loss: 0.174281
Trained 2801 batches 	Training Loss: 0.078816
Trained 2851 batches 	Training Loss: 0.215187
Trained 2901 batches 	Training Loss: 0.142116
Trained 2951 batches 	Training Loss: 0.186160
Trained 3001 batches 	Training Loss: 0.110655
Trained 3051 batches 	Training Loss: 0.146533
Trained 3101 batches 	Training Loss: 0.154607
Trained 3151 batches 	Training Loss: 0.218028
Trained 3201 batches 	Training Loss: 0.153680
Trained 3251 batches 	Training Loss: 0.204408
Trained 3301 batches 	Training Loss: 0.150206
Trained 3351 batches 	Training Loss: 0.189822
Trained 3401 batches 	Training Loss: 0.201353
Trained 3451 batches 	Training Loss: 0.140309
Trained 3501 batches 	Training Loss: 0.157466
Trained 3551 batches 	Training Loss: 0.227323
Trained 3601 batches 	Training Loss: 0.159240
Trained 3651 batches 	Training Loss: 0.157758
Trained 3701 batches 	Training Loss: 0.147413
Trained 3751 batches 	Training Loss: 0.179005
Trained 3801 batches 	Training Loss: 0.144699
Trained 3851 batches 	Training Loss: 0.108495
Trained 3901 batches 	Training Loss: 0.165157
Trained 3951 batches 	Training Loss: 0.131476
Trained 4001 batches 	Training Loss: 0.106466
Trained 4051 batches 	Training Loss: 0.148132
Trained 4101 batches 	Training Loss: 0.171429
Trained 4151 batches 	Training Loss: 0.121639
Trained 4201 batches 	Training Loss: 0.147148
Trained 4251 batches 	Training Loss: 0.159216
Trained 4301 batches 	Training Loss: 0.143588
Trained 4351 batches 	Training Loss: 0.208493
Trained 4401 batches 	Training Loss: 0.202568
Trained 4451 batches 	Training Loss: 0.136373
Trained 4501 batches 	Training Loss: 0.280605
Trained 4551 batches 	Training Loss: 0.224172
Trained 4601 batches 	Training Loss: 0.241066
Trained 4651 batches 	Training Loss: 0.152213
Trained 4701 batches 	Training Loss: 0.177772
Trained 4751 batches 	Training Loss: 0.263947
Trained 4801 batches 	Training Loss: 0.154682
Trained 4851 batches 	Training Loss: 0.078619
Trained 4901 batches 	Training Loss: 0.159194
Epoch: 1 	Training Loss: 0.173939
Started epoch 2
Trained 1 batches 	Training Loss: 0.206199
Trained 51 batches 	Training Loss: 0.259964
Trained 101 batches 	Training Loss: 0.100025
Trained 151 batches 	Training Loss: 0.212124
Trained 201 batches 	Training Loss: 0.153438
Trained 251 batches 	Training Loss: 0.143670
Trained 301 batches 	Training Loss: 0.124715
Trained 351 batches 	Training Loss: 0.178773
Trained 401 batches 	Training Loss: 0.139985
Trained 451 batches 	Training Loss: 0.093954
Trained 501 batches 	Training Loss: 0.115233
Trained 551 batches 	Training Loss: 0.139460
Trained 601 batches 	Training Loss: 0.159165
Trained 651 batches 	Training Loss: 0.169980
Trained 701 batches 	Training Loss: 0.137409
Trained 751 batches 	Training Loss: 0.122937
Trained 801 batches 	Training Loss: 0.184161
Trained 851 batches 	Training Loss: 0.143786
Trained 901 batches 	Training Loss: 0.159228
Trained 951 batches 	Training Loss: 0.177507
Trained 1001 batches 	Training Loss: 0.095359
Trained 1051 batches 	Training Loss: 0.180520
Trained 1101 batches 	Training Loss: 0.167486
Trained 1151 batches 	Training Loss: 0.228373
Trained 1201 batches 	Training Loss: 0.163603
Trained 1251 batches 	Training Loss: 0.100812
Trained 1301 batches 	Training Loss: 0.226075
Trained 1351 batches 	Training Loss: 0.217642
Trained 1401 batches 	Training Loss: 0.121756
Trained 1451 batches 	Training Loss: 0.151469
Trained 1501 batches 	Training Loss: 0.245148
Trained 1551 batches 	Training Loss: 0.151323
Trained 1601 batches 	Training Loss: 0.196231
Trained 1651 batches 	Training Loss: 0.238924
Trained 1701 batches 	Training Loss: 0.170941
Trained 1751 batches 	Training Loss: 0.194893
Trained 1801 batches 	Training Loss: 0.226129
Trained 1851 batches 	Training Loss: 0.203408
Trained 1901 batches 	Training Loss: 0.142063
Trained 1951 batches 	Training Loss: 0.199264
Trained 2001 batches 	Training Loss: 0.119013
Trained 2051 batches 	Training Loss: 0.142054
Trained 2101 batches 	Training Loss: 0.154617
Trained 2151 batches 	Training Loss: 0.211354
Trained 2201 batches 	Training Loss: 0.145759
Trained 2251 batches 	Training Loss: 0.156270
Trained 2301 batches 	Training Loss: 0.226391
Trained 2351 batches 	Training Loss: 0.262144
Trained 2401 batches 	Training Loss: 0.225790
Trained 2451 batches 	Training Loss: 0.151269
Trained 2501 batches 	Training Loss: 0.188889
Trained 2551 batches 	Training Loss: 0.185683
Trained 2601 batches 	Training Loss: 0.200438
Trained 2651 batches 	Training Loss: 0.113314
Trained 2701 batches 	Training Loss: 0.193237
Trained 2751 batches 	Training Loss: 0.200331
Trained 2801 batches 	Training Loss: 0.165211
Trained 2851 batches 	Training Loss: 0.094142
Trained 2901 batches 	Training Loss: 0.200916
Trained 2951 batches 	Training Loss: 0.136468
Trained 3001 batches 	Training Loss: 0.230189
Trained 3051 batches 	Training Loss: 0.205911
Trained 3101 batches 	Training Loss: 0.123671
Trained 3151 batches 	Training Loss: 0.183474
Trained 3201 batches 	Training Loss: 0.179125
Trained 3251 batches 	Training Loss: 0.128863
Trained 3301 batches 	Training Loss: 0.224369
Trained 3351 batches 	Training Loss: 0.141115
Trained 3401 batches 	Training Loss: 0.122257
Trained 3451 batches 	Training Loss: 0.135589
Trained 3501 batches 	Training Loss: 0.182426
Trained 3551 batches 	Training Loss: 0.220411
Trained 3601 batches 	Training Loss: 0.223489
Trained 3651 batches 	Training Loss: 0.131046
Trained 3701 batches 	Training Loss: 0.098879
Trained 3751 batches 	Training Loss: 0.138768
Trained 3801 batches 	Training Loss: 0.109511
Trained 3851 batches 	Training Loss: 0.135566
Trained 3901 batches 	Training Loss: 0.184774
Trained 3951 batches 	Training Loss: 0.179028
Trained 4001 batches 	Training Loss: 0.166972
Trained 4051 batches 	Training Loss: 0.136297
Trained 4101 batches 	Training Loss: 0.148311
Trained 4151 batches 	Training Loss: 0.165020
Trained 4201 batches 	Training Loss: 0.144925
Trained 4251 batches 	Training Loss: 0.089284
Trained 4301 batches 	Training Loss: 0.266395
Trained 4351 batches 	Training Loss: 0.154347
Trained 4401 batches 	Training Loss: 0.246110
Trained 4451 batches 	Training Loss: 0.126060
Trained 4501 batches 	Training Loss: 0.134661
Trained 4551 batches 	Training Loss: 0.166314
Trained 4601 batches 	Training Loss: 0.190294
Trained 4651 batches 	Training Loss: 0.241694
Trained 4701 batches 	Training Loss: 0.204157
Trained 4751 batches 	Training Loss: 0.146861
Trained 4801 batches 	Training Loss: 0.113951
Trained 4851 batches 	Training Loss: 0.224564
Trained 4901 batches 	Training Loss: 0.139251
Epoch: 2 	Training Loss: 0.164682
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 1
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.674978
Trained 51 batches 	Training Loss: 0.195250
Trained 101 batches 	Training Loss: 0.172576
Trained 151 batches 	Training Loss: 0.219827
Trained 201 batches 	Training Loss: 0.167349
Trained 251 batches 	Training Loss: 0.162164
Trained 301 batches 	Training Loss: 0.245263
Trained 351 batches 	Training Loss: 0.173444
Trained 401 batches 	Training Loss: 0.266765
Trained 451 batches 	Training Loss: 0.216626
Trained 501 batches 	Training Loss: 0.148072
Trained 551 batches 	Training Loss: 0.249459
Trained 601 batches 	Training Loss: 0.223829
Trained 651 batches 	Training Loss: 0.185431
Trained 701 batches 	Training Loss: 0.143469
Trained 751 batches 	Training Loss: 0.209250
Trained 801 batches 	Training Loss: 0.119277
Trained 851 batches 	Training Loss: 0.257561
Trained 901 batches 	Training Loss: 0.133625
Trained 951 batches 	Training Loss: 0.205492
Trained 1001 batches 	Training Loss: 0.204025
Trained 1051 batches 	Training Loss: 0.164877
Trained 1101 batches 	Training Loss: 0.265588
Trained 1151 batches 	Training Loss: 0.167002
Trained 1201 batches 	Training Loss: 0.184744
Trained 1251 batches 	Training Loss: 0.183255
Trained 1301 batches 	Training Loss: 0.102201
Trained 1351 batches 	Training Loss: 0.237641
Trained 1401 batches 	Training Loss: 0.145035
Trained 1451 batches 	Training Loss: 0.173497
Trained 1501 batches 	Training Loss: 0.123492
Trained 1551 batches 	Training Loss: 0.159726
Trained 1601 batches 	Training Loss: 0.112262
Trained 1651 batches 	Training Loss: 0.161781
Trained 1701 batches 	Training Loss: 0.182297
Trained 1751 batches 	Training Loss: 0.184637
Trained 1801 batches 	Training Loss: 0.163321
Trained 1851 batches 	Training Loss: 0.142212
Trained 1901 batches 	Training Loss: 0.267510
Trained 1951 batches 	Training Loss: 0.379466
Trained 2001 batches 	Training Loss: 0.201390
Trained 2051 batches 	Training Loss: 0.251283
Trained 2101 batches 	Training Loss: 0.281281
Trained 2151 batches 	Training Loss: 0.105447
Trained 2201 batches 	Training Loss: 0.172456
Trained 2251 batches 	Training Loss: 0.179159
Trained 2301 batches 	Training Loss: 0.229015
Trained 2351 batches 	Training Loss: 0.140314
Trained 2401 batches 	Training Loss: 0.270150
Trained 2451 batches 	Training Loss: 0.201945
Trained 2501 batches 	Training Loss: 0.202532
Trained 2551 batches 	Training Loss: 0.228678
Trained 2601 batches 	Training Loss: 0.148377
Trained 2651 batches 	Training Loss: 0.119431
Trained 2701 batches 	Training Loss: 0.112037
Trained 2751 batches 	Training Loss: 0.234499
Trained 2801 batches 	Training Loss: 0.116155
Trained 2851 batches 	Training Loss: 0.188738
Trained 2901 batches 	Training Loss: 0.176763
Trained 2951 batches 	Training Loss: 0.167170
Trained 3001 batches 	Training Loss: 0.142159
Trained 3051 batches 	Training Loss: 0.189523
Trained 3101 batches 	Training Loss: 0.153028
Trained 3151 batches 	Training Loss: 0.161394
Trained 3201 batches 	Training Loss: 0.170350
Trained 3251 batches 	Training Loss: 0.135287
Trained 3301 batches 	Training Loss: 0.172959
Trained 3351 batches 	Training Loss: 0.288926
Trained 3401 batches 	Training Loss: 0.204250
Trained 3451 batches 	Training Loss: 0.228468
Trained 3501 batches 	Training Loss: 0.199027
Trained 3551 batches 	Training Loss: 0.221414
Trained 3601 batches 	Training Loss: 0.169583
Trained 3651 batches 	Training Loss: 0.198376
Trained 3701 batches 	Training Loss: 0.108022
Trained 3751 batches 	Training Loss: 0.282211
Trained 3801 batches 	Training Loss: 0.227531
Trained 3851 batches 	Training Loss: 0.248462
Trained 3901 batches 	Training Loss: 0.177420
Trained 3951 batches 	Training Loss: 0.197836
Trained 4001 batches 	Training Loss: 0.132939
Trained 4051 batches 	Training Loss: 0.189373
Trained 4101 batches 	Training Loss: 0.160975
Trained 4151 batches 	Training Loss: 0.202450
Trained 4201 batches 	Training Loss: 0.140538
Trained 4251 batches 	Training Loss: 0.216515
Trained 4301 batches 	Training Loss: 0.184130
Trained 4351 batches 	Training Loss: 0.195471
Trained 4401 batches 	Training Loss: 0.224078
Trained 4451 batches 	Training Loss: 0.251535
Trained 4501 batches 	Training Loss: 0.216138
Trained 4551 batches 	Training Loss: 0.191299
Trained 4601 batches 	Training Loss: 0.138176
Trained 4651 batches 	Training Loss: 0.113332
Trained 4701 batches 	Training Loss: 0.152039
Trained 4751 batches 	Training Loss: 0.165935
Trained 4801 batches 	Training Loss: 0.153247
Trained 4851 batches 	Training Loss: 0.129085
Trained 4901 batches 	Training Loss: 0.122533
Epoch: 1 	Training Loss: 0.178333
Training time lapse: 11.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.658
The AUROC of Atelectasis is 0.7097655751113254
The AUROC of Cardiomegaly is 0.6725896891402616
The AUROC of Effusion is 0.787117280303881
The AUROC of Infiltration is 0.6510311313373092
The AUROC of Mass is 0.5215974542887238
The AUROC of Nodule is 0.5689163986472626
The AUROC of Pneumonia is 0.6420116084109304
The AUROC of Pneumothorax is 0.717545382905226
The AUROC of Consolidation is 0.7465318067718293
The AUROC of Edema is 0.8217734429338465
The AUROC of Emphysema is 0.667232576076247
The AUROC of Fibrosis is 0.6198790792662864
The AUROC of Pleural_Thickening is 0.6467390069387148
The AUROC of Hernia is 0.43277965458618195
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.801274
Trained 51 batches 	Training Loss: 0.202498
Trained 101 batches 	Training Loss: 0.196039
Trained 151 batches 	Training Loss: 0.168566
Trained 201 batches 	Training Loss: 0.218139
Trained 251 batches 	Training Loss: 0.141938
Trained 301 batches 	Training Loss: 0.170728
Trained 351 batches 	Training Loss: 0.146705
Trained 401 batches 	Training Loss: 0.200264
Trained 451 batches 	Training Loss: 0.151426
Trained 501 batches 	Training Loss: 0.140192
Trained 551 batches 	Training Loss: 0.206733
Trained 601 batches 	Training Loss: 0.138875
Trained 651 batches 	Training Loss: 0.211335
Trained 701 batches 	Training Loss: 0.251530
Trained 751 batches 	Training Loss: 0.195331
Trained 801 batches 	Training Loss: 0.238457
Trained 851 batches 	Training Loss: 0.169440
Trained 901 batches 	Training Loss: 0.207063
Trained 951 batches 	Training Loss: 0.199041
Trained 1001 batches 	Training Loss: 0.188789
Trained 1051 batches 	Training Loss: 0.297691
Trained 1101 batches 	Training Loss: 0.207449
Trained 1151 batches 	Training Loss: 0.231246
Trained 1201 batches 	Training Loss: 0.219833
Trained 1251 batches 	Training Loss: 0.157716
Trained 1301 batches 	Training Loss: 0.110637
Trained 1351 batches 	Training Loss: 0.207791
Trained 1401 batches 	Training Loss: 0.184797
Trained 1451 batches 	Training Loss: 0.138314
Trained 1501 batches 	Training Loss: 0.182519
Trained 1551 batches 	Training Loss: 0.239875
Trained 1601 batches 	Training Loss: 0.125939
Trained 1651 batches 	Training Loss: 0.186077
Trained 1701 batches 	Training Loss: 0.148451
Trained 1751 batches 	Training Loss: 0.145207
Trained 1801 batches 	Training Loss: 0.175622
Trained 1851 batches 	Training Loss: 0.123233
Trained 1901 batches 	Training Loss: 0.146474
Trained 1951 batches 	Training Loss: 0.214523
Trained 2001 batches 	Training Loss: 0.149128
Trained 2051 batches 	Training Loss: 0.156575
Trained 2101 batches 	Training Loss: 0.154826
Trained 2151 batches 	Training Loss: 0.270015
Trained 2201 batches 	Training Loss: 0.194305
Trained 2251 batches 	Training Loss: 0.182174
Trained 2301 batches 	Training Loss: 0.272395
Trained 2351 batches 	Training Loss: 0.169465
Trained 2401 batches 	Training Loss: 0.224364
Trained 2451 batches 	Training Loss: 0.125502
Trained 2501 batches 	Training Loss: 0.127588
Trained 2551 batches 	Training Loss: 0.269294
Trained 2601 batches 	Training Loss: 0.123728
Trained 2651 batches 	Training Loss: 0.183635
Trained 2701 batches 	Training Loss: 0.183347
Trained 2751 batches 	Training Loss: 0.140190
Trained 2801 batches 	Training Loss: 0.236993
Trained 2851 batches 	Training Loss: 0.130326
Trained 2901 batches 	Training Loss: 0.266568
Trained 2951 batches 	Training Loss: 0.130505
Trained 3001 batches 	Training Loss: 0.134668
Trained 3051 batches 	Training Loss: 0.166529
Trained 3101 batches 	Training Loss: 0.178829
Trained 3151 batches 	Training Loss: 0.150194
Trained 3201 batches 	Training Loss: 0.209177
Trained 3251 batches 	Training Loss: 0.168036
Trained 3301 batches 	Training Loss: 0.111059
Trained 3351 batches 	Training Loss: 0.128751
Trained 3401 batches 	Training Loss: 0.209956
Trained 3451 batches 	Training Loss: 0.165418
Trained 3501 batches 	Training Loss: 0.200574
Trained 3551 batches 	Training Loss: 0.162695
Trained 3601 batches 	Training Loss: 0.215190
Trained 3651 batches 	Training Loss: 0.215144
Trained 3701 batches 	Training Loss: 0.142915
Trained 3751 batches 	Training Loss: 0.292437
Trained 3801 batches 	Training Loss: 0.186895
Trained 3851 batches 	Training Loss: 0.223405
Trained 3901 batches 	Training Loss: 0.181180
Trained 3951 batches 	Training Loss: 0.184786
Trained 4001 batches 	Training Loss: 0.183382
Trained 4051 batches 	Training Loss: 0.171233
Trained 4101 batches 	Training Loss: 0.182336
Trained 4151 batches 	Training Loss: 0.154404
Trained 4201 batches 	Training Loss: 0.151920
Trained 4251 batches 	Training Loss: 0.162835
Trained 4301 batches 	Training Loss: 0.136471
Trained 4351 batches 	Training Loss: 0.238991
Trained 4401 batches 	Training Loss: 0.186423
Trained 4451 batches 	Training Loss: 0.223775
Trained 4501 batches 	Training Loss: 0.097129
Trained 4551 batches 	Training Loss: 0.286270
Trained 4601 batches 	Training Loss: 0.208908
Trained 4651 batches 	Training Loss: 0.196529
Trained 4701 batches 	Training Loss: 0.105426
Trained 4751 batches 	Training Loss: 0.150870
Trained 4801 batches 	Training Loss: 0.225426
Trained 4851 batches 	Training Loss: 0.163657
Trained 4901 batches 	Training Loss: 0.158436
Epoch: 1 	Training Loss: 0.174751
Started epoch 2
Trained 1 batches 	Training Loss: 0.237029
Trained 51 batches 	Training Loss: 0.319126
Trained 101 batches 	Training Loss: 0.191120
Trained 151 batches 	Training Loss: 0.209557
Trained 201 batches 	Training Loss: 0.143325
Trained 251 batches 	Training Loss: 0.196109
Trained 301 batches 	Training Loss: 0.134725
Trained 351 batches 	Training Loss: 0.144423
Trained 401 batches 	Training Loss: 0.226645
Trained 451 batches 	Training Loss: 0.157573
Trained 501 batches 	Training Loss: 0.204348
Trained 551 batches 	Training Loss: 0.224881
Trained 601 batches 	Training Loss: 0.094190
Trained 651 batches 	Training Loss: 0.190968
Trained 701 batches 	Training Loss: 0.141432
Trained 751 batches 	Training Loss: 0.140538
Trained 801 batches 	Training Loss: 0.137205
Trained 851 batches 	Training Loss: 0.148045
Trained 901 batches 	Training Loss: 0.186426
Trained 951 batches 	Training Loss: 0.193428
Trained 1001 batches 	Training Loss: 0.148200
Trained 1051 batches 	Training Loss: 0.101579
Trained 1101 batches 	Training Loss: 0.132822
Trained 1151 batches 	Training Loss: 0.165592
Trained 1201 batches 	Training Loss: 0.129467
Trained 1251 batches 	Training Loss: 0.095838
Trained 1301 batches 	Training Loss: 0.181670
Trained 1351 batches 	Training Loss: 0.140190
Trained 1401 batches 	Training Loss: 0.102741
Trained 1451 batches 	Training Loss: 0.230156
Trained 1501 batches 	Training Loss: 0.127808
Trained 1551 batches 	Training Loss: 0.180613
Trained 1601 batches 	Training Loss: 0.234403
Trained 1651 batches 	Training Loss: 0.213449
Trained 1701 batches 	Training Loss: 0.172322
Trained 1751 batches 	Training Loss: 0.151332
Trained 1801 batches 	Training Loss: 0.204454
Trained 1851 batches 	Training Loss: 0.215715
Trained 1901 batches 	Training Loss: 0.183190
Trained 1951 batches 	Training Loss: 0.137702
Trained 2001 batches 	Training Loss: 0.150066
Trained 2051 batches 	Training Loss: 0.268670
Trained 2101 batches 	Training Loss: 0.250228
Trained 2151 batches 	Training Loss: 0.125318
Trained 2201 batches 	Training Loss: 0.166242
Trained 2251 batches 	Training Loss: 0.177929
Trained 2301 batches 	Training Loss: 0.106196
Trained 2351 batches 	Training Loss: 0.185295
Trained 2401 batches 	Training Loss: 0.214382
Trained 2451 batches 	Training Loss: 0.193545
Trained 2501 batches 	Training Loss: 0.141893
Trained 2551 batches 	Training Loss: 0.151516
Trained 2601 batches 	Training Loss: 0.122531
Trained 2651 batches 	Training Loss: 0.087979
Trained 2701 batches 	Training Loss: 0.202068
Trained 2751 batches 	Training Loss: 0.151884
Trained 2801 batches 	Training Loss: 0.099978
Trained 2851 batches 	Training Loss: 0.087459
Trained 2901 batches 	Training Loss: 0.112523
Trained 2951 batches 	Training Loss: 0.183359
Trained 3001 batches 	Training Loss: 0.217891
Trained 3051 batches 	Training Loss: 0.164673
Trained 3101 batches 	Training Loss: 0.119568
Trained 3151 batches 	Training Loss: 0.107911
Trained 3201 batches 	Training Loss: 0.205649
Trained 3251 batches 	Training Loss: 0.125382
Trained 3301 batches 	Training Loss: 0.155070
Trained 3351 batches 	Training Loss: 0.128194
Trained 3401 batches 	Training Loss: 0.277963
Trained 3451 batches 	Training Loss: 0.147621
Trained 3501 batches 	Training Loss: 0.137429
Trained 3551 batches 	Training Loss: 0.134816
Trained 3601 batches 	Training Loss: 0.134734
Trained 3651 batches 	Training Loss: 0.174237
Trained 3701 batches 	Training Loss: 0.100478
Trained 3751 batches 	Training Loss: 0.221398
Trained 3801 batches 	Training Loss: 0.114957
Trained 3851 batches 	Training Loss: 0.138338
Trained 3901 batches 	Training Loss: 0.164154
Trained 3951 batches 	Training Loss: 0.153969
Trained 4001 batches 	Training Loss: 0.211643
Trained 4051 batches 	Training Loss: 0.125419
Trained 4101 batches 	Training Loss: 0.143135
Trained 4151 batches 	Training Loss: 0.084339
Trained 4201 batches 	Training Loss: 0.122251
Trained 4251 batches 	Training Loss: 0.127368
Trained 4301 batches 	Training Loss: 0.177269
Trained 4351 batches 	Training Loss: 0.153480
Trained 4401 batches 	Training Loss: 0.105606
Trained 4451 batches 	Training Loss: 0.152162
Trained 4501 batches 	Training Loss: 0.157204
Trained 4551 batches 	Training Loss: 0.141913
Trained 4601 batches 	Training Loss: 0.180204
Trained 4651 batches 	Training Loss: 0.194829
Trained 4701 batches 	Training Loss: 0.183294
Trained 4751 batches 	Training Loss: 0.124354
Trained 4801 batches 	Training Loss: 0.182152
Trained 4851 batches 	Training Loss: 0.129388
Trained 4901 batches 	Training Loss: 0.132440
Epoch: 2 	Training Loss: 0.165285
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.846609
Trained 51 batches 	Training Loss: 0.179898
Trained 101 batches 	Training Loss: 0.188100
Trained 151 batches 	Training Loss: 0.113660
Trained 201 batches 	Training Loss: 0.174258
Trained 251 batches 	Training Loss: 0.168997
Trained 301 batches 	Training Loss: 0.154365
Trained 351 batches 	Training Loss: 0.247420
Trained 401 batches 	Training Loss: 0.254712
Trained 451 batches 	Training Loss: 0.188048
Trained 501 batches 	Training Loss: 0.202475
Trained 551 batches 	Training Loss: 0.135529
Trained 601 batches 	Training Loss: 0.143406
Trained 651 batches 	Training Loss: 0.119195
Trained 701 batches 	Training Loss: 0.162824
Trained 751 batches 	Training Loss: 0.103246
Trained 801 batches 	Training Loss: 0.175802
Trained 851 batches 	Training Loss: 0.131746
Trained 901 batches 	Training Loss: 0.136575
Trained 951 batches 	Training Loss: 0.168967
Trained 1001 batches 	Training Loss: 0.253072
Trained 1051 batches 	Training Loss: 0.187312
Trained 1101 batches 	Training Loss: 0.207848
Trained 1151 batches 	Training Loss: 0.234569
Trained 1201 batches 	Training Loss: 0.137711
Trained 1251 batches 	Training Loss: 0.126119
Trained 1301 batches 	Training Loss: 0.206277
Trained 1351 batches 	Training Loss: 0.184881
Trained 1401 batches 	Training Loss: 0.191076
Trained 1451 batches 	Training Loss: 0.168110
Trained 1501 batches 	Training Loss: 0.176431
Trained 1551 batches 	Training Loss: 0.117466
Trained 1601 batches 	Training Loss: 0.139805
Trained 1651 batches 	Training Loss: 0.108749
Trained 1701 batches 	Training Loss: 0.198528
Trained 1751 batches 	Training Loss: 0.202518
Trained 1801 batches 	Training Loss: 0.227111
Trained 1851 batches 	Training Loss: 0.201040
Trained 1901 batches 	Training Loss: 0.183780
Trained 1951 batches 	Training Loss: 0.229190
Trained 2001 batches 	Training Loss: 0.145360
Trained 2051 batches 	Training Loss: 0.189091
Trained 2101 batches 	Training Loss: 0.210863
Trained 2151 batches 	Training Loss: 0.173439
Trained 2201 batches 	Training Loss: 0.207378
Trained 2251 batches 	Training Loss: 0.150896
Trained 2301 batches 	Training Loss: 0.132129
Trained 2351 batches 	Training Loss: 0.238805
Trained 2401 batches 	Training Loss: 0.135572
Trained 2451 batches 	Training Loss: 0.174900
Trained 2501 batches 	Training Loss: 0.165936
Trained 2551 batches 	Training Loss: 0.167491
Trained 2601 batches 	Training Loss: 0.140923
Trained 2651 batches 	Training Loss: 0.164318
Trained 2701 batches 	Training Loss: 0.186564
Trained 2751 batches 	Training Loss: 0.177693
Trained 2801 batches 	Training Loss: 0.190146
Trained 2851 batches 	Training Loss: 0.135316
Trained 2901 batches 	Training Loss: 0.120408
Trained 2951 batches 	Training Loss: 0.150858
Trained 3001 batches 	Training Loss: 0.135797
Trained 3051 batches 	Training Loss: 0.174141
Trained 3101 batches 	Training Loss: 0.115766
Trained 3151 batches 	Training Loss: 0.182545
Trained 3201 batches 	Training Loss: 0.166897
Trained 3251 batches 	Training Loss: 0.217043
Trained 3301 batches 	Training Loss: 0.145710
Trained 3351 batches 	Training Loss: 0.084848
Trained 3401 batches 	Training Loss: 0.160079
Trained 3451 batches 	Training Loss: 0.124591
Trained 3501 batches 	Training Loss: 0.241825
Trained 3551 batches 	Training Loss: 0.121760
Trained 3601 batches 	Training Loss: 0.196340
Trained 3651 batches 	Training Loss: 0.204820
Trained 3701 batches 	Training Loss: 0.184736
Trained 3751 batches 	Training Loss: 0.190350
Trained 3801 batches 	Training Loss: 0.133703
Trained 3851 batches 	Training Loss: 0.172213
Trained 3901 batches 	Training Loss: 0.092845
Trained 3951 batches 	Training Loss: 0.202052
Trained 4001 batches 	Training Loss: 0.165203
Trained 4051 batches 	Training Loss: 0.157492
Trained 4101 batches 	Training Loss: 0.179889
Trained 4151 batches 	Training Loss: 0.119047
Trained 4201 batches 	Training Loss: 0.120039
Trained 4251 batches 	Training Loss: 0.138622
Trained 4301 batches 	Training Loss: 0.163889
Trained 4351 batches 	Training Loss: 0.163062
Trained 4401 batches 	Training Loss: 0.176561
Trained 4451 batches 	Training Loss: 0.173677
Trained 4501 batches 	Training Loss: 0.202423
Trained 4551 batches 	Training Loss: 0.210609
Trained 4601 batches 	Training Loss: 0.175128
Trained 4651 batches 	Training Loss: 0.170865
Trained 4701 batches 	Training Loss: 0.132804
Trained 4751 batches 	Training Loss: 0.205643
Trained 4801 batches 	Training Loss: 0.181020
Trained 4851 batches 	Training Loss: 0.143120
Trained 4901 batches 	Training Loss: 0.115703
Epoch: 1 	Training Loss: 0.172736
Started epoch 2
Trained 1 batches 	Training Loss: 0.151895
Trained 51 batches 	Training Loss: 0.169770
Trained 101 batches 	Training Loss: 0.149228
Trained 151 batches 	Training Loss: 0.207936
Trained 201 batches 	Training Loss: 0.223317
Trained 251 batches 	Training Loss: 0.154003
Trained 301 batches 	Training Loss: 0.103196
Trained 351 batches 	Training Loss: 0.245118
Trained 401 batches 	Training Loss: 0.117316
Trained 451 batches 	Training Loss: 0.108588
Trained 501 batches 	Training Loss: 0.147275
Trained 551 batches 	Training Loss: 0.176503
Trained 601 batches 	Training Loss: 0.150878
Trained 651 batches 	Training Loss: 0.160974
Trained 701 batches 	Training Loss: 0.214082
Trained 751 batches 	Training Loss: 0.275836
Trained 801 batches 	Training Loss: 0.176371
Trained 851 batches 	Training Loss: 0.136893
Trained 901 batches 	Training Loss: 0.108014
Trained 951 batches 	Training Loss: 0.169124
Trained 1001 batches 	Training Loss: 0.170598
Trained 1051 batches 	Training Loss: 0.161136
Trained 1101 batches 	Training Loss: 0.232937
Trained 1151 batches 	Training Loss: 0.215631
Trained 1201 batches 	Training Loss: 0.169106
Trained 1251 batches 	Training Loss: 0.235137
Trained 1301 batches 	Training Loss: 0.196185
Trained 1351 batches 	Training Loss: 0.132448
Trained 1401 batches 	Training Loss: 0.143981
Trained 1451 batches 	Training Loss: 0.308742
Trained 1501 batches 	Training Loss: 0.085551
Trained 1551 batches 	Training Loss: 0.120968
Trained 1601 batches 	Training Loss: 0.101604
Trained 1651 batches 	Training Loss: 0.143739
Trained 1701 batches 	Training Loss: 0.247883
Trained 1751 batches 	Training Loss: 0.087185
Trained 1801 batches 	Training Loss: 0.127568
Trained 1851 batches 	Training Loss: 0.250330
Trained 1901 batches 	Training Loss: 0.176024
Trained 1951 batches 	Training Loss: 0.184461
Trained 2001 batches 	Training Loss: 0.161243
Trained 2051 batches 	Training Loss: 0.131223
Trained 2101 batches 	Training Loss: 0.139527
Trained 2151 batches 	Training Loss: 0.108887
Trained 2201 batches 	Training Loss: 0.184125
Trained 2251 batches 	Training Loss: 0.163522
Trained 2301 batches 	Training Loss: 0.161102
Trained 2351 batches 	Training Loss: 0.114369
Trained 2401 batches 	Training Loss: 0.154043
Trained 2451 batches 	Training Loss: 0.132330
Trained 2501 batches 	Training Loss: 0.249430
Trained 2551 batches 	Training Loss: 0.197304
Trained 2601 batches 	Training Loss: 0.121887
Trained 2651 batches 	Training Loss: 0.155522
Trained 2701 batches 	Training Loss: 0.154289
Trained 2751 batches 	Training Loss: 0.126404
Trained 2801 batches 	Training Loss: 0.177620
Trained 2851 batches 	Training Loss: 0.098105
Trained 2901 batches 	Training Loss: 0.114586
Trained 2951 batches 	Training Loss: 0.132435
Trained 3001 batches 	Training Loss: 0.154399
Trained 3051 batches 	Training Loss: 0.134267
Trained 3101 batches 	Training Loss: 0.136483
Trained 3151 batches 	Training Loss: 0.179207
Trained 3201 batches 	Training Loss: 0.183183
Trained 3251 batches 	Training Loss: 0.127701
Trained 3301 batches 	Training Loss: 0.226059
Trained 3351 batches 	Training Loss: 0.152649
Trained 3401 batches 	Training Loss: 0.197035
Trained 3451 batches 	Training Loss: 0.143718
Trained 3501 batches 	Training Loss: 0.247678
Trained 3551 batches 	Training Loss: 0.155653
Trained 3601 batches 	Training Loss: 0.172981
Trained 3651 batches 	Training Loss: 0.165302
Trained 3701 batches 	Training Loss: 0.178241
Trained 3751 batches 	Training Loss: 0.240371
Trained 3801 batches 	Training Loss: 0.185467
Trained 3851 batches 	Training Loss: 0.118816
Trained 3901 batches 	Training Loss: 0.162488
Trained 3951 batches 	Training Loss: 0.128485
Trained 4001 batches 	Training Loss: 0.155832
Trained 4051 batches 	Training Loss: 0.164878
Trained 4101 batches 	Training Loss: 0.185149
Trained 4151 batches 	Training Loss: 0.131593
Trained 4201 batches 	Training Loss: 0.186431
Trained 4251 batches 	Training Loss: 0.234000
Trained 4301 batches 	Training Loss: 0.186841
Trained 4351 batches 	Training Loss: 0.218031
Trained 4401 batches 	Training Loss: 0.184986
Trained 4451 batches 	Training Loss: 0.264590
Trained 4501 batches 	Training Loss: 0.119123
Trained 4551 batches 	Training Loss: 0.135743
Trained 4601 batches 	Training Loss: 0.214933
Trained 4651 batches 	Training Loss: 0.170358
Trained 4701 batches 	Training Loss: 0.115707
Trained 4751 batches 	Training Loss: 0.145910
Trained 4801 batches 	Training Loss: 0.115512
Trained 4851 batches 	Training Loss: 0.109564
Trained 4901 batches 	Training Loss: 0.117553
Epoch: 2 	Training Loss: 0.163400
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.669646
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.714918
Trained 51 batches 	Training Loss: 0.300236
Trained 101 batches 	Training Loss: 0.138705
Trained 151 batches 	Training Loss: 0.197036
Trained 201 batches 	Training Loss: 0.153190
Trained 251 batches 	Training Loss: 0.081404
Trained 301 batches 	Training Loss: 0.227844
Trained 351 batches 	Training Loss: 0.199770
Trained 401 batches 	Training Loss: 0.144563
Trained 451 batches 	Training Loss: 0.198345
Trained 501 batches 	Training Loss: 0.190424
Trained 551 batches 	Training Loss: 0.184573
Trained 601 batches 	Training Loss: 0.145133
Trained 651 batches 	Training Loss: 0.122692
Trained 701 batches 	Training Loss: 0.166015
Trained 751 batches 	Training Loss: 0.178716
Trained 801 batches 	Training Loss: 0.146052
Trained 851 batches 	Training Loss: 0.229859
Trained 901 batches 	Training Loss: 0.150568
Trained 951 batches 	Training Loss: 0.112166
Trained 1001 batches 	Training Loss: 0.159213
Trained 1051 batches 	Training Loss: 0.220977
Trained 1101 batches 	Training Loss: 0.122199
Trained 1151 batches 	Training Loss: 0.148072
Trained 1201 batches 	Training Loss: 0.148062
Trained 1251 batches 	Training Loss: 0.162641
Trained 1301 batches 	Training Loss: 0.171188
Trained 1351 batches 	Training Loss: 0.199101
Trained 1401 batches 	Training Loss: 0.216855
Trained 1451 batches 	Training Loss: 0.163042
Trained 1501 batches 	Training Loss: 0.228799
Trained 1551 batches 	Training Loss: 0.195055
Trained 1601 batches 	Training Loss: 0.185491
Trained 1651 batches 	Training Loss: 0.202761
Trained 1701 batches 	Training Loss: 0.205701
Trained 1751 batches 	Training Loss: 0.159798
Trained 1801 batches 	Training Loss: 0.148874
Trained 1851 batches 	Training Loss: 0.159180
Trained 1901 batches 	Training Loss: 0.192634
Trained 1951 batches 	Training Loss: 0.256993
Trained 2001 batches 	Training Loss: 0.193418
Trained 2051 batches 	Training Loss: 0.123305
Trained 2101 batches 	Training Loss: 0.153999
Trained 2151 batches 	Training Loss: 0.150808
Trained 2201 batches 	Training Loss: 0.273181
Trained 2251 batches 	Training Loss: 0.104451
Trained 2301 batches 	Training Loss: 0.165202
Trained 2351 batches 	Training Loss: 0.192194
Trained 2401 batches 	Training Loss: 0.273002
Trained 2451 batches 	Training Loss: 0.212635
Trained 2501 batches 	Training Loss: 0.248726
Trained 2551 batches 	Training Loss: 0.225267
Trained 2601 batches 	Training Loss: 0.195496
Trained 2651 batches 	Training Loss: 0.150800
Trained 2701 batches 	Training Loss: 0.220328
Trained 2751 batches 	Training Loss: 0.226353
Trained 2801 batches 	Training Loss: 0.147147
Trained 2851 batches 	Training Loss: 0.200896
Trained 2901 batches 	Training Loss: 0.132438
Trained 2951 batches 	Training Loss: 0.205699
Trained 3001 batches 	Training Loss: 0.189969
Trained 3051 batches 	Training Loss: 0.220121
Trained 3101 batches 	Training Loss: 0.133271
Trained 3151 batches 	Training Loss: 0.166069
Trained 3201 batches 	Training Loss: 0.139019
Trained 3251 batches 	Training Loss: 0.213150
Trained 3301 batches 	Training Loss: 0.209280
Trained 3351 batches 	Training Loss: 0.172055
Trained 3401 batches 	Training Loss: 0.187907
Trained 3451 batches 	Training Loss: 0.147603
Trained 3501 batches 	Training Loss: 0.106749
Trained 3551 batches 	Training Loss: 0.210988
Trained 3601 batches 	Training Loss: 0.170865
Trained 3651 batches 	Training Loss: 0.148964
Trained 3701 batches 	Training Loss: 0.145135
Trained 3751 batches 	Training Loss: 0.215049
Trained 3801 batches 	Training Loss: 0.198807
Trained 3851 batches 	Training Loss: 0.245140
Trained 3901 batches 	Training Loss: 0.224282
Trained 3951 batches 	Training Loss: 0.156734
Trained 4001 batches 	Training Loss: 0.174720
Trained 4051 batches 	Training Loss: 0.262613
Trained 4101 batches 	Training Loss: 0.112579
Trained 4151 batches 	Training Loss: 0.243942
Trained 4201 batches 	Training Loss: 0.245630
Trained 4251 batches 	Training Loss: 0.131485
Trained 4301 batches 	Training Loss: 0.152043
Trained 4351 batches 	Training Loss: 0.095428
Trained 4401 batches 	Training Loss: 0.144891
Trained 4451 batches 	Training Loss: 0.173918
Trained 4501 batches 	Training Loss: 0.115329
Trained 4551 batches 	Training Loss: 0.167715
Trained 4601 batches 	Training Loss: 0.127695
Trained 4651 batches 	Training Loss: 0.183563
Trained 4701 batches 	Training Loss: 0.216204
Trained 4751 batches 	Training Loss: 0.128199
Trained 4801 batches 	Training Loss: 0.122948
Trained 4851 batches 	Training Loss: 0.166903
Trained 4901 batches 	Training Loss: 0.239294
Epoch: 1 	Training Loss: 0.174734
Started epoch 2
Trained 1 batches 	Training Loss: 0.131212
Trained 51 batches 	Training Loss: 0.196432
Trained 101 batches 	Training Loss: 0.234385
Trained 151 batches 	Training Loss: 0.178957
Trained 201 batches 	Training Loss: 0.143672
Trained 251 batches 	Training Loss: 0.253824
Trained 301 batches 	Training Loss: 0.159737
Trained 351 batches 	Training Loss: 0.097380
Trained 401 batches 	Training Loss: 0.176440
Trained 451 batches 	Training Loss: 0.160793
Trained 501 batches 	Training Loss: 0.140926
Trained 551 batches 	Training Loss: 0.257604
Trained 601 batches 	Training Loss: 0.178631
Trained 651 batches 	Training Loss: 0.193048
Trained 701 batches 	Training Loss: 0.235236
Trained 751 batches 	Training Loss: 0.225518
Trained 801 batches 	Training Loss: 0.146291
Trained 851 batches 	Training Loss: 0.155498
Trained 901 batches 	Training Loss: 0.222454
Trained 951 batches 	Training Loss: 0.076181
Trained 1001 batches 	Training Loss: 0.172159
Trained 1051 batches 	Training Loss: 0.138832
Trained 1101 batches 	Training Loss: 0.128950
Trained 1151 batches 	Training Loss: 0.178671
Trained 1201 batches 	Training Loss: 0.143398
Trained 1251 batches 	Training Loss: 0.170095
Trained 1301 batches 	Training Loss: 0.091908
Trained 1351 batches 	Training Loss: 0.170225
Trained 1401 batches 	Training Loss: 0.168326
Trained 1451 batches 	Training Loss: 0.157505
Trained 1501 batches 	Training Loss: 0.141639
Trained 1551 batches 	Training Loss: 0.157564
Trained 1601 batches 	Training Loss: 0.215212
Trained 1651 batches 	Training Loss: 0.159114
Trained 1701 batches 	Training Loss: 0.169742
Trained 1751 batches 	Training Loss: 0.136638
Trained 1801 batches 	Training Loss: 0.132025
Trained 1851 batches 	Training Loss: 0.185881
Trained 1901 batches 	Training Loss: 0.149741
Trained 1951 batches 	Training Loss: 0.207181
Trained 2001 batches 	Training Loss: 0.125199
Trained 2051 batches 	Training Loss: 0.154157
Trained 2101 batches 	Training Loss: 0.189810
Trained 2151 batches 	Training Loss: 0.216390
Trained 2201 batches 	Training Loss: 0.099415
Trained 2251 batches 	Training Loss: 0.134071
Trained 2301 batches 	Training Loss: 0.174821
Trained 2351 batches 	Training Loss: 0.275826
Trained 2401 batches 	Training Loss: 0.205462
Trained 2451 batches 	Training Loss: 0.186115
Trained 2501 batches 	Training Loss: 0.135888
Trained 2551 batches 	Training Loss: 0.154482
Trained 2601 batches 	Training Loss: 0.145108
Trained 2651 batches 	Training Loss: 0.124871
Trained 2701 batches 	Training Loss: 0.139261
Trained 2751 batches 	Training Loss: 0.137215
Trained 2801 batches 	Training Loss: 0.093177
Trained 2851 batches 	Training Loss: 0.104986
Trained 2901 batches 	Training Loss: 0.194803
Trained 2951 batches 	Training Loss: 0.185745
Trained 3001 batches 	Training Loss: 0.138665
Trained 3051 batches 	Training Loss: 0.155182
Trained 3101 batches 	Training Loss: 0.122754
Trained 3151 batches 	Training Loss: 0.211215
Trained 3201 batches 	Training Loss: 0.236595
Trained 3251 batches 	Training Loss: 0.126789
Trained 3301 batches 	Training Loss: 0.189109
Trained 3351 batches 	Training Loss: 0.174557
Trained 3401 batches 	Training Loss: 0.122005
Trained 3451 batches 	Training Loss: 0.142090
Trained 3501 batches 	Training Loss: 0.150861
Trained 3551 batches 	Training Loss: 0.174860
Trained 3601 batches 	Training Loss: 0.146731
Trained 3651 batches 	Training Loss: 0.151375
Trained 3701 batches 	Training Loss: 0.142596
Trained 3751 batches 	Training Loss: 0.224176
Trained 3801 batches 	Training Loss: 0.168029
Trained 3851 batches 	Training Loss: 0.157628
Trained 3901 batches 	Training Loss: 0.140027
Trained 3951 batches 	Training Loss: 0.214586
Trained 4001 batches 	Training Loss: 0.152679
Trained 4051 batches 	Training Loss: 0.145211
Trained 4101 batches 	Training Loss: 0.194634
Trained 4151 batches 	Training Loss: 0.159896
Trained 4201 batches 	Training Loss: 0.136249
Trained 4251 batches 	Training Loss: 0.197271
Trained 4301 batches 	Training Loss: 0.102767
Trained 4351 batches 	Training Loss: 0.206652
Trained 4401 batches 	Training Loss: 0.213012
Trained 4451 batches 	Training Loss: 0.239298
Trained 4501 batches 	Training Loss: 0.149179
Trained 4551 batches 	Training Loss: 0.190406
Trained 4601 batches 	Training Loss: 0.212149
Trained 4651 batches 	Training Loss: 0.121571
Trained 4701 batches 	Training Loss: 0.111827
Trained 4751 batches 	Training Loss: 0.121413
Trained 4801 batches 	Training Loss: 0.173687
Trained 4851 batches 	Training Loss: 0.156240
Trained 4901 batches 	Training Loss: 0.141558
Epoch: 2 	Training Loss: 0.165363
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 1
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.832314
Trained 51 batches 	Training Loss: 0.147545
Trained 101 batches 	Training Loss: 0.155364
Trained 151 batches 	Training Loss: 0.118213
Trained 201 batches 	Training Loss: 0.134257
Trained 251 batches 	Training Loss: 0.202191
Trained 301 batches 	Training Loss: 0.149165
Trained 351 batches 	Training Loss: 0.209650
Trained 401 batches 	Training Loss: 0.198877
Trained 451 batches 	Training Loss: 0.183921
Trained 501 batches 	Training Loss: 0.180119
Trained 551 batches 	Training Loss: 0.149350
Trained 601 batches 	Training Loss: 0.160197
Trained 651 batches 	Training Loss: 0.160960
Trained 701 batches 	Training Loss: 0.190509
Trained 751 batches 	Training Loss: 0.178067
Trained 801 batches 	Training Loss: 0.096203
Trained 851 batches 	Training Loss: 0.160428
Trained 901 batches 	Training Loss: 0.178308
Trained 951 batches 	Training Loss: 0.158686
Trained 1001 batches 	Training Loss: 0.193103
Trained 1051 batches 	Training Loss: 0.150531
Trained 1101 batches 	Training Loss: 0.202206
Trained 1151 batches 	Training Loss: 0.093680
Trained 1201 batches 	Training Loss: 0.121842
Trained 1251 batches 	Training Loss: 0.164064
Trained 1301 batches 	Training Loss: 0.150513
Trained 1351 batches 	Training Loss: 0.187486
Trained 1401 batches 	Training Loss: 0.162407
Trained 1451 batches 	Training Loss: 0.123480
Trained 1501 batches 	Training Loss: 0.233324
Trained 1551 batches 	Training Loss: 0.183262
Trained 1601 batches 	Training Loss: 0.131758
Trained 1651 batches 	Training Loss: 0.208005
Trained 1701 batches 	Training Loss: 0.169251
Trained 1751 batches 	Training Loss: 0.205393
Trained 1801 batches 	Training Loss: 0.129981
Trained 1851 batches 	Training Loss: 0.184905
Trained 1901 batches 	Training Loss: 0.189258
Trained 1951 batches 	Training Loss: 0.173093
Trained 2001 batches 	Training Loss: 0.151924
Trained 2051 batches 	Training Loss: 0.208689
Trained 2101 batches 	Training Loss: 0.140166
Trained 2151 batches 	Training Loss: 0.134606
Trained 2201 batches 	Training Loss: 0.146619
Trained 2251 batches 	Training Loss: 0.188342
Trained 2301 batches 	Training Loss: 0.176412
Trained 2351 batches 	Training Loss: 0.125481
Trained 2401 batches 	Training Loss: 0.096720
Trained 2451 batches 	Training Loss: 0.213215
Epoch: 1 	Training Loss: 0.168133
Training time lapse: 10.0 min
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 1.0 min
The average AUROC is 0.766
The AUROC of Atelectasis is 0.7681259222728358
The AUROC of Cardiomegaly is 0.8400057531279211
The AUROC of Effusion is 0.8516229082856853
The AUROC of Infiltration is 0.6784302807670636
The AUROC of Mass is 0.7479509219334781
The AUROC of Nodule is 0.6824692726496
The AUROC of Pneumonia is 0.7046484074926564
The AUROC of Pneumothorax is 0.8031699699691441
The AUROC of Consolidation is 0.7704355482101513
The AUROC of Edema is 0.8651369085537477
The AUROC of Emphysema is 0.7963185174994569
The AUROC of Fibrosis is 0.6948375611181216
The AUROC of Pleural_Thickening is 0.7399738276547685
The AUROC of Hernia is 0.7824701028069434
Started training, total epoch : 1
Training data size: 1227
Started epoch 1
Trained 1 batches 	Training Loss: 0.727116
Trained 51 batches 	Training Loss: 0.159255
Trained 101 batches 	Training Loss: 0.173439
Trained 151 batches 	Training Loss: 0.180528
Trained 201 batches 	Training Loss: 0.139657
Trained 251 batches 	Training Loss: 0.161666
Trained 301 batches 	Training Loss: 0.153839
Trained 351 batches 	Training Loss: 0.183409
Trained 401 batches 	Training Loss: 0.176934
Trained 451 batches 	Training Loss: 0.157529
Trained 501 batches 	Training Loss: 0.177219
Trained 551 batches 	Training Loss: 0.151550
Trained 601 batches 	Training Loss: 0.132095
Trained 651 batches 	Training Loss: 0.134866
Trained 701 batches 	Training Loss: 0.151951
Trained 751 batches 	Training Loss: 0.157997
Trained 801 batches 	Training Loss: 0.167203
Trained 851 batches 	Training Loss: 0.165102
Trained 901 batches 	Training Loss: 0.153505
Trained 951 batches 	Training Loss: 0.131520
Trained 1001 batches 	Training Loss: 0.172813
Trained 1051 batches 	Training Loss: 0.140970
Trained 1101 batches 	Training Loss: 0.160277
Trained 1151 batches 	Training Loss: 0.176127
Trained 1201 batches 	Training Loss: 0.167858
Epoch: 1 	Training Loss: 0.165379
Training time lapse: 11.0 min
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 1.0 min
The average AUROC is 0.768
The AUROC of Atelectasis is 0.7665943191643538
The AUROC of Cardiomegaly is 0.8258193643946927
The AUROC of Effusion is 0.8578588176003186
The AUROC of Infiltration is 0.6804349299925023
The AUROC of Mass is 0.7287517491513366
The AUROC of Nodule is 0.6919287234623152
The AUROC of Pneumonia is 0.7162434115360314
The AUROC of Pneumothorax is 0.8128931566099374
The AUROC of Consolidation is 0.7819751200734478
The AUROC of Edema is 0.8811540596012715
The AUROC of Emphysema is 0.819929418665018
The AUROC of Fibrosis is 0.7351653966833521
The AUROC of Pleural_Thickening is 0.7256306385511205
The AUROC of Hernia is 0.7291704668996666
Started training, total epoch : 2
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.719465
Trained 51 batches 	Training Loss: 0.183180
Trained 101 batches 	Training Loss: 0.152707
Trained 151 batches 	Training Loss: 0.207387
Trained 201 batches 	Training Loss: 0.211300
Trained 251 batches 	Training Loss: 0.205623
Trained 301 batches 	Training Loss: 0.196375
Trained 351 batches 	Training Loss: 0.202263
Trained 401 batches 	Training Loss: 0.117062
Trained 451 batches 	Training Loss: 0.168040
Trained 501 batches 	Training Loss: 0.143905
Trained 551 batches 	Training Loss: 0.170630
Trained 601 batches 	Training Loss: 0.178418
Trained 651 batches 	Training Loss: 0.150475
Trained 701 batches 	Training Loss: 0.133350
Trained 751 batches 	Training Loss: 0.173933
Trained 801 batches 	Training Loss: 0.177286
Trained 851 batches 	Training Loss: 0.145580
Trained 901 batches 	Training Loss: 0.109885
Trained 951 batches 	Training Loss: 0.239244
Trained 1001 batches 	Training Loss: 0.169627
Trained 1051 batches 	Training Loss: 0.188924
Trained 1101 batches 	Training Loss: 0.145429
Trained 1151 batches 	Training Loss: 0.195457
Trained 1201 batches 	Training Loss: 0.174637
Trained 1251 batches 	Training Loss: 0.135695
Trained 1301 batches 	Training Loss: 0.112939
Trained 1351 batches 	Training Loss: 0.165526
Trained 1401 batches 	Training Loss: 0.177648
Trained 1451 batches 	Training Loss: 0.145878
Trained 1501 batches 	Training Loss: 0.131834
Trained 1551 batches 	Training Loss: 0.202557
Trained 1601 batches 	Training Loss: 0.152654
Trained 1651 batches 	Training Loss: 0.179365
Trained 1701 batches 	Training Loss: 0.216872
Trained 1751 batches 	Training Loss: 0.173093
Trained 1801 batches 	Training Loss: 0.195293
Trained 1851 batches 	Training Loss: 0.205567
Trained 1901 batches 	Training Loss: 0.146742
Trained 1951 batches 	Training Loss: 0.123367
Trained 2001 batches 	Training Loss: 0.190424
Trained 2051 batches 	Training Loss: 0.130121
Trained 2101 batches 	Training Loss: 0.146813
Trained 2151 batches 	Training Loss: 0.176930
Trained 2201 batches 	Training Loss: 0.189615
Trained 2251 batches 	Training Loss: 0.159059
Trained 2301 batches 	Training Loss: 0.179594
Trained 2351 batches 	Training Loss: 0.121084
Trained 2401 batches 	Training Loss: 0.123077
Trained 2451 batches 	Training Loss: 0.152490
Epoch: 1 	Training Loss: 0.167856
Started epoch 2
Trained 1 batches 	Training Loss: 0.185548
Trained 51 batches 	Training Loss: 0.126551
Trained 101 batches 	Training Loss: 0.169469
Trained 151 batches 	Training Loss: 0.206071
Trained 201 batches 	Training Loss: 0.220203
Trained 251 batches 	Training Loss: 0.145880
Trained 301 batches 	Training Loss: 0.187149
Trained 351 batches 	Training Loss: 0.194710
Trained 401 batches 	Training Loss: 0.146893
Trained 451 batches 	Training Loss: 0.178768
Trained 501 batches 	Training Loss: 0.208477
Trained 551 batches 	Training Loss: 0.207511
Trained 601 batches 	Training Loss: 0.143950
Trained 651 batches 	Training Loss: 0.174333
Trained 701 batches 	Training Loss: 0.162871
Trained 751 batches 	Training Loss: 0.091958
Trained 801 batches 	Training Loss: 0.131528
Trained 851 batches 	Training Loss: 0.152918
Trained 901 batches 	Training Loss: 0.204655
Trained 951 batches 	Training Loss: 0.134472
Trained 1001 batches 	Training Loss: 0.134358
Trained 1051 batches 	Training Loss: 0.191480
Trained 1101 batches 	Training Loss: 0.182325
Trained 1151 batches 	Training Loss: 0.142711
Trained 1201 batches 	Training Loss: 0.148450
Trained 1251 batches 	Training Loss: 0.115557
Trained 1301 batches 	Training Loss: 0.167049
Trained 1351 batches 	Training Loss: 0.157771
Trained 1401 batches 	Training Loss: 0.158277
Trained 1451 batches 	Training Loss: 0.122935
Trained 1501 batches 	Training Loss: 0.145363
Trained 1551 batches 	Training Loss: 0.116969
Trained 1601 batches 	Training Loss: 0.105302
Trained 1651 batches 	Training Loss: 0.180969
Trained 1701 batches 	Training Loss: 0.167202
Trained 1751 batches 	Training Loss: 0.183368
Trained 1801 batches 	Training Loss: 0.160855
Trained 1851 batches 	Training Loss: 0.120077
Trained 1901 batches 	Training Loss: 0.114795
Trained 1951 batches 	Training Loss: 0.184327
Trained 2001 batches 	Training Loss: 0.168117
Trained 2051 batches 	Training Loss: 0.125410
Trained 2101 batches 	Training Loss: 0.129728
Trained 2151 batches 	Training Loss: 0.188006
Trained 2201 batches 	Training Loss: 0.213398
Trained 2251 batches 	Training Loss: 0.132223
Trained 2301 batches 	Training Loss: 0.213162
Trained 2351 batches 	Training Loss: 0.248526
Trained 2401 batches 	Training Loss: 0.126937
Trained 2451 batches 	Training Loss: 0.129433
Epoch: 2 	Training Loss: 0.158728
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Started training, total epoch : 2
Training data size: 2453
Started epoch 1
Trained 1 batches 	Training Loss: 0.685269
Trained 51 batches 	Training Loss: 0.146657
Trained 101 batches 	Training Loss: 0.201597
Trained 151 batches 	Training Loss: 0.195627
Trained 201 batches 	Training Loss: 0.217692
Trained 251 batches 	Training Loss: 0.130720
Trained 301 batches 	Training Loss: 0.158676
Trained 351 batches 	Training Loss: 0.208834
Trained 401 batches 	Training Loss: 0.184254
Trained 451 batches 	Training Loss: 0.162146
Trained 501 batches 	Training Loss: 0.151318
Trained 551 batches 	Training Loss: 0.200017
Trained 601 batches 	Training Loss: 0.175274
Trained 651 batches 	Training Loss: 0.150496
Trained 701 batches 	Training Loss: 0.192782
Trained 751 batches 	Training Loss: 0.143034
Trained 801 batches 	Training Loss: 0.200238
Trained 851 batches 	Training Loss: 0.206537
Trained 901 batches 	Training Loss: 0.112081
Trained 951 batches 	Training Loss: 0.140199
Trained 1001 batches 	Training Loss: 0.217090
Trained 1051 batches 	Training Loss: 0.148359
Trained 1101 batches 	Training Loss: 0.121898
Trained 1151 batches 	Training Loss: 0.148947
Trained 1201 batches 	Training Loss: 0.188127
Trained 1251 batches 	Training Loss: 0.221000
Trained 1301 batches 	Training Loss: 0.165668
Trained 1351 batches 	Training Loss: 0.210940
Trained 1401 batches 	Training Loss: 0.199835
Trained 1451 batches 	Training Loss: 0.158015
Trained 1501 batches 	Training Loss: 0.174468
Trained 1551 batches 	Training Loss: 0.184907
Trained 1601 batches 	Training Loss: 0.135156
Trained 1651 batches 	Training Loss: 0.111414
Trained 1701 batches 	Training Loss: 0.155563
Trained 1751 batches 	Training Loss: 0.207825
Trained 1801 batches 	Training Loss: 0.145864
Trained 1851 batches 	Training Loss: 0.132159
Trained 1901 batches 	Training Loss: 0.218960
Trained 1951 batches 	Training Loss: 0.117294
Trained 2001 batches 	Training Loss: 0.146225
Trained 2051 batches 	Training Loss: 0.143575
Trained 2101 batches 	Training Loss: 0.138025
Trained 2151 batches 	Training Loss: 0.222390
Trained 2201 batches 	Training Loss: 0.113183
Trained 2251 batches 	Training Loss: 0.136875
Trained 2301 batches 	Training Loss: 0.149147
Trained 2351 batches 	Training Loss: 0.139434
Trained 2401 batches 	Training Loss: 0.174990
Trained 2451 batches 	Training Loss: 0.201884
Epoch: 1 	Training Loss: 0.168800
Started epoch 2
Trained 1 batches 	Training Loss: 0.258677
Trained 51 batches 	Training Loss: 0.178845
Trained 101 batches 	Training Loss: 0.195231
Trained 151 batches 	Training Loss: 0.163776
Trained 201 batches 	Training Loss: 0.150776
Trained 251 batches 	Training Loss: 0.193649
Trained 301 batches 	Training Loss: 0.181351
Trained 351 batches 	Training Loss: 0.153240
Trained 401 batches 	Training Loss: 0.169048
Trained 451 batches 	Training Loss: 0.186760
Trained 501 batches 	Training Loss: 0.157353
Trained 551 batches 	Training Loss: 0.173227
Trained 601 batches 	Training Loss: 0.147554
Trained 651 batches 	Training Loss: 0.194927
Trained 701 batches 	Training Loss: 0.221183
Trained 751 batches 	Training Loss: 0.176128
Trained 801 batches 	Training Loss: 0.125666
Trained 851 batches 	Training Loss: 0.175330
Trained 901 batches 	Training Loss: 0.163477
Trained 951 batches 	Training Loss: 0.153550
Trained 1001 batches 	Training Loss: 0.215868
Trained 1051 batches 	Training Loss: 0.171278
Trained 1101 batches 	Training Loss: 0.148908
Trained 1151 batches 	Training Loss: 0.121123
Trained 1201 batches 	Training Loss: 0.173362
Trained 1251 batches 	Training Loss: 0.148211
Trained 1301 batches 	Training Loss: 0.127822
Trained 1351 batches 	Training Loss: 0.222402
Trained 1401 batches 	Training Loss: 0.168012
Trained 1451 batches 	Training Loss: 0.157609
Trained 1501 batches 	Training Loss: 0.132017
Trained 1551 batches 	Training Loss: 0.187665
Trained 1601 batches 	Training Loss: 0.139630
Trained 1651 batches 	Training Loss: 0.152007
Trained 1701 batches 	Training Loss: 0.227494
Trained 1751 batches 	Training Loss: 0.192662
Trained 1801 batches 	Training Loss: 0.132489
Trained 1851 batches 	Training Loss: 0.195746
Trained 1901 batches 	Training Loss: 0.141060
Trained 1951 batches 	Training Loss: 0.128641
Trained 2001 batches 	Training Loss: 0.173813
Trained 2051 batches 	Training Loss: 0.148143
Trained 2101 batches 	Training Loss: 0.184670
Trained 2151 batches 	Training Loss: 0.200056
Trained 2201 batches 	Training Loss: 0.125283
Trained 2251 batches 	Training Loss: 0.150649
Trained 2301 batches 	Training Loss: 0.197373
Trained 2351 batches 	Training Loss: 0.152293
Trained 2401 batches 	Training Loss: 0.169413
Trained 2451 batches 	Training Loss: 0.175831
Epoch: 2 	Training Loss: 0.159438
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 351
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.693414
Trained 51 batches 	Training Loss: 0.115145
Trained 101 batches 	Training Loss: 0.145265
Trained 151 batches 	Training Loss: 0.210764
Trained 201 batches 	Training Loss: 0.133698
Trained 251 batches 	Training Loss: 0.152707
Trained 301 batches 	Training Loss: 0.193620
Trained 351 batches 	Training Loss: 0.250519
Trained 401 batches 	Training Loss: 0.191842
Trained 451 batches 	Training Loss: 0.157889
Trained 501 batches 	Training Loss: 0.115581
Trained 551 batches 	Training Loss: 0.191541
Trained 601 batches 	Training Loss: 0.191309
Trained 651 batches 	Training Loss: 0.126294
Trained 701 batches 	Training Loss: 0.180533
Trained 751 batches 	Training Loss: 0.234460
Trained 801 batches 	Training Loss: 0.074785
Trained 851 batches 	Training Loss: 0.160192
Trained 901 batches 	Training Loss: 0.191137
Trained 951 batches 	Training Loss: 0.192021
Trained 1001 batches 	Training Loss: 0.145139
Trained 1051 batches 	Training Loss: 0.088096
Trained 1101 batches 	Training Loss: 0.180039
Trained 1151 batches 	Training Loss: 0.134352
Trained 1201 batches 	Training Loss: 0.180019
Trained 1251 batches 	Training Loss: 0.191695
Trained 1301 batches 	Training Loss: 0.156579
Trained 1351 batches 	Training Loss: 0.220843
Trained 1401 batches 	Training Loss: 0.170566
Trained 1451 batches 	Training Loss: 0.178332
Trained 1501 batches 	Training Loss: 0.095689
Trained 1551 batches 	Training Loss: 0.198789
Trained 1601 batches 	Training Loss: 0.138995
Trained 1651 batches 	Training Loss: 0.130583
Trained 1701 batches 	Training Loss: 0.158895
Trained 1751 batches 	Training Loss: 0.200003
Trained 1801 batches 	Training Loss: 0.155231
Trained 1851 batches 	Training Loss: 0.126376
Trained 1901 batches 	Training Loss: 0.176560
Trained 1951 batches 	Training Loss: 0.180889
Trained 2001 batches 	Training Loss: 0.093007
Trained 2051 batches 	Training Loss: 0.125734
Trained 2101 batches 	Training Loss: 0.138192
Trained 2151 batches 	Training Loss: 0.201323
Trained 2201 batches 	Training Loss: 0.205106
Trained 2251 batches 	Training Loss: 0.144776
Trained 2301 batches 	Training Loss: 0.145174
Trained 2351 batches 	Training Loss: 0.218092
Trained 2401 batches 	Training Loss: 0.147503
Trained 2451 batches 	Training Loss: 0.309998
Trained 2501 batches 	Training Loss: 0.145076
Trained 2551 batches 	Training Loss: 0.155427
Trained 2601 batches 	Training Loss: 0.160844
Trained 2651 batches 	Training Loss: 0.130667
Trained 2701 batches 	Training Loss: 0.115142
Trained 2751 batches 	Training Loss: 0.096445
Trained 2801 batches 	Training Loss: 0.152097
Trained 2851 batches 	Training Loss: 0.189314
Trained 2901 batches 	Training Loss: 0.181876
Trained 2951 batches 	Training Loss: 0.079506
Trained 3001 batches 	Training Loss: 0.122232
Trained 3051 batches 	Training Loss: 0.205649
Trained 3101 batches 	Training Loss: 0.217492
Trained 3151 batches 	Training Loss: 0.111042
Trained 3201 batches 	Training Loss: 0.201178
Trained 3251 batches 	Training Loss: 0.174787
Trained 3301 batches 	Training Loss: 0.151277
Trained 3351 batches 	Training Loss: 0.182052
Trained 3401 batches 	Training Loss: 0.146178
Trained 3451 batches 	Training Loss: 0.215893
Trained 3501 batches 	Training Loss: 0.169549
Trained 3551 batches 	Training Loss: 0.166123
Trained 3601 batches 	Training Loss: 0.092810
Trained 3651 batches 	Training Loss: 0.259640
Trained 3701 batches 	Training Loss: 0.091940
Trained 3751 batches 	Training Loss: 0.131652
Trained 3801 batches 	Training Loss: 0.141395
Trained 3851 batches 	Training Loss: 0.243845
Trained 3901 batches 	Training Loss: 0.150859
Trained 3951 batches 	Training Loss: 0.183746
Trained 4001 batches 	Training Loss: 0.249696
Trained 4051 batches 	Training Loss: 0.262719
Trained 4101 batches 	Training Loss: 0.180281
Trained 4151 batches 	Training Loss: 0.237099
Trained 4201 batches 	Training Loss: 0.155369
Trained 4251 batches 	Training Loss: 0.158103
Trained 4301 batches 	Training Loss: 0.199977
Trained 4351 batches 	Training Loss: 0.292713
Trained 4401 batches 	Training Loss: 0.164592
Trained 4451 batches 	Training Loss: 0.141634
Trained 4501 batches 	Training Loss: 0.116731
Trained 4551 batches 	Training Loss: 0.156667
Trained 4601 batches 	Training Loss: 0.147240
Trained 4651 batches 	Training Loss: 0.227960
Trained 4701 batches 	Training Loss: 0.225398
Trained 4751 batches 	Training Loss: 0.171605
Trained 4801 batches 	Training Loss: 0.266194
Trained 4851 batches 	Training Loss: 0.129061
Trained 4901 batches 	Training Loss: 0.246280
Epoch: 1 	Training Loss: 0.173777
Started epoch 2
Trained 1 batches 	Training Loss: 0.185890
Trained 51 batches 	Training Loss: 0.247111
Trained 101 batches 	Training Loss: 0.170572
Trained 151 batches 	Training Loss: 0.205127
Trained 201 batches 	Training Loss: 0.145137
Trained 251 batches 	Training Loss: 0.264470
Trained 301 batches 	Training Loss: 0.110136
Trained 351 batches 	Training Loss: 0.199766
Trained 401 batches 	Training Loss: 0.163393
Trained 451 batches 	Training Loss: 0.149506
Trained 501 batches 	Training Loss: 0.176566
Trained 551 batches 	Training Loss: 0.119879
Trained 601 batches 	Training Loss: 0.192906
Trained 651 batches 	Training Loss: 0.102526
Trained 701 batches 	Training Loss: 0.117726
Trained 751 batches 	Training Loss: 0.145555
Trained 801 batches 	Training Loss: 0.200205
Trained 851 batches 	Training Loss: 0.083567
Trained 901 batches 	Training Loss: 0.108028
Trained 951 batches 	Training Loss: 0.134271
Trained 1001 batches 	Training Loss: 0.169336
Trained 1051 batches 	Training Loss: 0.244450
Trained 1101 batches 	Training Loss: 0.215703
Trained 1151 batches 	Training Loss: 0.190373
Trained 1201 batches 	Training Loss: 0.150248
Trained 1251 batches 	Training Loss: 0.117175
Trained 1301 batches 	Training Loss: 0.200385
Trained 1351 batches 	Training Loss: 0.200609
Trained 1401 batches 	Training Loss: 0.155065
Trained 1451 batches 	Training Loss: 0.148930
Trained 1501 batches 	Training Loss: 0.105142
Trained 1551 batches 	Training Loss: 0.161655
Trained 1601 batches 	Training Loss: 0.251840
Trained 1651 batches 	Training Loss: 0.180915
Trained 1701 batches 	Training Loss: 0.254554
Trained 1751 batches 	Training Loss: 0.077397
Trained 1801 batches 	Training Loss: 0.173566
Trained 1851 batches 	Training Loss: 0.145598
Trained 1901 batches 	Training Loss: 0.143228
Trained 1951 batches 	Training Loss: 0.149878
Trained 2001 batches 	Training Loss: 0.088855
Trained 2051 batches 	Training Loss: 0.140477
Trained 2101 batches 	Training Loss: 0.162411
Trained 2151 batches 	Training Loss: 0.131848
Trained 2201 batches 	Training Loss: 0.180276
Trained 2251 batches 	Training Loss: 0.088583
Trained 2301 batches 	Training Loss: 0.161514
Trained 2351 batches 	Training Loss: 0.146117
Trained 2401 batches 	Training Loss: 0.170535
Trained 2451 batches 	Training Loss: 0.175959
Trained 2501 batches 	Training Loss: 0.104831
Trained 2551 batches 	Training Loss: 0.300461
Trained 2601 batches 	Training Loss: 0.100226
Trained 2651 batches 	Training Loss: 0.122365
Trained 2701 batches 	Training Loss: 0.154435
Trained 2751 batches 	Training Loss: 0.211901
Trained 2801 batches 	Training Loss: 0.172836
Trained 2851 batches 	Training Loss: 0.105637
Trained 2901 batches 	Training Loss: 0.125673
Trained 2951 batches 	Training Loss: 0.139999
Trained 3001 batches 	Training Loss: 0.138111
Trained 3051 batches 	Training Loss: 0.159791
Trained 3101 batches 	Training Loss: 0.121911
Trained 3151 batches 	Training Loss: 0.146619
Trained 3201 batches 	Training Loss: 0.179771
Trained 3251 batches 	Training Loss: 0.196425
Trained 3301 batches 	Training Loss: 0.072166
Trained 3351 batches 	Training Loss: 0.150708
Trained 3401 batches 	Training Loss: 0.207401
Trained 3451 batches 	Training Loss: 0.195694
Trained 3501 batches 	Training Loss: 0.199479
Trained 3551 batches 	Training Loss: 0.109568
Trained 3601 batches 	Training Loss: 0.159945
Trained 3651 batches 	Training Loss: 0.216315
Trained 3701 batches 	Training Loss: 0.161031
Trained 3751 batches 	Training Loss: 0.114367
Trained 3801 batches 	Training Loss: 0.206011
Trained 3851 batches 	Training Loss: 0.171324
Trained 3901 batches 	Training Loss: 0.159313
Trained 3951 batches 	Training Loss: 0.197845
Trained 4001 batches 	Training Loss: 0.112311
Trained 4051 batches 	Training Loss: 0.116730
Trained 4101 batches 	Training Loss: 0.129271
Trained 4151 batches 	Training Loss: 0.168102
Trained 4201 batches 	Training Loss: 0.111386
Trained 4251 batches 	Training Loss: 0.080979
Trained 4301 batches 	Training Loss: 0.260923
Trained 4351 batches 	Training Loss: 0.185235
Trained 4401 batches 	Training Loss: 0.177369
Trained 4451 batches 	Training Loss: 0.154301
Trained 4501 batches 	Training Loss: 0.147165
Trained 4551 batches 	Training Loss: 0.138506
Trained 4601 batches 	Training Loss: 0.192314
Trained 4651 batches 	Training Loss: 0.182549
Trained 4701 batches 	Training Loss: 0.130110
Trained 4751 batches 	Training Loss: 0.158577
Trained 4801 batches 	Training Loss: 0.133031
Trained 4851 batches 	Training Loss: 0.131492
Trained 4901 batches 	Training Loss: 0.100457
Epoch: 2 	Training Loss: 0.164047
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 2
Training data size: 9811
Started epoch 1
Trained 1 batches 	Training Loss: 0.857928
Trained 51 batches 	Training Loss: 0.076998
Trained 101 batches 	Training Loss: 0.243871
Trained 151 batches 	Training Loss: 0.115504
Trained 201 batches 	Training Loss: 0.183336
Trained 251 batches 	Training Loss: 0.290399
Trained 301 batches 	Training Loss: 0.175105
Trained 351 batches 	Training Loss: 0.341570
Trained 401 batches 	Training Loss: 0.188451
Trained 451 batches 	Training Loss: 0.200626
Trained 501 batches 	Training Loss: 0.252450
Trained 551 batches 	Training Loss: 0.120296
Trained 601 batches 	Training Loss: 0.116715
Trained 651 batches 	Training Loss: 0.126391
Trained 701 batches 	Training Loss: 0.145072
Trained 751 batches 	Training Loss: 0.292119
Trained 801 batches 	Training Loss: 0.161783
Trained 851 batches 	Training Loss: 0.117111
Trained 901 batches 	Training Loss: 0.085270
Trained 951 batches 	Training Loss: 0.150601
Trained 1001 batches 	Training Loss: 0.149887
Trained 1051 batches 	Training Loss: 0.218897
Trained 1101 batches 	Training Loss: 0.186521
Trained 1151 batches 	Training Loss: 0.274798
Trained 1201 batches 	Training Loss: 0.167066
Trained 1251 batches 	Training Loss: 0.253646
Trained 1301 batches 	Training Loss: 0.214354
Trained 1351 batches 	Training Loss: 0.138937
Trained 1401 batches 	Training Loss: 0.232149
Trained 1451 batches 	Training Loss: 0.216344
Trained 1501 batches 	Training Loss: 0.098957
Trained 1551 batches 	Training Loss: 0.189570
Trained 1601 batches 	Training Loss: 0.182826
Trained 1651 batches 	Training Loss: 0.221451
Trained 1701 batches 	Training Loss: 0.179005
Trained 1751 batches 	Training Loss: 0.117264
Trained 1801 batches 	Training Loss: 0.184834
Trained 1851 batches 	Training Loss: 0.180465
Trained 1901 batches 	Training Loss: 0.208153
Trained 1951 batches 	Training Loss: 0.098721
Trained 2001 batches 	Training Loss: 0.296189
Trained 2051 batches 	Training Loss: 0.116439
Trained 2101 batches 	Training Loss: 0.279205
Trained 2151 batches 	Training Loss: 0.239679
Trained 2201 batches 	Training Loss: 0.150495
Trained 2251 batches 	Training Loss: 0.213774
Trained 2301 batches 	Training Loss: 0.144877
Trained 2351 batches 	Training Loss: 0.177180
Trained 2401 batches 	Training Loss: 0.245607
Trained 2451 batches 	Training Loss: 0.246670
Trained 2501 batches 	Training Loss: 0.243730
Trained 2551 batches 	Training Loss: 0.243147
Trained 2601 batches 	Training Loss: 0.192261
Trained 2651 batches 	Training Loss: 0.384578
Trained 2701 batches 	Training Loss: 0.162723
Trained 2751 batches 	Training Loss: 0.161833
Trained 2801 batches 	Training Loss: 0.210154
Trained 2851 batches 	Training Loss: 0.088049
Trained 2901 batches 	Training Loss: 0.128163
Trained 2951 batches 	Training Loss: 0.093946
Trained 3001 batches 	Training Loss: 0.209351
Trained 3051 batches 	Training Loss: 0.281070
Trained 3101 batches 	Training Loss: 0.243524
Trained 3151 batches 	Training Loss: 0.280351
Trained 3201 batches 	Training Loss: 0.211036
Trained 3251 batches 	Training Loss: 0.117538
Trained 3301 batches 	Training Loss: 0.259470
Trained 3351 batches 	Training Loss: 0.311318
Trained 3401 batches 	Training Loss: 0.097558
Trained 3451 batches 	Training Loss: 0.164038
Trained 3501 batches 	Training Loss: 0.284956
Trained 3551 batches 	Training Loss: 0.216507
Trained 3601 batches 	Training Loss: 0.138017
Trained 3651 batches 	Training Loss: 0.100667
Trained 3701 batches 	Training Loss: 0.192117
Trained 3751 batches 	Training Loss: 0.153370
Trained 3801 batches 	Training Loss: 0.168167
Trained 3851 batches 	Training Loss: 0.349366
Trained 3901 batches 	Training Loss: 0.223350
Trained 3951 batches 	Training Loss: 0.229702
Trained 4001 batches 	Training Loss: 0.238023
Trained 4051 batches 	Training Loss: 0.225340
Trained 4101 batches 	Training Loss: 0.211614
Trained 4151 batches 	Training Loss: 0.126195
Trained 4201 batches 	Training Loss: 0.190239
Trained 4251 batches 	Training Loss: 0.328368
Trained 4301 batches 	Training Loss: 0.158804
Trained 4351 batches 	Training Loss: 0.172337
Trained 4401 batches 	Training Loss: 0.139975
Trained 4451 batches 	Training Loss: 0.166255
Trained 4501 batches 	Training Loss: 0.129640
Trained 4551 batches 	Training Loss: 0.275138
Trained 4601 batches 	Training Loss: 0.223686
Trained 4651 batches 	Training Loss: 0.198254
Trained 4701 batches 	Training Loss: 0.206004
Trained 4751 batches 	Training Loss: 0.230083
Trained 4801 batches 	Training Loss: 0.219716
Trained 4851 batches 	Training Loss: 0.297941
Trained 4901 batches 	Training Loss: 0.352458
Trained 4951 batches 	Training Loss: 0.128220
Trained 5001 batches 	Training Loss: 0.105475
Trained 5051 batches 	Training Loss: 0.169689
Trained 5101 batches 	Training Loss: 0.271548
Trained 5151 batches 	Training Loss: 0.109087
Trained 5201 batches 	Training Loss: 0.202615
Trained 5251 batches 	Training Loss: 0.142375
Trained 5301 batches 	Training Loss: 0.165566
Trained 5351 batches 	Training Loss: 0.232601
Trained 5401 batches 	Training Loss: 0.228206
Trained 5451 batches 	Training Loss: 0.334424
Trained 5501 batches 	Training Loss: 0.161277
Trained 5551 batches 	Training Loss: 0.112131
Trained 5601 batches 	Training Loss: 0.117527
Trained 5651 batches 	Training Loss: 0.070877
Trained 5701 batches 	Training Loss: 0.228743
Trained 5751 batches 	Training Loss: 0.108793
Trained 5801 batches 	Training Loss: 0.222218
Trained 5851 batches 	Training Loss: 0.282657
Trained 5901 batches 	Training Loss: 0.174417
Trained 5951 batches 	Training Loss: 0.096709
Trained 6001 batches 	Training Loss: 0.190852
Trained 6051 batches 	Training Loss: 0.201933
Trained 6101 batches 	Training Loss: 0.192781
Trained 6151 batches 	Training Loss: 0.120810
Trained 6201 batches 	Training Loss: 0.135285
Trained 6251 batches 	Training Loss: 0.088391
Trained 6301 batches 	Training Loss: 0.138139
Trained 6351 batches 	Training Loss: 0.069237
Trained 6401 batches 	Training Loss: 0.176090
Trained 6451 batches 	Training Loss: 0.194652
Trained 6501 batches 	Training Loss: 0.134095
Trained 6551 batches 	Training Loss: 0.177014
Trained 6601 batches 	Training Loss: 0.207720
Trained 6651 batches 	Training Loss: 0.268221
Trained 6701 batches 	Training Loss: 0.205065
Trained 6751 batches 	Training Loss: 0.257201
Trained 6801 batches 	Training Loss: 0.117056
Trained 6851 batches 	Training Loss: 0.103740
Trained 6901 batches 	Training Loss: 0.147200
Trained 6951 batches 	Training Loss: 0.146041
Trained 7001 batches 	Training Loss: 0.092366
Trained 7051 batches 	Training Loss: 0.111510
Trained 7101 batches 	Training Loss: 0.146294
Trained 7151 batches 	Training Loss: 0.254150
Trained 7201 batches 	Training Loss: 0.114240
Trained 7251 batches 	Training Loss: 0.157065
Trained 7301 batches 	Training Loss: 0.257451
Trained 7351 batches 	Training Loss: 0.147665
Trained 7401 batches 	Training Loss: 0.243949
Trained 7451 batches 	Training Loss: 0.232325
Trained 7501 batches 	Training Loss: 0.146884
Trained 7551 batches 	Training Loss: 0.121531
Trained 7601 batches 	Training Loss: 0.123351
Trained 7651 batches 	Training Loss: 0.207474
Trained 7701 batches 	Training Loss: 0.226268
Trained 7751 batches 	Training Loss: 0.214287
Trained 7801 batches 	Training Loss: 0.185881
Trained 7851 batches 	Training Loss: 0.079983
Trained 7901 batches 	Training Loss: 0.154881
Trained 7951 batches 	Training Loss: 0.198808
Trained 8001 batches 	Training Loss: 0.186325
Trained 8051 batches 	Training Loss: 0.186185
Trained 8101 batches 	Training Loss: 0.232436
Trained 8151 batches 	Training Loss: 0.162125
Trained 8201 batches 	Training Loss: 0.220271
Trained 8251 batches 	Training Loss: 0.181886
Trained 8301 batches 	Training Loss: 0.128942
Trained 8351 batches 	Training Loss: 0.218219
Trained 8401 batches 	Training Loss: 0.383994
Trained 8451 batches 	Training Loss: 0.149714
Trained 8501 batches 	Training Loss: 0.186928
Trained 8551 batches 	Training Loss: 0.230885
Trained 8601 batches 	Training Loss: 0.296101
Trained 8651 batches 	Training Loss: 0.301628
Trained 8701 batches 	Training Loss: 0.157343
Trained 8751 batches 	Training Loss: 0.151081
Trained 8801 batches 	Training Loss: 0.212197
Trained 8851 batches 	Training Loss: 0.266451
Trained 8901 batches 	Training Loss: 0.157832
Trained 8951 batches 	Training Loss: 0.237398
Trained 9001 batches 	Training Loss: 0.365465
Trained 9051 batches 	Training Loss: 0.239898
Trained 9101 batches 	Training Loss: 0.216929
Trained 9151 batches 	Training Loss: 0.203279
Trained 9201 batches 	Training Loss: 0.206744
Trained 9251 batches 	Training Loss: 0.106778
Trained 9301 batches 	Training Loss: 0.123252
Trained 9351 batches 	Training Loss: 0.111615
Trained 9401 batches 	Training Loss: 0.087527
Trained 9451 batches 	Training Loss: 0.167618
Trained 9501 batches 	Training Loss: 0.168921
Trained 9551 batches 	Training Loss: 0.149196
Trained 9601 batches 	Training Loss: 0.381823
Trained 9651 batches 	Training Loss: 0.151147
Trained 9701 batches 	Training Loss: 0.124075
Trained 9751 batches 	Training Loss: 0.272523
Trained 9801 batches 	Training Loss: 0.181156
Epoch: 1 	Training Loss: 0.180972
Started epoch 2
Trained 1 batches 	Training Loss: 0.226875
Trained 51 batches 	Training Loss: 0.144271
Trained 101 batches 	Training Loss: 0.274241
Trained 151 batches 	Training Loss: 0.112235
Trained 201 batches 	Training Loss: 0.195432
Trained 251 batches 	Training Loss: 0.221247
Trained 301 batches 	Training Loss: 0.201654
Trained 351 batches 	Training Loss: 0.163615
Trained 401 batches 	Training Loss: 0.102366
Trained 451 batches 	Training Loss: 0.143259
Trained 501 batches 	Training Loss: 0.091649
Trained 551 batches 	Training Loss: 0.205852
Trained 601 batches 	Training Loss: 0.150727
Trained 651 batches 	Training Loss: 0.193570
Trained 701 batches 	Training Loss: 0.151689
Trained 751 batches 	Training Loss: 0.244999
Trained 801 batches 	Training Loss: 0.247328
Trained 851 batches 	Training Loss: 0.195908
Trained 901 batches 	Training Loss: 0.278658
Trained 951 batches 	Training Loss: 0.196267
Trained 1001 batches 	Training Loss: 0.182148
Trained 1051 batches 	Training Loss: 0.132507
Trained 1101 batches 	Training Loss: 0.137344
Trained 1151 batches 	Training Loss: 0.131036
Trained 1201 batches 	Training Loss: 0.093143
Trained 1251 batches 	Training Loss: 0.192636
Trained 1301 batches 	Training Loss: 0.160748
Trained 1351 batches 	Training Loss: 0.260222
Trained 1401 batches 	Training Loss: 0.150722
Trained 1451 batches 	Training Loss: 0.216529
Trained 1501 batches 	Training Loss: 0.129127
Trained 1551 batches 	Training Loss: 0.124167
Trained 1601 batches 	Training Loss: 0.147264
Trained 1651 batches 	Training Loss: 0.132862
Trained 1701 batches 	Training Loss: 0.150759
Trained 1751 batches 	Training Loss: 0.110996
Trained 1801 batches 	Training Loss: 0.229724
Trained 1851 batches 	Training Loss: 0.219534
Trained 1901 batches 	Training Loss: 0.162715
Trained 1951 batches 	Training Loss: 0.233384
Trained 2001 batches 	Training Loss: 0.176457
Trained 2051 batches 	Training Loss: 0.115084
Trained 2101 batches 	Training Loss: 0.126962
Trained 2151 batches 	Training Loss: 0.130913
Trained 2201 batches 	Training Loss: 0.238557
Trained 2251 batches 	Training Loss: 0.264241
Trained 2301 batches 	Training Loss: 0.127168
Trained 2351 batches 	Training Loss: 0.230835
Trained 2401 batches 	Training Loss: 0.156505
Trained 2451 batches 	Training Loss: 0.203095
Trained 2501 batches 	Training Loss: 0.072459
Trained 2551 batches 	Training Loss: 0.127610
Trained 2601 batches 	Training Loss: 0.158332
Trained 2651 batches 	Training Loss: 0.145011
Trained 2701 batches 	Training Loss: 0.201846
Trained 2751 batches 	Training Loss: 0.184407
Trained 2801 batches 	Training Loss: 0.138134
Trained 2851 batches 	Training Loss: 0.132524
Trained 2901 batches 	Training Loss: 0.184199
Trained 2951 batches 	Training Loss: 0.192426
Trained 3001 batches 	Training Loss: 0.284148
Trained 3051 batches 	Training Loss: 0.165369
Trained 3101 batches 	Training Loss: 0.185571
Trained 3151 batches 	Training Loss: 0.132968
Trained 3201 batches 	Training Loss: 0.271272
Trained 3251 batches 	Training Loss: 0.135618
Trained 3301 batches 	Training Loss: 0.211183
Trained 3351 batches 	Training Loss: 0.103732
Trained 3401 batches 	Training Loss: 0.197094
Trained 3451 batches 	Training Loss: 0.132759
Trained 3501 batches 	Training Loss: 0.198427
Trained 3551 batches 	Training Loss: 0.293623
Trained 3601 batches 	Training Loss: 0.160226
Trained 3651 batches 	Training Loss: 0.148391
Trained 3701 batches 	Training Loss: 0.253594
Trained 3751 batches 	Training Loss: 0.114187
Trained 3801 batches 	Training Loss: 0.229190
Trained 3851 batches 	Training Loss: 0.199926
Trained 3901 batches 	Training Loss: 0.156511
Trained 3951 batches 	Training Loss: 0.155172
Trained 4001 batches 	Training Loss: 0.283983
Trained 4051 batches 	Training Loss: 0.239195
Trained 4101 batches 	Training Loss: 0.175098
Trained 4151 batches 	Training Loss: 0.079348
Trained 4201 batches 	Training Loss: 0.126771
Trained 4251 batches 	Training Loss: 0.324086
Trained 4301 batches 	Training Loss: 0.144237
Trained 4351 batches 	Training Loss: 0.200609
Trained 4401 batches 	Training Loss: 0.207873
Trained 4451 batches 	Training Loss: 0.091869
Trained 4501 batches 	Training Loss: 0.143425
Trained 4551 batches 	Training Loss: 0.161712
Trained 4601 batches 	Training Loss: 0.181890
Trained 4651 batches 	Training Loss: 0.103092
Trained 4701 batches 	Training Loss: 0.171362
Trained 4751 batches 	Training Loss: 0.200290
Trained 4801 batches 	Training Loss: 0.193061
Trained 4851 batches 	Training Loss: 0.219504
Trained 4901 batches 	Training Loss: 0.095882
Trained 4951 batches 	Training Loss: 0.097795
Trained 5001 batches 	Training Loss: 0.090790
Trained 5051 batches 	Training Loss: 0.140849
Trained 5101 batches 	Training Loss: 0.115993
Trained 5151 batches 	Training Loss: 0.171230
Trained 5201 batches 	Training Loss: 0.138105
Trained 5251 batches 	Training Loss: 0.156195
Trained 5301 batches 	Training Loss: 0.066879
Trained 5351 batches 	Training Loss: 0.172780
Trained 5401 batches 	Training Loss: 0.101049
Trained 5451 batches 	Training Loss: 0.159735
Trained 5501 batches 	Training Loss: 0.099978
Trained 5551 batches 	Training Loss: 0.255263
Trained 5601 batches 	Training Loss: 0.138773
Trained 5651 batches 	Training Loss: 0.271457
Trained 5701 batches 	Training Loss: 0.112545
Trained 5751 batches 	Training Loss: 0.214402
Trained 5801 batches 	Training Loss: 0.290470
Trained 5851 batches 	Training Loss: 0.111313
Trained 5901 batches 	Training Loss: 0.290870
Trained 5951 batches 	Training Loss: 0.086598
Trained 6001 batches 	Training Loss: 0.159509
Trained 6051 batches 	Training Loss: 0.140466
Trained 6101 batches 	Training Loss: 0.327059
Trained 6151 batches 	Training Loss: 0.116518
Trained 6201 batches 	Training Loss: 0.124815
Trained 6251 batches 	Training Loss: 0.228660
Trained 6301 batches 	Training Loss: 0.202343
Trained 6351 batches 	Training Loss: 0.103417
Trained 6401 batches 	Training Loss: 0.242322
Trained 6451 batches 	Training Loss: 0.231439
Trained 6501 batches 	Training Loss: 0.259014
Trained 6551 batches 	Training Loss: 0.246595
Trained 6601 batches 	Training Loss: 0.118284
Trained 6651 batches 	Training Loss: 0.192158
Trained 6701 batches 	Training Loss: 0.096261
Trained 6751 batches 	Training Loss: 0.175588
Trained 6801 batches 	Training Loss: 0.135853
Trained 6851 batches 	Training Loss: 0.126946
Trained 6901 batches 	Training Loss: 0.114319
Trained 6951 batches 	Training Loss: 0.225729
Trained 7001 batches 	Training Loss: 0.156293
Trained 7051 batches 	Training Loss: 0.260857
Trained 7101 batches 	Training Loss: 0.312057
Trained 7151 batches 	Training Loss: 0.100874
Trained 7201 batches 	Training Loss: 0.172118
Trained 7251 batches 	Training Loss: 0.091128
Trained 7301 batches 	Training Loss: 0.123903
Trained 7351 batches 	Training Loss: 0.153333
Trained 7401 batches 	Training Loss: 0.138092
Trained 7451 batches 	Training Loss: 0.169159
Trained 7501 batches 	Training Loss: 0.165405
Trained 7551 batches 	Training Loss: 0.085004
Trained 7601 batches 	Training Loss: 0.257782
Trained 7651 batches 	Training Loss: 0.163208
Trained 7701 batches 	Training Loss: 0.289914
Trained 7751 batches 	Training Loss: 0.158344
Trained 7801 batches 	Training Loss: 0.195377
Trained 7851 batches 	Training Loss: 0.158406
Trained 7901 batches 	Training Loss: 0.110029
Trained 7951 batches 	Training Loss: 0.043564
Trained 8001 batches 	Training Loss: 0.229660
Trained 8051 batches 	Training Loss: 0.073895
Trained 8101 batches 	Training Loss: 0.092285
Trained 8151 batches 	Training Loss: 0.244839
Trained 8201 batches 	Training Loss: 0.305988
Trained 8251 batches 	Training Loss: 0.319778
Trained 8301 batches 	Training Loss: 0.161880
Trained 8351 batches 	Training Loss: 0.175348
Trained 8401 batches 	Training Loss: 0.215427
Trained 8451 batches 	Training Loss: 0.114792
Trained 8501 batches 	Training Loss: 0.232999
Trained 8551 batches 	Training Loss: 0.182305
Trained 8601 batches 	Training Loss: 0.146308
Trained 8651 batches 	Training Loss: 0.227991
Trained 8701 batches 	Training Loss: 0.154987
Trained 8751 batches 	Training Loss: 0.135925
Trained 8801 batches 	Training Loss: 0.217945
Trained 8851 batches 	Training Loss: 0.108815
Trained 8901 batches 	Training Loss: 0.150337
Trained 8951 batches 	Training Loss: 0.236117
Trained 9001 batches 	Training Loss: 0.169904
Trained 9051 batches 	Training Loss: 0.146610
Trained 9101 batches 	Training Loss: 0.100472
Trained 9151 batches 	Training Loss: 0.250070
Trained 9201 batches 	Training Loss: 0.188819
Trained 9251 batches 	Training Loss: 0.101337
Trained 9301 batches 	Training Loss: 0.206611
Trained 9351 batches 	Training Loss: 0.111300
Trained 9401 batches 	Training Loss: 0.159517
Trained 9451 batches 	Training Loss: 0.118452
Trained 9501 batches 	Training Loss: 0.144044
Trained 9551 batches 	Training Loss: 0.276753
Trained 9601 batches 	Training Loss: 0.137415
Trained 9651 batches 	Training Loss: 0.196357
Trained 9701 batches 	Training Loss: 0.168090
Trained 9751 batches 	Training Loss: 0.301753
Trained 9801 batches 	Training Loss: 0.214062
Epoch: 2 	Training Loss: 0.172709
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 1402
Started training, total epoch : 2
Training data size: 1227
Started epoch 1
Trained 1 batches 	Training Loss: 0.670141
Trained 51 batches 	Training Loss: 0.177121
Trained 101 batches 	Training Loss: 0.181238
Trained 151 batches 	Training Loss: 0.167617
Trained 201 batches 	Training Loss: 0.173532
Trained 251 batches 	Training Loss: 0.186787
Trained 301 batches 	Training Loss: 0.124920
Trained 351 batches 	Training Loss: 0.193527
Trained 401 batches 	Training Loss: 0.150745
Trained 451 batches 	Training Loss: 0.160007
Trained 501 batches 	Training Loss: 0.175652
Trained 551 batches 	Training Loss: 0.205854
Trained 601 batches 	Training Loss: 0.156042
Trained 651 batches 	Training Loss: 0.178826
Trained 701 batches 	Training Loss: 0.168918
Trained 751 batches 	Training Loss: 0.157715
Trained 801 batches 	Training Loss: 0.174058
Trained 851 batches 	Training Loss: 0.156693
Trained 901 batches 	Training Loss: 0.192081
Trained 951 batches 	Training Loss: 0.132065
Trained 1001 batches 	Training Loss: 0.163224
Trained 1051 batches 	Training Loss: 0.155765
Trained 1101 batches 	Training Loss: 0.171305
Trained 1151 batches 	Training Loss: 0.177375
Trained 1201 batches 	Training Loss: 0.161363
Epoch: 1 	Training Loss: 0.165737
Started epoch 2
Trained 1 batches 	Training Loss: 0.162431
Trained 51 batches 	Training Loss: 0.164643
Trained 101 batches 	Training Loss: 0.147361
Trained 151 batches 	Training Loss: 0.123067
Trained 201 batches 	Training Loss: 0.149490
Trained 251 batches 	Training Loss: 0.152009
Trained 301 batches 	Training Loss: 0.188956
Trained 351 batches 	Training Loss: 0.141428
Trained 401 batches 	Training Loss: 0.152729
Trained 451 batches 	Training Loss: 0.123949
Trained 501 batches 	Training Loss: 0.173152
Trained 551 batches 	Training Loss: 0.140300
Trained 601 batches 	Training Loss: 0.150080
Trained 651 batches 	Training Loss: 0.167470
Trained 701 batches 	Training Loss: 0.129171
Trained 751 batches 	Training Loss: 0.119292
Trained 801 batches 	Training Loss: 0.157531
Trained 851 batches 	Training Loss: 0.151811
Trained 901 batches 	Training Loss: 0.189581
Trained 951 batches 	Training Loss: 0.127515
Trained 1001 batches 	Training Loss: 0.135745
Trained 1051 batches 	Training Loss: 0.164973
Trained 1101 batches 	Training Loss: 0.171534
Trained 1151 batches 	Training Loss: 0.127719
Trained 1201 batches 	Training Loss: 0.155653
Epoch: 2 	Training Loss: 0.156179
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 176
Started training, total epoch : 1
Training data size: 1227
Started epoch 1
Trained 1 batches 	Training Loss: 0.696923
Trained 51 batches 	Training Loss: 0.151774
Trained 101 batches 	Training Loss: 0.201338
Trained 151 batches 	Training Loss: 0.191462
Started training, total epoch : 1
Training data size: 1227
Started epoch 1
Trained 1 batches 	Training Loss: 0.806182
Trained 51 batches 	Training Loss: 0.170985
Trained 101 batches 	Training Loss: 0.209795
Trained 151 batches 	Training Loss: 0.170487
Trained 201 batches 	Training Loss: 0.174854
Trained 251 batches 	Training Loss: 0.157193
Trained 301 batches 	Training Loss: 0.161669
Trained 351 batches 	Training Loss: 0.154844
Trained 401 batches 	Training Loss: 0.168502
Trained 451 batches 	Training Loss: 0.161520
Trained 501 batches 	Training Loss: 0.205987
Trained 551 batches 	Training Loss: 0.142120
Trained 601 batches 	Training Loss: 0.122283
Trained 651 batches 	Training Loss: 0.147629
Trained 701 batches 	Training Loss: 0.165846
Trained 751 batches 	Training Loss: 0.150526
Trained 801 batches 	Training Loss: 0.185888
Trained 851 batches 	Training Loss: 0.164469
Trained 901 batches 	Training Loss: 0.149423
Trained 951 batches 	Training Loss: 0.152044
Trained 1001 batches 	Training Loss: 0.192215
Trained 1051 batches 	Training Loss: 0.170527
Trained 1101 batches 	Training Loss: 0.175428
Trained 1151 batches 	Training Loss: 0.146959
Trained 1201 batches 	Training Loss: 0.196979
Epoch: 1 	Training Loss: 0.165945
Training time lapse: 11.0 min
Evaluating test data...	 test_loader: 351
Evaluating time lapse: 1.0 min
The average AUROC is 0.764
The AUROC of Atelectasis is 0.7568664959906334
The AUROC of Cardiomegaly is 0.8539346864827381
The AUROC of Effusion is 0.8543821464782616
The AUROC of Infiltration is 0.673705400478776
The AUROC of Mass is 0.7682811943817189
The AUROC of Nodule is 0.676204694683712
The AUROC of Pneumonia is 0.7023778490523016
The AUROC of Pneumothorax is 0.8102823092919501
The AUROC of Consolidation is 0.7674378320725882
The AUROC of Edema is 0.8813829921216336
The AUROC of Emphysema is 0.7763626975917879
The AUROC of Fibrosis is 0.7369682171802739
The AUROC of Pleural_Thickening is 0.7451030662897488
The AUROC of Hernia is 0.6949145586014169
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.814195
Trained 51 batches 	Training Loss: 0.229856
Trained 101 batches 	Training Loss: 0.229751
Trained 151 batches 	Training Loss: 0.191545
Trained 201 batches 	Training Loss: 0.161453
Trained 251 batches 	Training Loss: 0.179762
Trained 301 batches 	Training Loss: 0.198238
Trained 351 batches 	Training Loss: 0.142917
Trained 401 batches 	Training Loss: 0.174319
Trained 451 batches 	Training Loss: 0.163617
Trained 501 batches 	Training Loss: 0.220027
Trained 551 batches 	Training Loss: 0.114502
Trained 601 batches 	Training Loss: 0.195540
Trained 651 batches 	Training Loss: 0.140722
Trained 701 batches 	Training Loss: 0.126165
Trained 751 batches 	Training Loss: 0.148250
Trained 801 batches 	Training Loss: 0.181796
Trained 851 batches 	Training Loss: 0.185496
Trained 901 batches 	Training Loss: 0.193718
Trained 951 batches 	Training Loss: 0.192226
Trained 1001 batches 	Training Loss: 0.134929
Trained 1051 batches 	Training Loss: 0.233457
Trained 1101 batches 	Training Loss: 0.241959
Trained 1151 batches 	Training Loss: 0.191061
Trained 1201 batches 	Training Loss: 0.224094
Trained 1251 batches 	Training Loss: 0.222286
Trained 1301 batches 	Training Loss: 0.214112
Trained 1351 batches 	Training Loss: 0.175588
Trained 1401 batches 	Training Loss: 0.218772
Trained 1451 batches 	Training Loss: 0.122523
Trained 1501 batches 	Training Loss: 0.255077
Trained 1551 batches 	Training Loss: 0.222288
Trained 1601 batches 	Training Loss: 0.231259
Trained 1651 batches 	Training Loss: 0.136949
Trained 1701 batches 	Training Loss: 0.119133
Trained 1751 batches 	Training Loss: 0.221473
Trained 1801 batches 	Training Loss: 0.189708
Trained 1851 batches 	Training Loss: 0.147699
Trained 1901 batches 	Training Loss: 0.138437
Trained 1951 batches 	Training Loss: 0.335563
Trained 2001 batches 	Training Loss: 0.135662
Trained 2051 batches 	Training Loss: 0.154942
Trained 2101 batches 	Training Loss: 0.084153
Trained 2151 batches 	Training Loss: 0.224519
Trained 2201 batches 	Training Loss: 0.225282
Trained 2251 batches 	Training Loss: 0.226655
Trained 2301 batches 	Training Loss: 0.147126
Trained 2351 batches 	Training Loss: 0.134998
Trained 2401 batches 	Training Loss: 0.176448
Trained 2451 batches 	Training Loss: 0.175681
Trained 2501 batches 	Training Loss: 0.146956
Trained 2551 batches 	Training Loss: 0.131296
Trained 2601 batches 	Training Loss: 0.230014
Trained 2651 batches 	Training Loss: 0.114360
Trained 2701 batches 	Training Loss: 0.199581
Trained 2751 batches 	Training Loss: 0.126772
Trained 2801 batches 	Training Loss: 0.199196
Trained 2851 batches 	Training Loss: 0.167777
Trained 2901 batches 	Training Loss: 0.144151
Trained 2951 batches 	Training Loss: 0.240648
Trained 3001 batches 	Training Loss: 0.135785
Trained 3051 batches 	Training Loss: 0.207368
Trained 3101 batches 	Training Loss: 0.177730
Trained 3151 batches 	Training Loss: 0.111969
Trained 3201 batches 	Training Loss: 0.115701
Trained 3251 batches 	Training Loss: 0.262262
Trained 3301 batches 	Training Loss: 0.106462
Trained 3351 batches 	Training Loss: 0.152554
Trained 3401 batches 	Training Loss: 0.224323
Trained 3451 batches 	Training Loss: 0.209937
Trained 3501 batches 	Training Loss: 0.182957
Trained 3551 batches 	Training Loss: 0.135020
Trained 3601 batches 	Training Loss: 0.176310
Trained 3651 batches 	Training Loss: 0.117539
Trained 3701 batches 	Training Loss: 0.130495
Trained 3751 batches 	Training Loss: 0.202856
Trained 3801 batches 	Training Loss: 0.232233
Trained 3851 batches 	Training Loss: 0.154077
Trained 3901 batches 	Training Loss: 0.169190
Trained 3951 batches 	Training Loss: 0.113529
Trained 4001 batches 	Training Loss: 0.171872
Trained 4051 batches 	Training Loss: 0.241518
Trained 4101 batches 	Training Loss: 0.169423
Trained 4151 batches 	Training Loss: 0.159870
Trained 4201 batches 	Training Loss: 0.122887
Trained 4251 batches 	Training Loss: 0.186596
Trained 4301 batches 	Training Loss: 0.372563
Trained 4351 batches 	Training Loss: 0.201961
Trained 4401 batches 	Training Loss: 0.181731
Trained 4451 batches 	Training Loss: 0.144004
Trained 4501 batches 	Training Loss: 0.111606
Trained 4551 batches 	Training Loss: 0.108871
Trained 4601 batches 	Training Loss: 0.113523
Trained 4651 batches 	Training Loss: 0.159604
Trained 4701 batches 	Training Loss: 0.184031
Trained 4751 batches 	Training Loss: 0.101701
Trained 4801 batches 	Training Loss: 0.169291
Trained 4851 batches 	Training Loss: 0.131265
Trained 4901 batches 	Training Loss: 0.191325
Epoch: 1 	Training Loss: 0.172262
Started epoch 2
Trained 1 batches 	Training Loss: 0.100478
Trained 51 batches 	Training Loss: 0.209700
Trained 101 batches 	Training Loss: 0.154189
Trained 151 batches 	Training Loss: 0.140334
Trained 201 batches 	Training Loss: 0.208894
Trained 251 batches 	Training Loss: 0.135317
Trained 301 batches 	Training Loss: 0.155647
Trained 351 batches 	Training Loss: 0.119719
Trained 401 batches 	Training Loss: 0.103320
Trained 451 batches 	Training Loss: 0.117769
Trained 501 batches 	Training Loss: 0.148017
Trained 551 batches 	Training Loss: 0.196935
Trained 601 batches 	Training Loss: 0.143872
Trained 651 batches 	Training Loss: 0.102289
Trained 701 batches 	Training Loss: 0.120705
Trained 751 batches 	Training Loss: 0.200117
Trained 801 batches 	Training Loss: 0.209058
Trained 851 batches 	Training Loss: 0.141950
Trained 901 batches 	Training Loss: 0.129803
Trained 951 batches 	Training Loss: 0.130532
Trained 1001 batches 	Training Loss: 0.237538
Trained 1051 batches 	Training Loss: 0.167871
Trained 1101 batches 	Training Loss: 0.095819
Trained 1151 batches 	Training Loss: 0.202511
Trained 1201 batches 	Training Loss: 0.168253
Trained 1251 batches 	Training Loss: 0.185120
Trained 1301 batches 	Training Loss: 0.221723
Trained 1351 batches 	Training Loss: 0.133493
Trained 1401 batches 	Training Loss: 0.165176
Trained 1451 batches 	Training Loss: 0.200106
Trained 1501 batches 	Training Loss: 0.161962
Trained 1551 batches 	Training Loss: 0.241019
Trained 1601 batches 	Training Loss: 0.131264
Trained 1651 batches 	Training Loss: 0.121446
Trained 1701 batches 	Training Loss: 0.172675
Trained 1751 batches 	Training Loss: 0.249030
Trained 1801 batches 	Training Loss: 0.177908
Trained 1851 batches 	Training Loss: 0.098741
Trained 1901 batches 	Training Loss: 0.252779
Trained 1951 batches 	Training Loss: 0.130404
Trained 2001 batches 	Training Loss: 0.120254
Trained 2051 batches 	Training Loss: 0.195024
Trained 2101 batches 	Training Loss: 0.181247
Trained 2151 batches 	Training Loss: 0.140530
Trained 2201 batches 	Training Loss: 0.153773
Trained 2251 batches 	Training Loss: 0.227585
Trained 2301 batches 	Training Loss: 0.126915
Trained 2351 batches 	Training Loss: 0.146840
Trained 2401 batches 	Training Loss: 0.155413
Trained 2451 batches 	Training Loss: 0.115895
Trained 2501 batches 	Training Loss: 0.202238
Trained 2551 batches 	Training Loss: 0.184432
Trained 2601 batches 	Training Loss: 0.087971
Trained 2651 batches 	Training Loss: 0.156388
Trained 2701 batches 	Training Loss: 0.223242
Trained 2751 batches 	Training Loss: 0.108805
Trained 2801 batches 	Training Loss: 0.209580
Trained 2851 batches 	Training Loss: 0.208174
Trained 2901 batches 	Training Loss: 0.176477
Trained 2951 batches 	Training Loss: 0.119253
Trained 3001 batches 	Training Loss: 0.177855
Trained 3051 batches 	Training Loss: 0.111818
Trained 3101 batches 	Training Loss: 0.109815
Trained 3151 batches 	Training Loss: 0.191082
Trained 3201 batches 	Training Loss: 0.144514
Trained 3251 batches 	Training Loss: 0.181493
Trained 3301 batches 	Training Loss: 0.289489
Trained 3351 batches 	Training Loss: 0.156655
Trained 3401 batches 	Training Loss: 0.135097
Trained 3451 batches 	Training Loss: 0.195831
Trained 3501 batches 	Training Loss: 0.212191
Trained 3551 batches 	Training Loss: 0.204604
Trained 3601 batches 	Training Loss: 0.110170
Trained 3651 batches 	Training Loss: 0.117757
Trained 3701 batches 	Training Loss: 0.159744
Trained 3751 batches 	Training Loss: 0.177745
Trained 3801 batches 	Training Loss: 0.075418
Trained 3851 batches 	Training Loss: 0.160851
Trained 3901 batches 	Training Loss: 0.209814
Trained 3951 batches 	Training Loss: 0.230114
Trained 4001 batches 	Training Loss: 0.142860
Trained 4051 batches 	Training Loss: 0.154028
Trained 4101 batches 	Training Loss: 0.201736
Trained 4151 batches 	Training Loss: 0.164366
Trained 4201 batches 	Training Loss: 0.176704
Trained 4251 batches 	Training Loss: 0.199991
Trained 4301 batches 	Training Loss: 0.200255
Trained 4351 batches 	Training Loss: 0.186043
Trained 4401 batches 	Training Loss: 0.262060
Trained 4451 batches 	Training Loss: 0.099224
Trained 4501 batches 	Training Loss: 0.145842
Trained 4551 batches 	Training Loss: 0.218811
Trained 4601 batches 	Training Loss: 0.131198
Trained 4651 batches 	Training Loss: 0.124124
Trained 4701 batches 	Training Loss: 0.225609
Trained 4751 batches 	Training Loss: 0.127353
Trained 4801 batches 	Training Loss: 0.144958
Trained 4851 batches 	Training Loss: 0.140579
Trained 4901 batches 	Training Loss: 0.191976
Epoch: 2 	Training Loss: 0.163616
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.793014
Trained 51 batches 	Training Loss: 0.154519
Trained 101 batches 	Training Loss: 0.120884
Trained 151 batches 	Training Loss: 0.178188
Trained 201 batches 	Training Loss: 0.113764
Trained 251 batches 	Training Loss: 0.174918
Trained 301 batches 	Training Loss: 0.141122
Trained 351 batches 	Training Loss: 0.137027
Trained 401 batches 	Training Loss: 0.189848
Trained 451 batches 	Training Loss: 0.219359
Trained 501 batches 	Training Loss: 0.177481
Trained 551 batches 	Training Loss: 0.209279
Trained 601 batches 	Training Loss: 0.217189
Trained 651 batches 	Training Loss: 0.211429
Trained 701 batches 	Training Loss: 0.237289
Trained 751 batches 	Training Loss: 0.183446
Trained 801 batches 	Training Loss: 0.122601
Trained 851 batches 	Training Loss: 0.117910
Trained 901 batches 	Training Loss: 0.127178
Trained 951 batches 	Training Loss: 0.098417
Trained 1001 batches 	Training Loss: 0.166362
Trained 1051 batches 	Training Loss: 0.259437
Trained 1101 batches 	Training Loss: 0.128488
Trained 1151 batches 	Training Loss: 0.164668
Trained 1201 batches 	Training Loss: 0.221412
Trained 1251 batches 	Training Loss: 0.168103
Trained 1301 batches 	Training Loss: 0.121743
Trained 1351 batches 	Training Loss: 0.239254
Trained 1401 batches 	Training Loss: 0.134777
Trained 1451 batches 	Training Loss: 0.254968
Trained 1501 batches 	Training Loss: 0.132323
Trained 1551 batches 	Training Loss: 0.151257
Trained 1601 batches 	Training Loss: 0.176637
Trained 1651 batches 	Training Loss: 0.118673
Trained 1701 batches 	Training Loss: 0.217857
Trained 1751 batches 	Training Loss: 0.172962
Trained 1801 batches 	Training Loss: 0.145268
Trained 1851 batches 	Training Loss: 0.145400
Trained 1901 batches 	Training Loss: 0.121164
Trained 1951 batches 	Training Loss: 0.162581
Trained 2001 batches 	Training Loss: 0.222999
Trained 2051 batches 	Training Loss: 0.115688
Trained 2101 batches 	Training Loss: 0.172329
Trained 2151 batches 	Training Loss: 0.183204
Trained 2201 batches 	Training Loss: 0.190989
Trained 2251 batches 	Training Loss: 0.152998
Trained 2301 batches 	Training Loss: 0.168200
Trained 2351 batches 	Training Loss: 0.174045
Trained 2401 batches 	Training Loss: 0.174843
Trained 2451 batches 	Training Loss: 0.113821
Trained 2501 batches 	Training Loss: 0.171512
Trained 2551 batches 	Training Loss: 0.130306
Trained 2601 batches 	Training Loss: 0.170569
Trained 2651 batches 	Training Loss: 0.191824
Trained 2701 batches 	Training Loss: 0.195627
Trained 2751 batches 	Training Loss: 0.187979
Trained 2801 batches 	Training Loss: 0.165803
Trained 2851 batches 	Training Loss: 0.168315
Trained 2901 batches 	Training Loss: 0.119107
Trained 2951 batches 	Training Loss: 0.319492
Trained 3001 batches 	Training Loss: 0.128857
Trained 3051 batches 	Training Loss: 0.170789
Trained 3101 batches 	Training Loss: 0.143446
Trained 3151 batches 	Training Loss: 0.217184
Trained 3201 batches 	Training Loss: 0.187453
Trained 3251 batches 	Training Loss: 0.201281
Trained 3301 batches 	Training Loss: 0.149276
Trained 3351 batches 	Training Loss: 0.255932
Trained 3401 batches 	Training Loss: 0.191752
Trained 3451 batches 	Training Loss: 0.109116
Trained 3501 batches 	Training Loss: 0.128715
Trained 3551 batches 	Training Loss: 0.149010
Trained 3601 batches 	Training Loss: 0.146611
Trained 3651 batches 	Training Loss: 0.136964
Trained 3701 batches 	Training Loss: 0.173699
Trained 3751 batches 	Training Loss: 0.126469
Trained 3801 batches 	Training Loss: 0.193048
Trained 3851 batches 	Training Loss: 0.156247
Trained 3901 batches 	Training Loss: 0.142172
Trained 3951 batches 	Training Loss: 0.147184
Trained 4001 batches 	Training Loss: 0.176609
Trained 4051 batches 	Training Loss: 0.213187
Trained 4101 batches 	Training Loss: 0.139452
Trained 4151 batches 	Training Loss: 0.238274
Trained 4201 batches 	Training Loss: 0.152548
Trained 4251 batches 	Training Loss: 0.162185
Trained 4301 batches 	Training Loss: 0.206192
Trained 4351 batches 	Training Loss: 0.184537
Trained 4401 batches 	Training Loss: 0.174766
Trained 4451 batches 	Training Loss: 0.192935
Trained 4501 batches 	Training Loss: 0.245929
Trained 4551 batches 	Training Loss: 0.134885
Trained 4601 batches 	Training Loss: 0.171283
Trained 4651 batches 	Training Loss: 0.163770
Trained 4701 batches 	Training Loss: 0.165093
Trained 4751 batches 	Training Loss: 0.144116
Trained 4801 batches 	Training Loss: 0.166845
Trained 4851 batches 	Training Loss: 0.137801
Trained 4901 batches 	Training Loss: 0.187840
Epoch: 1 	Training Loss: 0.171109
Started epoch 2
Trained 1 batches 	Training Loss: 0.186886
Trained 51 batches 	Training Loss: 0.150209
Trained 101 batches 	Training Loss: 0.145331
Trained 151 batches 	Training Loss: 0.131463
Trained 201 batches 	Training Loss: 0.159849
Trained 251 batches 	Training Loss: 0.206980
Trained 301 batches 	Training Loss: 0.112617
Trained 351 batches 	Training Loss: 0.132734
Trained 401 batches 	Training Loss: 0.179337
Trained 451 batches 	Training Loss: 0.155610
Trained 501 batches 	Training Loss: 0.118280
Trained 551 batches 	Training Loss: 0.148439
Trained 601 batches 	Training Loss: 0.080432
Trained 651 batches 	Training Loss: 0.204327
Trained 701 batches 	Training Loss: 0.171506
Trained 751 batches 	Training Loss: 0.193819
Trained 801 batches 	Training Loss: 0.149789
Trained 851 batches 	Training Loss: 0.162215
Trained 901 batches 	Training Loss: 0.189381
Trained 951 batches 	Training Loss: 0.177879
Trained 1001 batches 	Training Loss: 0.168897
Trained 1051 batches 	Training Loss: 0.174841
Trained 1101 batches 	Training Loss: 0.197404
Trained 1151 batches 	Training Loss: 0.195181
Trained 1201 batches 	Training Loss: 0.226398
Trained 1251 batches 	Training Loss: 0.193465
Trained 1301 batches 	Training Loss: 0.109808
Trained 1351 batches 	Training Loss: 0.202456
Trained 1401 batches 	Training Loss: 0.234519
Trained 1451 batches 	Training Loss: 0.112554
Trained 1501 batches 	Training Loss: 0.099845
Trained 1551 batches 	Training Loss: 0.109414
Trained 1601 batches 	Training Loss: 0.098694
Trained 1651 batches 	Training Loss: 0.189766
Trained 1701 batches 	Training Loss: 0.113373
Trained 1751 batches 	Training Loss: 0.205878
Trained 1801 batches 	Training Loss: 0.156258
Trained 1851 batches 	Training Loss: 0.100109
Trained 1901 batches 	Training Loss: 0.175561
Trained 1951 batches 	Training Loss: 0.181394
Trained 2001 batches 	Training Loss: 0.137704
Trained 2051 batches 	Training Loss: 0.169670
Trained 2101 batches 	Training Loss: 0.177403
Trained 2151 batches 	Training Loss: 0.209402
Trained 2201 batches 	Training Loss: 0.199007
Trained 2251 batches 	Training Loss: 0.113685
Trained 2301 batches 	Training Loss: 0.121990
Trained 2351 batches 	Training Loss: 0.130317
Trained 2401 batches 	Training Loss: 0.256676
Trained 2451 batches 	Training Loss: 0.128417
Trained 2501 batches 	Training Loss: 0.114768
Trained 2551 batches 	Training Loss: 0.106833
Trained 2601 batches 	Training Loss: 0.104386
Trained 2651 batches 	Training Loss: 0.131148
Trained 2701 batches 	Training Loss: 0.213513
Trained 2751 batches 	Training Loss: 0.178578
Trained 2801 batches 	Training Loss: 0.218580
Trained 2851 batches 	Training Loss: 0.160911
Trained 2901 batches 	Training Loss: 0.164217
Trained 2951 batches 	Training Loss: 0.212085
Trained 3001 batches 	Training Loss: 0.142495
Trained 3051 batches 	Training Loss: 0.172945
Trained 3101 batches 	Training Loss: 0.135190
Trained 3151 batches 	Training Loss: 0.141781
Trained 3201 batches 	Training Loss: 0.194615
Trained 3251 batches 	Training Loss: 0.222860
Trained 3301 batches 	Training Loss: 0.169356
Trained 3351 batches 	Training Loss: 0.230676
Trained 3401 batches 	Training Loss: 0.195641
Trained 3451 batches 	Training Loss: 0.098071
Trained 3501 batches 	Training Loss: 0.195597
Trained 3551 batches 	Training Loss: 0.095756
Trained 3601 batches 	Training Loss: 0.147247
Trained 3651 batches 	Training Loss: 0.129566
Trained 3701 batches 	Training Loss: 0.111750
Trained 3751 batches 	Training Loss: 0.101120
Trained 3801 batches 	Training Loss: 0.211745
Trained 3851 batches 	Training Loss: 0.194315
Trained 3901 batches 	Training Loss: 0.210272
Trained 3951 batches 	Training Loss: 0.154300
Trained 4001 batches 	Training Loss: 0.163228
Trained 4051 batches 	Training Loss: 0.094906
Trained 4101 batches 	Training Loss: 0.165403
Trained 4151 batches 	Training Loss: 0.146906
Trained 4201 batches 	Training Loss: 0.166937
Trained 4251 batches 	Training Loss: 0.130712
Trained 4301 batches 	Training Loss: 0.120478
Trained 4351 batches 	Training Loss: 0.245268
Trained 4401 batches 	Training Loss: 0.179719
Trained 4451 batches 	Training Loss: 0.129338
Trained 4501 batches 	Training Loss: 0.146556
Trained 4551 batches 	Training Loss: 0.165587
Trained 4601 batches 	Training Loss: 0.106221
Trained 4651 batches 	Training Loss: 0.165049
Trained 4701 batches 	Training Loss: 0.192359
Trained 4751 batches 	Training Loss: 0.149161
Trained 4801 batches 	Training Loss: 0.173738
Trained 4851 batches 	Training Loss: 0.132375
Trained 4901 batches 	Training Loss: 0.302046
Epoch: 2 	Training Loss: 0.163141
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.770539
Trained 51 batches 	Training Loss: 0.135722
Trained 101 batches 	Training Loss: 0.194214
Trained 151 batches 	Training Loss: 0.103526
Trained 201 batches 	Training Loss: 0.157178
Trained 251 batches 	Training Loss: 0.113857
Trained 301 batches 	Training Loss: 0.176453
Trained 351 batches 	Training Loss: 0.193604
Trained 401 batches 	Training Loss: 0.259609
Trained 451 batches 	Training Loss: 0.146877
Trained 501 batches 	Training Loss: 0.193737
Trained 551 batches 	Training Loss: 0.117062
Trained 601 batches 	Training Loss: 0.135427
Trained 651 batches 	Training Loss: 0.201060
Trained 701 batches 	Training Loss: 0.135032
Trained 751 batches 	Training Loss: 0.188897
Trained 801 batches 	Training Loss: 0.153644
Trained 851 batches 	Training Loss: 0.171142
Trained 901 batches 	Training Loss: 0.134713
Trained 951 batches 	Training Loss: 0.207350
Trained 1001 batches 	Training Loss: 0.118686
Trained 1051 batches 	Training Loss: 0.267045
Trained 1101 batches 	Training Loss: 0.226703
Trained 1151 batches 	Training Loss: 0.307871
Trained 1201 batches 	Training Loss: 0.194576
Trained 1251 batches 	Training Loss: 0.159201
Trained 1301 batches 	Training Loss: 0.179961
Trained 1351 batches 	Training Loss: 0.309373
Trained 1401 batches 	Training Loss: 0.172977
Trained 1451 batches 	Training Loss: 0.168019
Trained 1501 batches 	Training Loss: 0.186283
Trained 1551 batches 	Training Loss: 0.219003
Trained 1601 batches 	Training Loss: 0.110289
Trained 1651 batches 	Training Loss: 0.183265
Trained 1701 batches 	Training Loss: 0.150544
Trained 1751 batches 	Training Loss: 0.134123
Trained 1801 batches 	Training Loss: 0.157549
Trained 1851 batches 	Training Loss: 0.152628
Trained 1901 batches 	Training Loss: 0.140418
Trained 1951 batches 	Training Loss: 0.165282
Trained 2001 batches 	Training Loss: 0.161307
Trained 2051 batches 	Training Loss: 0.171518
Trained 2101 batches 	Training Loss: 0.200272
Trained 2151 batches 	Training Loss: 0.130246
Trained 2201 batches 	Training Loss: 0.143344
Trained 2251 batches 	Training Loss: 0.115157
Trained 2301 batches 	Training Loss: 0.143176
Trained 2351 batches 	Training Loss: 0.164677
Trained 2401 batches 	Training Loss: 0.176749
Trained 2451 batches 	Training Loss: 0.106428
Trained 2501 batches 	Training Loss: 0.139875
Trained 2551 batches 	Training Loss: 0.261679
Trained 2601 batches 	Training Loss: 0.138198
Trained 2651 batches 	Training Loss: 0.159871
Trained 2701 batches 	Training Loss: 0.215304
Trained 2751 batches 	Training Loss: 0.166842
Trained 2801 batches 	Training Loss: 0.207632
Trained 2851 batches 	Training Loss: 0.165114
Trained 2901 batches 	Training Loss: 0.177975
Trained 2951 batches 	Training Loss: 0.133991
Trained 3001 batches 	Training Loss: 0.149277
Trained 3051 batches 	Training Loss: 0.195535
Trained 3101 batches 	Training Loss: 0.185677
Trained 3151 batches 	Training Loss: 0.115208
Trained 3201 batches 	Training Loss: 0.160942
Trained 3251 batches 	Training Loss: 0.175303
Trained 3301 batches 	Training Loss: 0.228338
Trained 3351 batches 	Training Loss: 0.185065
Trained 3401 batches 	Training Loss: 0.248896
Trained 3451 batches 	Training Loss: 0.151089
Trained 3501 batches 	Training Loss: 0.177319
Trained 3551 batches 	Training Loss: 0.158446
Trained 3601 batches 	Training Loss: 0.130581
Trained 3651 batches 	Training Loss: 0.234975
Trained 3701 batches 	Training Loss: 0.227316
Trained 3751 batches 	Training Loss: 0.153917
Trained 3801 batches 	Training Loss: 0.183229
Trained 3851 batches 	Training Loss: 0.142575
Trained 3901 batches 	Training Loss: 0.357977
Trained 3951 batches 	Training Loss: 0.267104
Trained 4001 batches 	Training Loss: 0.203862
Trained 4051 batches 	Training Loss: 0.141973
Trained 4101 batches 	Training Loss: 0.105579
Trained 4151 batches 	Training Loss: 0.172655
Trained 4201 batches 	Training Loss: 0.238683
Trained 4251 batches 	Training Loss: 0.166472
Trained 4301 batches 	Training Loss: 0.162433
Trained 4351 batches 	Training Loss: 0.185078
Trained 4401 batches 	Training Loss: 0.165677
Trained 4451 batches 	Training Loss: 0.157676
Trained 4501 batches 	Training Loss: 0.178877
Trained 4551 batches 	Training Loss: 0.175780
Trained 4601 batches 	Training Loss: 0.259150
Trained 4651 batches 	Training Loss: 0.138226
Trained 4701 batches 	Training Loss: 0.146825
Trained 4751 batches 	Training Loss: 0.161882
Trained 4801 batches 	Training Loss: 0.239744
Trained 4851 batches 	Training Loss: 0.138933
Trained 4901 batches 	Training Loss: 0.205251
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.702521
Trained 51 batches 	Training Loss: 0.262678
Trained 101 batches 	Training Loss: 0.185262
Trained 151 batches 	Training Loss: 0.226704
Trained 201 batches 	Training Loss: 0.170330
Trained 251 batches 	Training Loss: 0.192387
Trained 301 batches 	Training Loss: 0.142737
Trained 351 batches 	Training Loss: 0.113862
Trained 401 batches 	Training Loss: 0.223963
Trained 451 batches 	Training Loss: 0.151809
Trained 501 batches 	Training Loss: 0.171153
Trained 551 batches 	Training Loss: 0.163454
Trained 601 batches 	Training Loss: 0.107635
Trained 651 batches 	Training Loss: 0.130838
Trained 701 batches 	Training Loss: 0.207134
Trained 751 batches 	Training Loss: 0.132359
Trained 801 batches 	Training Loss: 0.233076
Trained 851 batches 	Training Loss: 0.206774
Trained 901 batches 	Training Loss: 0.225220
Trained 951 batches 	Training Loss: 0.146523
Trained 1001 batches 	Training Loss: 0.128495
Trained 1051 batches 	Training Loss: 0.199427
Trained 1101 batches 	Training Loss: 0.111929
Trained 1151 batches 	Training Loss: 0.264050
Trained 1201 batches 	Training Loss: 0.132693
Trained 1251 batches 	Training Loss: 0.162356
Trained 1301 batches 	Training Loss: 0.138272
Trained 1351 batches 	Training Loss: 0.190308
Trained 1401 batches 	Training Loss: 0.126851
Trained 1451 batches 	Training Loss: 0.187713
Trained 1501 batches 	Training Loss: 0.265221
Trained 1551 batches 	Training Loss: 0.180569
Trained 1601 batches 	Training Loss: 0.220492
Trained 1651 batches 	Training Loss: 0.202267
Trained 1701 batches 	Training Loss: 0.126809
Trained 1751 batches 	Training Loss: 0.304470
Trained 1801 batches 	Training Loss: 0.186108
Trained 1851 batches 	Training Loss: 0.199148
Trained 1901 batches 	Training Loss: 0.157429
Trained 1951 batches 	Training Loss: 0.242498
Trained 2001 batches 	Training Loss: 0.146608
Trained 2051 batches 	Training Loss: 0.148936
Trained 2101 batches 	Training Loss: 0.189059
Trained 2151 batches 	Training Loss: 0.116453
Trained 2201 batches 	Training Loss: 0.216152
Trained 2251 batches 	Training Loss: 0.174868
Trained 2301 batches 	Training Loss: 0.180588
Trained 2351 batches 	Training Loss: 0.209180
Trained 2401 batches 	Training Loss: 0.294432
Trained 2451 batches 	Training Loss: 0.120101
Trained 2501 batches 	Training Loss: 0.148779
Trained 2551 batches 	Training Loss: 0.172372
Trained 2601 batches 	Training Loss: 0.136814
Trained 2651 batches 	Training Loss: 0.204985
Trained 2701 batches 	Training Loss: 0.152493
Trained 2751 batches 	Training Loss: 0.135635
Trained 2801 batches 	Training Loss: 0.131163
Trained 2851 batches 	Training Loss: 0.137563
Trained 2901 batches 	Training Loss: 0.199962
Trained 2951 batches 	Training Loss: 0.134245
Trained 3001 batches 	Training Loss: 0.174278
Trained 3051 batches 	Training Loss: 0.149759
Trained 3101 batches 	Training Loss: 0.091317
Trained 3151 batches 	Training Loss: 0.105318
Trained 3201 batches 	Training Loss: 0.189148
Trained 3251 batches 	Training Loss: 0.130892
Trained 3301 batches 	Training Loss: 0.260019
Trained 3351 batches 	Training Loss: 0.170063
Trained 3401 batches 	Training Loss: 0.192412
Trained 3451 batches 	Training Loss: 0.158326
Trained 3501 batches 	Training Loss: 0.202156
Trained 3551 batches 	Training Loss: 0.114327
Trained 3601 batches 	Training Loss: 0.107700
Trained 3651 batches 	Training Loss: 0.170874
Trained 3701 batches 	Training Loss: 0.140229
Trained 3751 batches 	Training Loss: 0.154423
Trained 3801 batches 	Training Loss: 0.189956
Trained 3851 batches 	Training Loss: 0.126149
Trained 3901 batches 	Training Loss: 0.160984
Trained 3951 batches 	Training Loss: 0.204289
Trained 4001 batches 	Training Loss: 0.141493
Trained 4051 batches 	Training Loss: 0.124041
Trained 4101 batches 	Training Loss: 0.149740
Trained 4151 batches 	Training Loss: 0.189765
Trained 4201 batches 	Training Loss: 0.192216
Trained 4251 batches 	Training Loss: 0.163078
Trained 4301 batches 	Training Loss: 0.108075
Trained 4351 batches 	Training Loss: 0.170822
Trained 4401 batches 	Training Loss: 0.147417
Trained 4451 batches 	Training Loss: 0.163920
Trained 4501 batches 	Training Loss: 0.212415
Trained 4551 batches 	Training Loss: 0.147928
Trained 4601 batches 	Training Loss: 0.134881
Trained 4651 batches 	Training Loss: 0.166083
Trained 4701 batches 	Training Loss: 0.092116
Trained 4751 batches 	Training Loss: 0.138248
Trained 4801 batches 	Training Loss: 0.139774
Trained 4851 batches 	Training Loss: 0.182538
Trained 4901 batches 	Training Loss: 0.118943
Epoch: 1 	Training Loss: 0.172091
Started epoch 2
Trained 1 batches 	Training Loss: 0.118576
Trained 51 batches 	Training Loss: 0.128339
Trained 101 batches 	Training Loss: 0.083548
Trained 151 batches 	Training Loss: 0.219086
Trained 201 batches 	Training Loss: 0.154464
Trained 251 batches 	Training Loss: 0.205033
Trained 301 batches 	Training Loss: 0.108616
Trained 351 batches 	Training Loss: 0.141683
Trained 401 batches 	Training Loss: 0.096773
Trained 451 batches 	Training Loss: 0.097977
Trained 501 batches 	Training Loss: 0.150512
Trained 551 batches 	Training Loss: 0.161924
Trained 601 batches 	Training Loss: 0.132885
Trained 651 batches 	Training Loss: 0.196002
Trained 701 batches 	Training Loss: 0.091389
Trained 751 batches 	Training Loss: 0.139090
Trained 801 batches 	Training Loss: 0.156149
Trained 851 batches 	Training Loss: 0.085145
Trained 901 batches 	Training Loss: 0.156957
Trained 951 batches 	Training Loss: 0.183802
Trained 1001 batches 	Training Loss: 0.136933
Trained 1051 batches 	Training Loss: 0.161057
Trained 1101 batches 	Training Loss: 0.192629
Trained 1151 batches 	Training Loss: 0.209911
Trained 1201 batches 	Training Loss: 0.136949
Trained 1251 batches 	Training Loss: 0.148811
Trained 1301 batches 	Training Loss: 0.135647
Trained 1351 batches 	Training Loss: 0.169737
Trained 1401 batches 	Training Loss: 0.130365
Trained 1451 batches 	Training Loss: 0.148977
Trained 1501 batches 	Training Loss: 0.160815
Trained 1551 batches 	Training Loss: 0.129035
Trained 1601 batches 	Training Loss: 0.165203
Trained 1651 batches 	Training Loss: 0.189033
Trained 1701 batches 	Training Loss: 0.215878
Trained 1751 batches 	Training Loss: 0.193163
Trained 1801 batches 	Training Loss: 0.154661
Trained 1851 batches 	Training Loss: 0.109907
Trained 1901 batches 	Training Loss: 0.134222
Trained 1951 batches 	Training Loss: 0.151386
Trained 2001 batches 	Training Loss: 0.147732
Trained 2051 batches 	Training Loss: 0.171879
Trained 2101 batches 	Training Loss: 0.154410
Trained 2151 batches 	Training Loss: 0.170741
Trained 2201 batches 	Training Loss: 0.229617
Trained 2251 batches 	Training Loss: 0.146197
Trained 2301 batches 	Training Loss: 0.286699
Trained 2351 batches 	Training Loss: 0.191768
Trained 2401 batches 	Training Loss: 0.105725
Trained 2451 batches 	Training Loss: 0.163985
Trained 2501 batches 	Training Loss: 0.124564
Trained 2551 batches 	Training Loss: 0.243568
Trained 2601 batches 	Training Loss: 0.131683
Trained 2651 batches 	Training Loss: 0.200769
Trained 2701 batches 	Training Loss: 0.107191
Trained 2751 batches 	Training Loss: 0.126515
Trained 2801 batches 	Training Loss: 0.133610
Trained 2851 batches 	Training Loss: 0.188415
Trained 2901 batches 	Training Loss: 0.161182
Trained 2951 batches 	Training Loss: 0.134657
Trained 3001 batches 	Training Loss: 0.217469
Trained 3051 batches 	Training Loss: 0.134656
Trained 3101 batches 	Training Loss: 0.199725
Trained 3151 batches 	Training Loss: 0.137064
Trained 3201 batches 	Training Loss: 0.120815
Trained 3251 batches 	Training Loss: 0.135475
Trained 3301 batches 	Training Loss: 0.243993
Trained 3351 batches 	Training Loss: 0.162724
Trained 3401 batches 	Training Loss: 0.129931
Trained 3451 batches 	Training Loss: 0.122167
Trained 3501 batches 	Training Loss: 0.105487
Trained 3551 batches 	Training Loss: 0.139162
Trained 3601 batches 	Training Loss: 0.176428
Trained 3651 batches 	Training Loss: 0.143728
Trained 3701 batches 	Training Loss: 0.090141
Trained 3751 batches 	Training Loss: 0.203773
Trained 3801 batches 	Training Loss: 0.139235
Trained 3851 batches 	Training Loss: 0.198063
Trained 3901 batches 	Training Loss: 0.129008
Trained 3951 batches 	Training Loss: 0.168313
Trained 4001 batches 	Training Loss: 0.153968
Trained 4051 batches 	Training Loss: 0.091799
Trained 4101 batches 	Training Loss: 0.107267
Trained 4151 batches 	Training Loss: 0.157045
Trained 4201 batches 	Training Loss: 0.145492
Trained 4251 batches 	Training Loss: 0.149036
Trained 4301 batches 	Training Loss: 0.191288
Trained 4351 batches 	Training Loss: 0.146364
Trained 4401 batches 	Training Loss: 0.120869
Trained 4451 batches 	Training Loss: 0.110030
Trained 4501 batches 	Training Loss: 0.138987
Trained 4551 batches 	Training Loss: 0.147783
Trained 4601 batches 	Training Loss: 0.111272
Trained 4651 batches 	Training Loss: 0.154815
Trained 4701 batches 	Training Loss: 0.095838
Trained 4751 batches 	Training Loss: 0.202052
Trained 4801 batches 	Training Loss: 0.163892
Trained 4851 batches 	Training Loss: 0.125747
Trained 4901 batches 	Training Loss: 0.137729
Epoch: 2 	Training Loss: 0.163516
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.767
The AUROC of Atelectasis is 0.748233289309822
The AUROC of Cardiomegaly is 0.8738647331265441
The AUROC of Effusion is 0.8458451008732674
The AUROC of Infiltration is 0.6802207101461131
The AUROC of Mass is 0.7638639105143746
The AUROC of Nodule is 0.6939172192499247
The AUROC of Pneumonia is 0.7176382911093357
The AUROC of Pneumothorax is 0.7865122884400114
The AUROC of Consolidation is 0.7673893666466406
The AUROC of Edema is 0.8568345676036071
The AUROC of Emphysema is 0.7622586155584118
The AUROC of Fibrosis is 0.7247407662864154
The AUROC of Pleural_Thickening is 0.7132090713037113
The AUROC of Hernia is 0.8027922097263532
Training time lapse: 21.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.767
The AUROC of Atelectasis is 0.754115148004687
The AUROC of Cardiomegaly is 0.8773914186297467
The AUROC of Effusion is 0.8495095331325965
The AUROC of Infiltration is 0.6702639492152895
The AUROC of Mass is 0.7464197851592056
The AUROC of Nodule is 0.6743859705250796
The AUROC of Pneumonia is 0.699432773251005
The AUROC of Pneumothorax is 0.8067908109310631
The AUROC of Consolidation is 0.7788984978713986
The AUROC of Edema is 0.8741782195997114
The AUROC of Emphysema is 0.7845994040754536
The AUROC of Fibrosis is 0.7164974519336312
The AUROC of Pleural_Thickening is 0.7304745512829183
The AUROC of Hernia is 0.7695234690229327
Started training, total epoch : 2
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.778293
Trained 51 batches 	Training Loss: 0.160523
Trained 101 batches 	Training Loss: 0.125242
Trained 151 batches 	Training Loss: 0.264879
Trained 201 batches 	Training Loss: 0.104422
Trained 251 batches 	Training Loss: 0.186219
Trained 301 batches 	Training Loss: 0.186401
Trained 351 batches 	Training Loss: 0.136954
Trained 401 batches 	Training Loss: 0.110547
Trained 451 batches 	Training Loss: 0.142053
Trained 501 batches 	Training Loss: 0.214376
Trained 551 batches 	Training Loss: 0.120716
Trained 601 batches 	Training Loss: 0.171282
Trained 651 batches 	Training Loss: 0.165078
Trained 701 batches 	Training Loss: 0.182752
Trained 751 batches 	Training Loss: 0.256715
Trained 801 batches 	Training Loss: 0.201925
Trained 851 batches 	Training Loss: 0.181778
Trained 901 batches 	Training Loss: 0.171593
Trained 951 batches 	Training Loss: 0.190357
Trained 1001 batches 	Training Loss: 0.217775
Trained 1051 batches 	Training Loss: 0.133248
Trained 1101 batches 	Training Loss: 0.107681
Trained 1151 batches 	Training Loss: 0.112770
Trained 1201 batches 	Training Loss: 0.204524
Trained 1251 batches 	Training Loss: 0.195303
Trained 1301 batches 	Training Loss: 0.188462
Trained 1351 batches 	Training Loss: 0.224932
Trained 1401 batches 	Training Loss: 0.165555
Trained 1451 batches 	Training Loss: 0.137070
Trained 1501 batches 	Training Loss: 0.108167
Trained 1551 batches 	Training Loss: 0.116971
Trained 1601 batches 	Training Loss: 0.205687
Trained 1651 batches 	Training Loss: 0.249205
Trained 1701 batches 	Training Loss: 0.148320
Trained 1751 batches 	Training Loss: 0.234385
Trained 1801 batches 	Training Loss: 0.190225
Trained 1851 batches 	Training Loss: 0.184704
Trained 1901 batches 	Training Loss: 0.195738
Trained 1951 batches 	Training Loss: 0.105414
Trained 2001 batches 	Training Loss: 0.146261
Trained 2051 batches 	Training Loss: 0.196462
Trained 2101 batches 	Training Loss: 0.239145
Trained 2151 batches 	Training Loss: 0.113172
Trained 2201 batches 	Training Loss: 0.132951
Trained 2251 batches 	Training Loss: 0.196416
Trained 2301 batches 	Training Loss: 0.200106
Trained 2351 batches 	Training Loss: 0.186301
Trained 2401 batches 	Training Loss: 0.150413
Trained 2451 batches 	Training Loss: 0.148904
Trained 2501 batches 	Training Loss: 0.138102
Trained 2551 batches 	Training Loss: 0.187436
Trained 2601 batches 	Training Loss: 0.110218
Trained 2651 batches 	Training Loss: 0.196249
Trained 2701 batches 	Training Loss: 0.190630
Trained 2751 batches 	Training Loss: 0.141964
Trained 2801 batches 	Training Loss: 0.157510
Trained 2851 batches 	Training Loss: 0.103040
Trained 2901 batches 	Training Loss: 0.168836
Trained 2951 batches 	Training Loss: 0.168562
Trained 3001 batches 	Training Loss: 0.192494
Trained 3051 batches 	Training Loss: 0.221051
Trained 3101 batches 	Training Loss: 0.137815
Trained 3151 batches 	Training Loss: 0.264134
Trained 3201 batches 	Training Loss: 0.198319
Trained 3251 batches 	Training Loss: 0.191912
Trained 3301 batches 	Training Loss: 0.142706
Trained 3351 batches 	Training Loss: 0.193222
Trained 3401 batches 	Training Loss: 0.147141
Trained 3451 batches 	Training Loss: 0.239747
Trained 3501 batches 	Training Loss: 0.173595
Trained 3551 batches 	Training Loss: 0.246998
Trained 3601 batches 	Training Loss: 0.189659
Trained 3651 batches 	Training Loss: 0.232421
Trained 3701 batches 	Training Loss: 0.182013
Trained 3751 batches 	Training Loss: 0.139066
Trained 3801 batches 	Training Loss: 0.201538
Trained 3851 batches 	Training Loss: 0.178551
Trained 3901 batches 	Training Loss: 0.191735
Trained 3951 batches 	Training Loss: 0.164769
Trained 4001 batches 	Training Loss: 0.222008
Trained 4051 batches 	Training Loss: 0.158911
Trained 4101 batches 	Training Loss: 0.195088
Trained 4151 batches 	Training Loss: 0.296980
Trained 4201 batches 	Training Loss: 0.214523
Trained 4251 batches 	Training Loss: 0.206732
Trained 4301 batches 	Training Loss: 0.099021
Trained 4351 batches 	Training Loss: 0.098245
Trained 4401 batches 	Training Loss: 0.208289
Trained 4451 batches 	Training Loss: 0.176059
Trained 4501 batches 	Training Loss: 0.149921
Trained 4551 batches 	Training Loss: 0.181631
Trained 4601 batches 	Training Loss: 0.122835
Trained 4651 batches 	Training Loss: 0.183387
Trained 4701 batches 	Training Loss: 0.177052
Trained 4751 batches 	Training Loss: 0.252596
Trained 4801 batches 	Training Loss: 0.159573
Trained 4851 batches 	Training Loss: 0.093221
Trained 4901 batches 	Training Loss: 0.179189
Epoch: 1 	Training Loss: 0.175200
Started epoch 2
Trained 1 batches 	Training Loss: 0.195206
Trained 51 batches 	Training Loss: 0.112046
Trained 101 batches 	Training Loss: 0.191172
Trained 151 batches 	Training Loss: 0.115542
Trained 201 batches 	Training Loss: 0.130644
Trained 251 batches 	Training Loss: 0.136394
Trained 301 batches 	Training Loss: 0.279486
Trained 351 batches 	Training Loss: 0.168484
Trained 401 batches 	Training Loss: 0.130383
Trained 451 batches 	Training Loss: 0.193955
Trained 501 batches 	Training Loss: 0.100957
Trained 551 batches 	Training Loss: 0.204803
Trained 601 batches 	Training Loss: 0.127475
Trained 651 batches 	Training Loss: 0.120043
Trained 701 batches 	Training Loss: 0.095874
Trained 751 batches 	Training Loss: 0.212477
Trained 801 batches 	Training Loss: 0.188412
Trained 851 batches 	Training Loss: 0.122484
Trained 901 batches 	Training Loss: 0.188979
Trained 951 batches 	Training Loss: 0.162261
Trained 1001 batches 	Training Loss: 0.134478
Trained 1051 batches 	Training Loss: 0.211014
Trained 1101 batches 	Training Loss: 0.192685
Trained 1151 batches 	Training Loss: 0.172394
Trained 1201 batches 	Training Loss: 0.131763
Trained 1251 batches 	Training Loss: 0.172056
Trained 1301 batches 	Training Loss: 0.167245
Trained 1351 batches 	Training Loss: 0.116501
Trained 1401 batches 	Training Loss: 0.201899
Trained 1451 batches 	Training Loss: 0.140731
Trained 1501 batches 	Training Loss: 0.204062
Trained 1551 batches 	Training Loss: 0.228394
Trained 1601 batches 	Training Loss: 0.206865
Trained 1651 batches 	Training Loss: 0.194129
Trained 1701 batches 	Training Loss: 0.219413
Trained 1751 batches 	Training Loss: 0.186724
Trained 1801 batches 	Training Loss: 0.251280
Trained 1851 batches 	Training Loss: 0.115326
Trained 1901 batches 	Training Loss: 0.115141
Trained 1951 batches 	Training Loss: 0.145880
Trained 2001 batches 	Training Loss: 0.240806
Trained 2051 batches 	Training Loss: 0.227703
Trained 2101 batches 	Training Loss: 0.144083
Trained 2151 batches 	Training Loss: 0.121414
Trained 2201 batches 	Training Loss: 0.194104
Trained 2251 batches 	Training Loss: 0.120336
Trained 2301 batches 	Training Loss: 0.131576
Trained 2351 batches 	Training Loss: 0.102036
Trained 2401 batches 	Training Loss: 0.144274
Trained 2451 batches 	Training Loss: 0.137429
Trained 2501 batches 	Training Loss: 0.175605
Trained 2551 batches 	Training Loss: 0.242531
Trained 2601 batches 	Training Loss: 0.211852
Trained 2651 batches 	Training Loss: 0.158187
Trained 2701 batches 	Training Loss: 0.190244
Trained 2751 batches 	Training Loss: 0.149067
Trained 2801 batches 	Training Loss: 0.166271
Trained 2851 batches 	Training Loss: 0.178887
Trained 2901 batches 	Training Loss: 0.132944
Trained 2951 batches 	Training Loss: 0.271406
Trained 3001 batches 	Training Loss: 0.170034
Trained 3051 batches 	Training Loss: 0.159459
Trained 3101 batches 	Training Loss: 0.199625
Trained 3151 batches 	Training Loss: 0.148621
Trained 3201 batches 	Training Loss: 0.128615
Trained 3251 batches 	Training Loss: 0.105357
Trained 3301 batches 	Training Loss: 0.157186
Trained 3351 batches 	Training Loss: 0.209007
Trained 3401 batches 	Training Loss: 0.266305
Trained 3451 batches 	Training Loss: 0.119836
Trained 3501 batches 	Training Loss: 0.185733
Trained 3551 batches 	Training Loss: 0.243746
Trained 3601 batches 	Training Loss: 0.184545
Trained 3651 batches 	Training Loss: 0.168527
Trained 3701 batches 	Training Loss: 0.156739
Trained 3751 batches 	Training Loss: 0.141877
Trained 3801 batches 	Training Loss: 0.187117
Trained 3851 batches 	Training Loss: 0.224325
Trained 3901 batches 	Training Loss: 0.101842
Trained 3951 batches 	Training Loss: 0.172310
Trained 4001 batches 	Training Loss: 0.159054
Trained 4051 batches 	Training Loss: 0.175151
Trained 4101 batches 	Training Loss: 0.136817
Trained 4151 batches 	Training Loss: 0.214696
Trained 4201 batches 	Training Loss: 0.197587
Trained 4251 batches 	Training Loss: 0.158465
Trained 4301 batches 	Training Loss: 0.183587
Trained 4351 batches 	Training Loss: 0.099658
Trained 4401 batches 	Training Loss: 0.159597
Trained 4451 batches 	Training Loss: 0.143144
Trained 4501 batches 	Training Loss: 0.145434
Trained 4551 batches 	Training Loss: 0.137180
Trained 4601 batches 	Training Loss: 0.113610
Trained 4651 batches 	Training Loss: 0.177734
Trained 4701 batches 	Training Loss: 0.160543
Trained 4751 batches 	Training Loss: 0.166579
Trained 4801 batches 	Training Loss: 0.157042
Trained 4851 batches 	Training Loss: 0.163966
Trained 4901 batches 	Training Loss: 0.152128
Epoch: 2 	Training Loss: 0.166067
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.758
The AUROC of Atelectasis is 0.7452796069993649
The AUROC of Cardiomegaly is 0.8607250168446229
The AUROC of Effusion is 0.8245839280098697
The AUROC of Infiltration is 0.6817555626553082
The AUROC of Mass is 0.7453804311944491
The AUROC of Nodule is 0.6789843031875599
The AUROC of Pneumonia is 0.7287899223674074
The AUROC of Pneumothorax is 0.7836733329991813
The AUROC of Consolidation is 0.7722656514372328
The AUROC of Edema is 0.8482442725087956
The AUROC of Emphysema is 0.7381959635183795
The AUROC of Fibrosis is 0.7320488249688213
The AUROC of Pleural_Thickening is 0.7196458646528547
The AUROC of Hernia is 0.7557178174638634
Training time lapse: 21.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.755
The AUROC of Atelectasis is 0.7485392005624614
The AUROC of Cardiomegaly is 0.8612741975282625
The AUROC of Effusion is 0.8272560538168373
The AUROC of Infiltration is 0.6690903525099242
The AUROC of Mass is 0.7298448752181056
The AUROC of Nodule is 0.6577447790321944
The AUROC of Pneumonia is 0.6883317819226862
The AUROC of Pneumothorax is 0.8071035446274292
The AUROC of Consolidation is 0.7727682645472118
The AUROC of Edema is 0.8660810774576019
The AUROC of Emphysema is 0.7840739268762952
The AUROC of Fibrosis is 0.7227967451335926
The AUROC of Pleural_Thickening is 0.7356969147741874
The AUROC of Hernia is 0.6979343757524461
Started training, total epoch : 4
Training data size: 4906
Started epoch 1
Trained 1 batches 	Training Loss: 0.648253
Trained 51 batches 	Training Loss: 0.136698
Trained 101 batches 	Training Loss: 0.263494
Trained 151 batches 	Training Loss: 0.221946
Trained 201 batches 	Training Loss: 0.146012
Trained 251 batches 	Training Loss: 0.154334
Trained 301 batches 	Training Loss: 0.238356
Trained 351 batches 	Training Loss: 0.144434
Trained 401 batches 	Training Loss: 0.227781
Trained 451 batches 	Training Loss: 0.145836
Trained 501 batches 	Training Loss: 0.245776
Trained 551 batches 	Training Loss: 0.127712
Trained 601 batches 	Training Loss: 0.103699
Trained 651 batches 	Training Loss: 0.171159
Trained 701 batches 	Training Loss: 0.114369
Trained 751 batches 	Training Loss: 0.160630
Trained 801 batches 	Training Loss: 0.155090
Trained 851 batches 	Training Loss: 0.222235
Trained 901 batches 	Training Loss: 0.228153
Trained 951 batches 	Training Loss: 0.158946
Trained 1001 batches 	Training Loss: 0.110983
Trained 1051 batches 	Training Loss: 0.194581
Trained 1101 batches 	Training Loss: 0.229783
Trained 1151 batches 	Training Loss: 0.138778
Trained 1201 batches 	Training Loss: 0.122538
Trained 1251 batches 	Training Loss: 0.105683
Trained 1301 batches 	Training Loss: 0.137699
Trained 1351 batches 	Training Loss: 0.209552
Trained 1401 batches 	Training Loss: 0.177794
Trained 1451 batches 	Training Loss: 0.136519
Trained 1501 batches 	Training Loss: 0.188117
Trained 1551 batches 	Training Loss: 0.238699
Trained 1601 batches 	Training Loss: 0.214269
Trained 1651 batches 	Training Loss: 0.158535
Trained 1701 batches 	Training Loss: 0.159166
Trained 1751 batches 	Training Loss: 0.166947
Trained 1801 batches 	Training Loss: 0.131633
Trained 1851 batches 	Training Loss: 0.172893
Trained 1901 batches 	Training Loss: 0.172789
Trained 1951 batches 	Training Loss: 0.246989
Trained 2001 batches 	Training Loss: 0.278205
Trained 2051 batches 	Training Loss: 0.187892
Trained 2101 batches 	Training Loss: 0.144059
Trained 2151 batches 	Training Loss: 0.147370
Trained 2201 batches 	Training Loss: 0.133449
Trained 2251 batches 	Training Loss: 0.146988
Trained 2301 batches 	Training Loss: 0.164689
Trained 2351 batches 	Training Loss: 0.219019
Trained 2401 batches 	Training Loss: 0.121388
Trained 2451 batches 	Training Loss: 0.159172
Trained 2501 batches 	Training Loss: 0.109429
Trained 2551 batches 	Training Loss: 0.173869
Trained 2601 batches 	Training Loss: 0.203189
Trained 2651 batches 	Training Loss: 0.145925
Trained 2701 batches 	Training Loss: 0.092543
Trained 2751 batches 	Training Loss: 0.177836
Trained 2801 batches 	Training Loss: 0.208479
Trained 2851 batches 	Training Loss: 0.107722
Trained 2901 batches 	Training Loss: 0.162083
Trained 2951 batches 	Training Loss: 0.123800
Trained 3001 batches 	Training Loss: 0.201471
Trained 3051 batches 	Training Loss: 0.191129
Trained 3101 batches 	Training Loss: 0.154291
Trained 3151 batches 	Training Loss: 0.211011
Trained 3201 batches 	Training Loss: 0.135831
Trained 3251 batches 	Training Loss: 0.150434
Trained 3301 batches 	Training Loss: 0.180792
Trained 3351 batches 	Training Loss: 0.117828
Trained 3401 batches 	Training Loss: 0.096690
Trained 3451 batches 	Training Loss: 0.187525
Trained 3501 batches 	Training Loss: 0.162190
Trained 3551 batches 	Training Loss: 0.154011
Trained 3601 batches 	Training Loss: 0.102391
Trained 3651 batches 	Training Loss: 0.133915
Trained 3701 batches 	Training Loss: 0.287065
Trained 3751 batches 	Training Loss: 0.246118
Trained 3801 batches 	Training Loss: 0.133872
Trained 3851 batches 	Training Loss: 0.183555
Trained 3901 batches 	Training Loss: 0.201865
Trained 3951 batches 	Training Loss: 0.216977
Trained 4001 batches 	Training Loss: 0.163497
Trained 4051 batches 	Training Loss: 0.200192
Trained 4101 batches 	Training Loss: 0.179978
Trained 4151 batches 	Training Loss: 0.167580
Trained 4201 batches 	Training Loss: 0.137001
Trained 4251 batches 	Training Loss: 0.123584
Trained 4301 batches 	Training Loss: 0.190630
Trained 4351 batches 	Training Loss: 0.156828
Trained 4401 batches 	Training Loss: 0.254502
Trained 4451 batches 	Training Loss: 0.183801
Trained 4501 batches 	Training Loss: 0.143899
Trained 4551 batches 	Training Loss: 0.217197
Trained 4601 batches 	Training Loss: 0.164451
Trained 4651 batches 	Training Loss: 0.154031
Trained 4701 batches 	Training Loss: 0.179392
Trained 4751 batches 	Training Loss: 0.337759
Trained 4801 batches 	Training Loss: 0.203581
Trained 4851 batches 	Training Loss: 0.085679
Trained 4901 batches 	Training Loss: 0.199090
Epoch: 1 	Training Loss: 0.175263
Started epoch 2
Trained 1 batches 	Training Loss: 0.124844
Trained 51 batches 	Training Loss: 0.171284
Trained 101 batches 	Training Loss: 0.149740
Trained 151 batches 	Training Loss: 0.161561
Trained 201 batches 	Training Loss: 0.170513
Trained 251 batches 	Training Loss: 0.114246
Trained 301 batches 	Training Loss: 0.275055
Trained 351 batches 	Training Loss: 0.122034
Trained 401 batches 	Training Loss: 0.173158
Trained 451 batches 	Training Loss: 0.182823
Trained 501 batches 	Training Loss: 0.161227
Trained 551 batches 	Training Loss: 0.184350
Trained 601 batches 	Training Loss: 0.215215
Trained 651 batches 	Training Loss: 0.148802
Trained 701 batches 	Training Loss: 0.217891
Trained 751 batches 	Training Loss: 0.220280
Trained 801 batches 	Training Loss: 0.183855
Trained 851 batches 	Training Loss: 0.215066
Trained 901 batches 	Training Loss: 0.187127
Trained 951 batches 	Training Loss: 0.167974
Trained 1001 batches 	Training Loss: 0.219367
Trained 1051 batches 	Training Loss: 0.134022
Trained 1101 batches 	Training Loss: 0.146922
Trained 1151 batches 	Training Loss: 0.148692
Trained 1201 batches 	Training Loss: 0.155059
Trained 1251 batches 	Training Loss: 0.137570
Trained 1301 batches 	Training Loss: 0.155196
Trained 1351 batches 	Training Loss: 0.260949
Trained 1401 batches 	Training Loss: 0.227136
Trained 1451 batches 	Training Loss: 0.089622
Trained 1501 batches 	Training Loss: 0.297001
Trained 1551 batches 	Training Loss: 0.182580
Trained 1601 batches 	Training Loss: 0.182328
Trained 1651 batches 	Training Loss: 0.147962
Trained 1701 batches 	Training Loss: 0.154090
Trained 1751 batches 	Training Loss: 0.203774
Trained 1801 batches 	Training Loss: 0.143882
Trained 1851 batches 	Training Loss: 0.160749
Trained 1901 batches 	Training Loss: 0.170714
Trained 1951 batches 	Training Loss: 0.227641
Trained 2001 batches 	Training Loss: 0.188847
Trained 2051 batches 	Training Loss: 0.149199
Trained 2101 batches 	Training Loss: 0.196910
Trained 2151 batches 	Training Loss: 0.174999
Trained 2201 batches 	Training Loss: 0.120843
Trained 2251 batches 	Training Loss: 0.155959
Trained 2301 batches 	Training Loss: 0.160498
Trained 2351 batches 	Training Loss: 0.213787
Trained 2401 batches 	Training Loss: 0.135490
Trained 2451 batches 	Training Loss: 0.133447
Trained 2501 batches 	Training Loss: 0.165231
Trained 2551 batches 	Training Loss: 0.246297
Trained 2601 batches 	Training Loss: 0.133680
Trained 2651 batches 	Training Loss: 0.076379
Trained 2701 batches 	Training Loss: 0.214065
Trained 2751 batches 	Training Loss: 0.200976
Trained 2801 batches 	Training Loss: 0.114111
Trained 2851 batches 	Training Loss: 0.110986
Trained 2901 batches 	Training Loss: 0.123949
Trained 2951 batches 	Training Loss: 0.157929
Trained 3001 batches 	Training Loss: 0.161319
Trained 3051 batches 	Training Loss: 0.110330
Trained 3101 batches 	Training Loss: 0.092279
Trained 3151 batches 	Training Loss: 0.188053
Trained 3201 batches 	Training Loss: 0.086136
Trained 3251 batches 	Training Loss: 0.287382
Trained 3301 batches 	Training Loss: 0.115179
Trained 3351 batches 	Training Loss: 0.172768
Trained 3401 batches 	Training Loss: 0.222244
Trained 3451 batches 	Training Loss: 0.146815
Trained 3501 batches 	Training Loss: 0.209320
Trained 3551 batches 	Training Loss: 0.159849
Trained 3601 batches 	Training Loss: 0.193428
Trained 3651 batches 	Training Loss: 0.153127
Trained 3701 batches 	Training Loss: 0.146712
Trained 3751 batches 	Training Loss: 0.244922
Trained 3801 batches 	Training Loss: 0.106586
Trained 3851 batches 	Training Loss: 0.109623
Trained 3901 batches 	Training Loss: 0.094950
Trained 3951 batches 	Training Loss: 0.155433
Trained 4001 batches 	Training Loss: 0.142039
Trained 4051 batches 	Training Loss: 0.234726
Trained 4101 batches 	Training Loss: 0.165126
Trained 4151 batches 	Training Loss: 0.157023
Trained 4201 batches 	Training Loss: 0.181501
Trained 4251 batches 	Training Loss: 0.161254
Trained 4301 batches 	Training Loss: 0.198102
Trained 4351 batches 	Training Loss: 0.196160
Trained 4401 batches 	Training Loss: 0.179384
Trained 4451 batches 	Training Loss: 0.135632
Trained 4501 batches 	Training Loss: 0.130742
Trained 4551 batches 	Training Loss: 0.283364
Trained 4601 batches 	Training Loss: 0.105908
Trained 4651 batches 	Training Loss: 0.121668
Trained 4701 batches 	Training Loss: 0.149406
Trained 4751 batches 	Training Loss: 0.177364
Trained 4801 batches 	Training Loss: 0.162638
Trained 4851 batches 	Training Loss: 0.141026
Trained 4901 batches 	Training Loss: 0.261240
Epoch: 2 	Training Loss: 0.165653
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.755
The AUROC of Atelectasis is 0.749191827444128
The AUROC of Cardiomegaly is 0.8476186863015278
The AUROC of Effusion is 0.8370790899634727
The AUROC of Infiltration is 0.667111585107939
The AUROC of Mass is 0.7368662166287234
The AUROC of Nodule is 0.6655683226191236
The AUROC of Pneumonia is 0.714239896451695
The AUROC of Pneumothorax is 0.7755066579787145
The AUROC of Consolidation is 0.7643269272181399
The AUROC of Edema is 0.8286342542019182
The AUROC of Emphysema is 0.7694572173023423
The AUROC of Fibrosis is 0.7238193258217076
The AUROC of Pleural_Thickening is 0.6991884320716154
The AUROC of Hernia is 0.7940819791414019
Started epoch 3
Trained 1 batches 	Training Loss: 0.138132
Trained 51 batches 	Training Loss: 0.209565
Trained 101 batches 	Training Loss: 0.165902
Trained 151 batches 	Training Loss: 0.229089
Trained 201 batches 	Training Loss: 0.187343
Trained 251 batches 	Training Loss: 0.228930
Trained 301 batches 	Training Loss: 0.214826
Trained 351 batches 	Training Loss: 0.155066
Trained 401 batches 	Training Loss: 0.169159
Trained 451 batches 	Training Loss: 0.123815
Trained 501 batches 	Training Loss: 0.100696
Trained 551 batches 	Training Loss: 0.128724
Trained 601 batches 	Training Loss: 0.060020
Trained 651 batches 	Training Loss: 0.228906
Trained 701 batches 	Training Loss: 0.134354
Trained 751 batches 	Training Loss: 0.141010
Trained 801 batches 	Training Loss: 0.146920
Trained 851 batches 	Training Loss: 0.220976
Trained 901 batches 	Training Loss: 0.145902
Trained 951 batches 	Training Loss: 0.270478
Trained 1001 batches 	Training Loss: 0.223249
Trained 1051 batches 	Training Loss: 0.093981
Trained 1101 batches 	Training Loss: 0.134037
Trained 1151 batches 	Training Loss: 0.180696
Trained 1201 batches 	Training Loss: 0.203249
Trained 1251 batches 	Training Loss: 0.199991
Trained 1301 batches 	Training Loss: 0.137151
Trained 1351 batches 	Training Loss: 0.157647
Trained 1401 batches 	Training Loss: 0.122342
Trained 1451 batches 	Training Loss: 0.167685
Trained 1501 batches 	Training Loss: 0.153221
Trained 1551 batches 	Training Loss: 0.250379
Trained 1601 batches 	Training Loss: 0.141359
Trained 1651 batches 	Training Loss: 0.194233
Trained 1701 batches 	Training Loss: 0.187316
Trained 1751 batches 	Training Loss: 0.106909
Trained 1801 batches 	Training Loss: 0.183977
Trained 1851 batches 	Training Loss: 0.110527
Trained 1901 batches 	Training Loss: 0.193195
Trained 1951 batches 	Training Loss: 0.176094
Trained 2001 batches 	Training Loss: 0.164091
Trained 2051 batches 	Training Loss: 0.185011
Trained 2101 batches 	Training Loss: 0.129063
Trained 2151 batches 	Training Loss: 0.193539
Trained 2201 batches 	Training Loss: 0.154542
Trained 2251 batches 	Training Loss: 0.141625
Trained 2301 batches 	Training Loss: 0.156811
Trained 2351 batches 	Training Loss: 0.125889
Trained 2401 batches 	Training Loss: 0.138601
Trained 2451 batches 	Training Loss: 0.173419
Trained 2501 batches 	Training Loss: 0.077816
Trained 2551 batches 	Training Loss: 0.203146
Trained 2601 batches 	Training Loss: 0.195671
Trained 2651 batches 	Training Loss: 0.168269
Trained 2701 batches 	Training Loss: 0.149791
Trained 2751 batches 	Training Loss: 0.172582
Trained 2801 batches 	Training Loss: 0.220681
Trained 2851 batches 	Training Loss: 0.175542
Trained 2901 batches 	Training Loss: 0.210767
Trained 2951 batches 	Training Loss: 0.189306
Trained 3001 batches 	Training Loss: 0.112253
Trained 3051 batches 	Training Loss: 0.121599
Trained 3101 batches 	Training Loss: 0.147660
Trained 3151 batches 	Training Loss: 0.176957
Trained 3201 batches 	Training Loss: 0.207742
Trained 3251 batches 	Training Loss: 0.136288
Trained 3301 batches 	Training Loss: 0.183932
Trained 3351 batches 	Training Loss: 0.224277
Trained 3401 batches 	Training Loss: 0.167011
Trained 3451 batches 	Training Loss: 0.123613
Trained 3501 batches 	Training Loss: 0.147426
Trained 3551 batches 	Training Loss: 0.157378
Trained 3601 batches 	Training Loss: 0.138558
Trained 3651 batches 	Training Loss: 0.166287
Trained 3701 batches 	Training Loss: 0.140517
Trained 3751 batches 	Training Loss: 0.184632
Trained 3801 batches 	Training Loss: 0.090940
Trained 3851 batches 	Training Loss: 0.147408
Trained 3901 batches 	Training Loss: 0.191259
Trained 3951 batches 	Training Loss: 0.128314
Trained 4001 batches 	Training Loss: 0.166465
Trained 4051 batches 	Training Loss: 0.199657
Trained 4101 batches 	Training Loss: 0.137861
Trained 4151 batches 	Training Loss: 0.139049
Trained 4201 batches 	Training Loss: 0.106942
Trained 4251 batches 	Training Loss: 0.190953
Trained 4301 batches 	Training Loss: 0.181212
Trained 4351 batches 	Training Loss: 0.182559
Trained 4401 batches 	Training Loss: 0.109196
Trained 4451 batches 	Training Loss: 0.182429
Trained 4501 batches 	Training Loss: 0.149965
Trained 4551 batches 	Training Loss: 0.167051
Trained 4601 batches 	Training Loss: 0.169766
Trained 4651 batches 	Training Loss: 0.153082
Trained 4701 batches 	Training Loss: 0.202237
Trained 4751 batches 	Training Loss: 0.207331
Trained 4801 batches 	Training Loss: 0.117359
Trained 4851 batches 	Training Loss: 0.176650
Trained 4901 batches 	Training Loss: 0.182257
Epoch: 3 	Training Loss: 0.161176
Started epoch 4
Trained 1 batches 	Training Loss: 0.207503
Trained 51 batches 	Training Loss: 0.198415
Trained 101 batches 	Training Loss: 0.157218
Trained 151 batches 	Training Loss: 0.185904
Trained 201 batches 	Training Loss: 0.163528
Trained 251 batches 	Training Loss: 0.235652
Trained 301 batches 	Training Loss: 0.150295
Trained 351 batches 	Training Loss: 0.166408
Trained 401 batches 	Training Loss: 0.144295
Trained 451 batches 	Training Loss: 0.225724
Trained 501 batches 	Training Loss: 0.175679
Trained 551 batches 	Training Loss: 0.175756
Trained 601 batches 	Training Loss: 0.180082
Trained 651 batches 	Training Loss: 0.201720
Trained 701 batches 	Training Loss: 0.091600
Trained 751 batches 	Training Loss: 0.157556
Trained 801 batches 	Training Loss: 0.147484
Trained 851 batches 	Training Loss: 0.185734
Trained 901 batches 	Training Loss: 0.093129
Trained 951 batches 	Training Loss: 0.153973
Trained 1001 batches 	Training Loss: 0.318329
Trained 1051 batches 	Training Loss: 0.115027
Trained 1101 batches 	Training Loss: 0.115852
Trained 1151 batches 	Training Loss: 0.105543
Trained 1201 batches 	Training Loss: 0.159302
Trained 1251 batches 	Training Loss: 0.172705
Trained 1301 batches 	Training Loss: 0.169075
Trained 1351 batches 	Training Loss: 0.146184
Trained 1401 batches 	Training Loss: 0.143963
Trained 1451 batches 	Training Loss: 0.164399
Trained 1501 batches 	Training Loss: 0.122334
Trained 1551 batches 	Training Loss: 0.142775
Trained 1601 batches 	Training Loss: 0.146190
Trained 1651 batches 	Training Loss: 0.129419
Trained 1701 batches 	Training Loss: 0.136903
Trained 1751 batches 	Training Loss: 0.133356
Trained 1801 batches 	Training Loss: 0.087106
Trained 1851 batches 	Training Loss: 0.134066
Trained 1901 batches 	Training Loss: 0.122121
Trained 1951 batches 	Training Loss: 0.196448
Trained 2001 batches 	Training Loss: 0.177845
Trained 2051 batches 	Training Loss: 0.179920
Trained 2101 batches 	Training Loss: 0.132962
Trained 2151 batches 	Training Loss: 0.165199
Trained 2201 batches 	Training Loss: 0.150435
Trained 2251 batches 	Training Loss: 0.169521
Trained 2301 batches 	Training Loss: 0.135597
Trained 2351 batches 	Training Loss: 0.128353
Trained 2401 batches 	Training Loss: 0.144108
Trained 2451 batches 	Training Loss: 0.120675
Trained 2501 batches 	Training Loss: 0.111001
Trained 2551 batches 	Training Loss: 0.208578
Trained 2601 batches 	Training Loss: 0.150555
Trained 2651 batches 	Training Loss: 0.077482
Trained 2701 batches 	Training Loss: 0.153643
Trained 2751 batches 	Training Loss: 0.172095
Trained 2801 batches 	Training Loss: 0.200776
Trained 2851 batches 	Training Loss: 0.113640
Trained 2901 batches 	Training Loss: 0.090174
Trained 2951 batches 	Training Loss: 0.160718
Trained 3001 batches 	Training Loss: 0.128637
Trained 3051 batches 	Training Loss: 0.186472
Trained 3101 batches 	Training Loss: 0.145389
Trained 3151 batches 	Training Loss: 0.121251
Trained 3201 batches 	Training Loss: 0.120457
Trained 3251 batches 	Training Loss: 0.195712
Trained 3301 batches 	Training Loss: 0.117479
Trained 3351 batches 	Training Loss: 0.135112
Trained 3401 batches 	Training Loss: 0.167758
Trained 3451 batches 	Training Loss: 0.187674
Trained 3501 batches 	Training Loss: 0.110965
Trained 3551 batches 	Training Loss: 0.140867
Trained 3601 batches 	Training Loss: 0.183207
Trained 3651 batches 	Training Loss: 0.140853
Trained 3701 batches 	Training Loss: 0.166503
Trained 3751 batches 	Training Loss: 0.185585
Trained 3801 batches 	Training Loss: 0.117807
Trained 3851 batches 	Training Loss: 0.132663
Trained 3901 batches 	Training Loss: 0.164479
Trained 3951 batches 	Training Loss: 0.145659
Trained 4001 batches 	Training Loss: 0.161911
Trained 4051 batches 	Training Loss: 0.128953
Trained 4101 batches 	Training Loss: 0.092785
Trained 4151 batches 	Training Loss: 0.153471
Trained 4201 batches 	Training Loss: 0.105732
Trained 4251 batches 	Training Loss: 0.118683
Trained 4301 batches 	Training Loss: 0.102515
Trained 4351 batches 	Training Loss: 0.248755
Trained 4401 batches 	Training Loss: 0.185449
Trained 4451 batches 	Training Loss: 0.138839
Trained 4501 batches 	Training Loss: 0.191915
Trained 4551 batches 	Training Loss: 0.147841
Trained 4601 batches 	Training Loss: 0.141601
Trained 4651 batches 	Training Loss: 0.199987
Trained 4701 batches 	Training Loss: 0.151448
Trained 4751 batches 	Training Loss: 0.214459
Trained 4801 batches 	Training Loss: 0.176830
Trained 4851 batches 	Training Loss: 0.128865
Trained 4901 batches 	Training Loss: 0.132316
Epoch: 4 	Training Loss: 0.158004
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.781
The AUROC of Atelectasis is 0.7699290396663039
The AUROC of Cardiomegaly is 0.8908010149264602
The AUROC of Effusion is 0.8603785166549747
The AUROC of Infiltration is 0.6926176378292307
The AUROC of Mass is 0.7804124752319902
The AUROC of Nodule is 0.6991015616116577
The AUROC of Pneumonia is 0.7351972600935257
The AUROC of Pneumothorax is 0.8179264197283345
The AUROC of Consolidation is 0.7751988848989187
The AUROC of Edema is 0.8663556506475208
The AUROC of Emphysema is 0.8164399964300436
The AUROC of Fibrosis is 0.7389575329105126
The AUROC of Pleural_Thickening is 0.7468008871071562
The AUROC of Hernia is 0.7480458361523502
Training time lapse: 43.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.789
The AUROC of Atelectasis is 0.7737843326409927
The AUROC of Cardiomegaly is 0.8879679145716093
The AUROC of Effusion is 0.8638469059991687
The AUROC of Infiltration is 0.6809356547809424
The AUROC of Mass is 0.7877743087045761
The AUROC of Nodule is 0.6893454994382414
The AUROC of Pneumonia is 0.7242457249787533
The AUROC of Pneumothorax is 0.8275704376905499
The AUROC of Consolidation is 0.787392545655054
The AUROC of Edema is 0.8948975267974961
The AUROC of Emphysema is 0.8171625250306991
The AUROC of Fibrosis is 0.7572354415559097
The AUROC of Pleural_Thickening is 0.7624322021289545
The AUROC of Hernia is 0.7956483987946269
