Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.683528
Trained 51 batches 	Training Loss: 0.181848
Trained 101 batches 	Training Loss: 0.231665
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.699852
Trained 51 batches 	Training Loss: 0.206493
Trained 101 batches 	Training Loss: 0.276482
Trained 151 batches 	Training Loss: 0.132311
Trained 201 batches 	Training Loss: 0.275982
Trained 251 batches 	Training Loss: 0.218506
Trained 301 batches 	Training Loss: 0.255357
Trained 351 batches 	Training Loss: 0.173556
Trained 401 batches 	Training Loss: 0.151710
Trained 451 batches 	Training Loss: 0.171277
Trained 501 batches 	Training Loss: 0.234437
Trained 551 batches 	Training Loss: 0.226463
Trained 601 batches 	Training Loss: 0.154076
Trained 651 batches 	Training Loss: 0.301066
Trained 701 batches 	Training Loss: 0.207459
Trained 751 batches 	Training Loss: 0.159626
Trained 801 batches 	Training Loss: 0.264873
Trained 851 batches 	Training Loss: 0.262019
Trained 901 batches 	Training Loss: 0.120863
Trained 951 batches 	Training Loss: 0.170588
Trained 1001 batches 	Training Loss: 0.178508
Trained 1051 batches 	Training Loss: 0.146641
Trained 1101 batches 	Training Loss: 0.169381
Trained 1151 batches 	Training Loss: 0.138995
Trained 1201 batches 	Training Loss: 0.146266
Trained 1251 batches 	Training Loss: 0.166752
Trained 1301 batches 	Training Loss: 0.204161
Trained 1351 batches 	Training Loss: 0.172512
Trained 1401 batches 	Training Loss: 0.165401
Trained 1451 batches 	Training Loss: 0.196901
Trained 1501 batches 	Training Loss: 0.242707
Trained 1551 batches 	Training Loss: 0.188217
Trained 1601 batches 	Training Loss: 0.178245
Trained 1651 batches 	Training Loss: 0.125210
Trained 1701 batches 	Training Loss: 0.167132
Trained 1751 batches 	Training Loss: 0.097344
Trained 1801 batches 	Training Loss: 0.156226
Trained 1851 batches 	Training Loss: 0.160076
Trained 1901 batches 	Training Loss: 0.272924
Trained 1951 batches 	Training Loss: 0.173151
Trained 2001 batches 	Training Loss: 0.226734
Trained 2051 batches 	Training Loss: 0.120139
Trained 2101 batches 	Training Loss: 0.190381
Trained 2151 batches 	Training Loss: 0.103259
Trained 2201 batches 	Training Loss: 0.284925
Trained 2251 batches 	Training Loss: 0.079617
Trained 2301 batches 	Training Loss: 0.159577
Trained 2351 batches 	Training Loss: 0.144185
Trained 2401 batches 	Training Loss: 0.117549
Trained 2451 batches 	Training Loss: 0.109839
Trained 2501 batches 	Training Loss: 0.167326
Trained 2551 batches 	Training Loss: 0.190700
Trained 2601 batches 	Training Loss: 0.245943
Trained 2651 batches 	Training Loss: 0.185600
Trained 2701 batches 	Training Loss: 0.169955
Trained 2751 batches 	Training Loss: 0.183967
Trained 2801 batches 	Training Loss: 0.264973
Trained 2851 batches 	Training Loss: 0.152200
Trained 2901 batches 	Training Loss: 0.251526
Trained 2951 batches 	Training Loss: 0.156186
Trained 3001 batches 	Training Loss: 0.207768
Trained 3051 batches 	Training Loss: 0.222044
Trained 3101 batches 	Training Loss: 0.186899
Trained 3151 batches 	Training Loss: 0.119343
Trained 3201 batches 	Training Loss: 0.111398
Trained 3251 batches 	Training Loss: 0.181020
Trained 3301 batches 	Training Loss: 0.188590
Trained 3351 batches 	Training Loss: 0.208888
Trained 3401 batches 	Training Loss: 0.188752
Trained 3451 batches 	Training Loss: 0.151654
Trained 3501 batches 	Training Loss: 0.223026
Trained 3551 batches 	Training Loss: 0.152769
Trained 3601 batches 	Training Loss: 0.145212
Trained 3651 batches 	Training Loss: 0.151901
Trained 3701 batches 	Training Loss: 0.283324
Trained 3751 batches 	Training Loss: 0.204067
Trained 3801 batches 	Training Loss: 0.131860
Trained 3851 batches 	Training Loss: 0.205139
Trained 3901 batches 	Training Loss: 0.221085
Trained 3951 batches 	Training Loss: 0.135672
Trained 4001 batches 	Training Loss: 0.309510
Trained 4051 batches 	Training Loss: 0.205763
Trained 4101 batches 	Training Loss: 0.257177
Trained 4151 batches 	Training Loss: 0.271326
Trained 4201 batches 	Training Loss: 0.180042
Trained 4251 batches 	Training Loss: 0.287081
Trained 4301 batches 	Training Loss: 0.197476
Trained 4351 batches 	Training Loss: 0.157042
Trained 4401 batches 	Training Loss: 0.193908
Trained 4451 batches 	Training Loss: 0.160325
Trained 4501 batches 	Training Loss: 0.186916
Trained 4551 batches 	Training Loss: 0.179655
Trained 4601 batches 	Training Loss: 0.139031
Trained 4651 batches 	Training Loss: 0.192617
Trained 4701 batches 	Training Loss: 0.260225
Trained 4751 batches 	Training Loss: 0.192508
Trained 4801 batches 	Training Loss: 0.230998
Trained 4851 batches 	Training Loss: 0.209301
Trained 4901 batches 	Training Loss: 0.155043
Epoch: 1 	Training Loss: 0.185148
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.560
The AUROC of Atelectasis is 0.5735203335441188
The AUROC of Cardiomegaly is 0.6097510732975684
The AUROC of Effusion is 0.6088235934510201
The AUROC of Infiltration is 0.5859479930288528
The AUROC of Mass is 0.4857167757390526
The AUROC of Nodule is 0.4890952116984748
The AUROC of Pneumonia is 0.5519214193191991
The AUROC of Pneumothorax is 0.5608045118537864
The AUROC of Consolidation is 0.6245766699147847
The AUROC of Edema is 0.7228023819770907
The AUROC of Emphysema is 0.5147413333686652
The AUROC of Fibrosis is 0.561262309542258
The AUROC of Pleural_Thickening is 0.4673758331336684
The AUROC of Hernia is 0.4776935707970191
Started epoch 2
Trained 1 batches 	Training Loss: 0.238977
Trained 51 batches 	Training Loss: 0.215699
Trained 101 batches 	Training Loss: 0.145030
Trained 151 batches 	Training Loss: 0.211909
Trained 201 batches 	Training Loss: 0.144499
Trained 251 batches 	Training Loss: 0.205450
Trained 301 batches 	Training Loss: 0.169991
Trained 351 batches 	Training Loss: 0.148317
Trained 401 batches 	Training Loss: 0.185480
Trained 451 batches 	Training Loss: 0.176790
Trained 501 batches 	Training Loss: 0.174940
Trained 551 batches 	Training Loss: 0.218590
Trained 601 batches 	Training Loss: 0.263465
Trained 651 batches 	Training Loss: 0.193969
Trained 701 batches 	Training Loss: 0.166065
Trained 751 batches 	Training Loss: 0.208911
Trained 801 batches 	Training Loss: 0.132766
Trained 851 batches 	Training Loss: 0.123522
Trained 901 batches 	Training Loss: 0.117602
Trained 951 batches 	Training Loss: 0.163868
Trained 1001 batches 	Training Loss: 0.233728
Trained 1051 batches 	Training Loss: 0.164969
Trained 1101 batches 	Training Loss: 0.166639
Trained 1151 batches 	Training Loss: 0.178626
Trained 1201 batches 	Training Loss: 0.188601
Trained 1251 batches 	Training Loss: 0.210551
Trained 1301 batches 	Training Loss: 0.110633
Trained 1351 batches 	Training Loss: 0.252941
Trained 1401 batches 	Training Loss: 0.163031
Trained 1451 batches 	Training Loss: 0.182912
Trained 1501 batches 	Training Loss: 0.198928
Trained 1551 batches 	Training Loss: 0.246413
Trained 1601 batches 	Training Loss: 0.141770
Trained 1651 batches 	Training Loss: 0.193872
Trained 1701 batches 	Training Loss: 0.313625
Trained 1751 batches 	Training Loss: 0.184182
Trained 1801 batches 	Training Loss: 0.220484
Trained 1851 batches 	Training Loss: 0.139328
Trained 1901 batches 	Training Loss: 0.122292
Trained 1951 batches 	Training Loss: 0.225032
Trained 2001 batches 	Training Loss: 0.187393
Trained 2051 batches 	Training Loss: 0.154639
Trained 2101 batches 	Training Loss: 0.124596
Trained 2151 batches 	Training Loss: 0.137760
Trained 2201 batches 	Training Loss: 0.134704
Trained 2251 batches 	Training Loss: 0.165773
Trained 2301 batches 	Training Loss: 0.153843
Trained 2351 batches 	Training Loss: 0.155594
Trained 2401 batches 	Training Loss: 0.197047
Started training, total epoch : 10
Training data size: 4907
Started epoch 1
Trained 1 batches 	Training Loss: 0.684604
Trained 51 batches 	Training Loss: 0.191108
Trained 101 batches 	Training Loss: 0.232591
Trained 151 batches 	Training Loss: 0.134396
Trained 201 batches 	Training Loss: 0.252993
Trained 251 batches 	Training Loss: 0.210826
Trained 301 batches 	Training Loss: 0.234413
Trained 351 batches 	Training Loss: 0.174705
Trained 401 batches 	Training Loss: 0.162835
Trained 451 batches 	Training Loss: 0.165817
Trained 501 batches 	Training Loss: 0.246726
Trained 551 batches 	Training Loss: 0.229631
Trained 601 batches 	Training Loss: 0.156385
Trained 651 batches 	Training Loss: 0.279181
Trained 701 batches 	Training Loss: 0.179569
Trained 751 batches 	Training Loss: 0.158392
Trained 801 batches 	Training Loss: 0.237551
Trained 851 batches 	Training Loss: 0.291180
Trained 901 batches 	Training Loss: 0.109825
Trained 951 batches 	Training Loss: 0.156448
Trained 1001 batches 	Training Loss: 0.166289
Trained 1051 batches 	Training Loss: 0.142735
Trained 1101 batches 	Training Loss: 0.165602
Trained 1151 batches 	Training Loss: 0.130972
Trained 1201 batches 	Training Loss: 0.151972
Trained 1251 batches 	Training Loss: 0.149682
Trained 1301 batches 	Training Loss: 0.190062
Trained 1351 batches 	Training Loss: 0.168078
Trained 1401 batches 	Training Loss: 0.168259
Trained 1451 batches 	Training Loss: 0.178855
Trained 1501 batches 	Training Loss: 0.223861
Trained 1551 batches 	Training Loss: 0.184980
Trained 1601 batches 	Training Loss: 0.177303
Trained 1651 batches 	Training Loss: 0.112903
Trained 1701 batches 	Training Loss: 0.162324
Trained 1751 batches 	Training Loss: 0.114135
Trained 1801 batches 	Training Loss: 0.145371
Trained 1851 batches 	Training Loss: 0.144992
Trained 1901 batches 	Training Loss: 0.268853
Trained 1951 batches 	Training Loss: 0.148494
Trained 2001 batches 	Training Loss: 0.186784
Trained 2051 batches 	Training Loss: 0.116023
Trained 2101 batches 	Training Loss: 0.178197
Trained 2151 batches 	Training Loss: 0.102131
Trained 2201 batches 	Training Loss: 0.271495
Trained 2251 batches 	Training Loss: 0.071582
Trained 2301 batches 	Training Loss: 0.149420
Trained 2351 batches 	Training Loss: 0.136760
Trained 2401 batches 	Training Loss: 0.110622
Trained 2451 batches 	Training Loss: 0.097607
Trained 2501 batches 	Training Loss: 0.153022
Trained 2551 batches 	Training Loss: 0.178029
Trained 2601 batches 	Training Loss: 0.219697
Trained 2651 batches 	Training Loss: 0.177141
Trained 2701 batches 	Training Loss: 0.147864
Trained 2751 batches 	Training Loss: 0.177175
Trained 2801 batches 	Training Loss: 0.246998
Trained 2851 batches 	Training Loss: 0.155002
Trained 2901 batches 	Training Loss: 0.254261
Trained 2951 batches 	Training Loss: 0.151482
Trained 3001 batches 	Training Loss: 0.199282
Trained 3051 batches 	Training Loss: 0.220375
Trained 3101 batches 	Training Loss: 0.169957
Trained 3151 batches 	Training Loss: 0.104597
Trained 3201 batches 	Training Loss: 0.126278
Trained 3251 batches 	Training Loss: 0.158177
Trained 3301 batches 	Training Loss: 0.162184
Trained 3351 batches 	Training Loss: 0.206535
Trained 3401 batches 	Training Loss: 0.171104
Trained 3451 batches 	Training Loss: 0.154071
Trained 3501 batches 	Training Loss: 0.206132
Trained 3551 batches 	Training Loss: 0.144582
Trained 3601 batches 	Training Loss: 0.133553
Trained 3651 batches 	Training Loss: 0.132891
Trained 3701 batches 	Training Loss: 0.246521
Trained 3751 batches 	Training Loss: 0.198864
Trained 3801 batches 	Training Loss: 0.123962
Trained 3851 batches 	Training Loss: 0.190797
Trained 3901 batches 	Training Loss: 0.215755
Trained 3951 batches 	Training Loss: 0.152126
Trained 4001 batches 	Training Loss: 0.315266
Trained 4051 batches 	Training Loss: 0.197299
Trained 4101 batches 	Training Loss: 0.218396
Trained 4151 batches 	Training Loss: 0.234078
Trained 4201 batches 	Training Loss: 0.171986
Trained 4251 batches 	Training Loss: 0.266403
Trained 4301 batches 	Training Loss: 0.189945
Trained 4351 batches 	Training Loss: 0.144136
Trained 4401 batches 	Training Loss: 0.161465
Trained 4451 batches 	Training Loss: 0.128158
Trained 4501 batches 	Training Loss: 0.178057
Trained 4551 batches 	Training Loss: 0.170170
Trained 4601 batches 	Training Loss: 0.122402
Trained 4651 batches 	Training Loss: 0.175561
Trained 4701 batches 	Training Loss: 0.214975
Trained 4751 batches 	Training Loss: 0.176290
Trained 4801 batches 	Training Loss: 0.202210
Trained 4851 batches 	Training Loss: 0.176348
Trained 4901 batches 	Training Loss: 0.133670
Epoch: 1 	Training Loss: 0.174610
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.725
The AUROC of Atelectasis is 0.731184539490656
The AUROC of Cardiomegaly is 0.7355000907116114
The AUROC of Effusion is 0.8281938270716508
The AUROC of Infiltration is 0.6646719687958174
The AUROC of Mass is 0.6868457477660245
The AUROC of Nodule is 0.6292502534445992
The AUROC of Pneumonia is 0.6640081814198907
The AUROC of Pneumothorax is 0.7898692388674623
The AUROC of Consolidation is 0.718802045518234
The AUROC of Edema is 0.8325837581937363
The AUROC of Emphysema is 0.7005004990637035
The AUROC of Fibrosis is 0.6929323856583275
The AUROC of Pleural_Thickening is 0.6900897177256389
The AUROC of Hernia is 0.792283578490475
Started epoch 2
Trained 1 batches 	Training Loss: 0.217192
Trained 51 batches 	Training Loss: 0.193378
Trained 101 batches 	Training Loss: 0.108544
Trained 151 batches 	Training Loss: 0.202857
Trained 201 batches 	Training Loss: 0.136003
Trained 251 batches 	Training Loss: 0.182015
Trained 301 batches 	Training Loss: 0.161617
Trained 351 batches 	Training Loss: 0.152996
Trained 401 batches 	Training Loss: 0.169259
Trained 451 batches 	Training Loss: 0.170111
Trained 501 batches 	Training Loss: 0.163606
Trained 551 batches 	Training Loss: 0.192779
Trained 601 batches 	Training Loss: 0.257139
Trained 651 batches 	Training Loss: 0.180647
Trained 701 batches 	Training Loss: 0.157006
Trained 751 batches 	Training Loss: 0.193801
Trained 801 batches 	Training Loss: 0.130671
Trained 851 batches 	Training Loss: 0.125666
Trained 901 batches 	Training Loss: 0.102855
Trained 951 batches 	Training Loss: 0.143045
Trained 1001 batches 	Training Loss: 0.213720
Trained 1051 batches 	Training Loss: 0.146397
Trained 1101 batches 	Training Loss: 0.179031
Trained 1151 batches 	Training Loss: 0.178598
Trained 1201 batches 	Training Loss: 0.180602
Trained 1251 batches 	Training Loss: 0.212014
Trained 1301 batches 	Training Loss: 0.099460
Trained 1351 batches 	Training Loss: 0.234246
Trained 1401 batches 	Training Loss: 0.145211
Trained 1451 batches 	Training Loss: 0.155267
Trained 1501 batches 	Training Loss: 0.189530
Trained 1551 batches 	Training Loss: 0.215308
Trained 1601 batches 	Training Loss: 0.135817
Trained 1651 batches 	Training Loss: 0.142030
Trained 1701 batches 	Training Loss: 0.270461
Trained 1751 batches 	Training Loss: 0.169441
Trained 1801 batches 	Training Loss: 0.190402
Trained 1851 batches 	Training Loss: 0.122046
Trained 1901 batches 	Training Loss: 0.100658
Trained 1951 batches 	Training Loss: 0.226977
Trained 2001 batches 	Training Loss: 0.153513
Trained 2051 batches 	Training Loss: 0.135330
Trained 2101 batches 	Training Loss: 0.107267
Trained 2151 batches 	Training Loss: 0.123338
Trained 2201 batches 	Training Loss: 0.113278
Trained 2251 batches 	Training Loss: 0.165258
Trained 2301 batches 	Training Loss: 0.147795
Trained 2351 batches 	Training Loss: 0.140591
Trained 2401 batches 	Training Loss: 0.167042
Trained 2451 batches 	Training Loss: 0.205143
Trained 2501 batches 	Training Loss: 0.139848
Trained 2551 batches 	Training Loss: 0.148534
Trained 2601 batches 	Training Loss: 0.123116
Trained 2651 batches 	Training Loss: 0.164433
Trained 2701 batches 	Training Loss: 0.150531
Trained 2751 batches 	Training Loss: 0.365084
Trained 2801 batches 	Training Loss: 0.179638
Trained 2851 batches 	Training Loss: 0.227736
Trained 2901 batches 	Training Loss: 0.179394
Trained 2951 batches 	Training Loss: 0.174813
Trained 3001 batches 	Training Loss: 0.130051
Trained 3051 batches 	Training Loss: 0.180722
Trained 3101 batches 	Training Loss: 0.131129
Trained 3151 batches 	Training Loss: 0.107365
Trained 3201 batches 	Training Loss: 0.169921
Trained 3251 batches 	Training Loss: 0.199413
Trained 3301 batches 	Training Loss: 0.192693
Trained 3351 batches 	Training Loss: 0.185261
Trained 3401 batches 	Training Loss: 0.257762
Trained 3451 batches 	Training Loss: 0.215137
Trained 3501 batches 	Training Loss: 0.071108
Trained 3551 batches 	Training Loss: 0.140712
Trained 3601 batches 	Training Loss: 0.201200
Trained 3651 batches 	Training Loss: 0.102624
Trained 3701 batches 	Training Loss: 0.160493
Trained 3751 batches 	Training Loss: 0.130269
Trained 3801 batches 	Training Loss: 0.109552
Trained 3851 batches 	Training Loss: 0.138127
Trained 3901 batches 	Training Loss: 0.201518
Trained 3951 batches 	Training Loss: 0.131784
Trained 4001 batches 	Training Loss: 0.217808
Trained 4051 batches 	Training Loss: 0.168504
Trained 4101 batches 	Training Loss: 0.186371
Trained 4151 batches 	Training Loss: 0.121598
Trained 4201 batches 	Training Loss: 0.151875
Trained 4251 batches 	Training Loss: 0.144694
Trained 4301 batches 	Training Loss: 0.213736
Trained 4351 batches 	Training Loss: 0.144078
Trained 4401 batches 	Training Loss: 0.126048
Trained 4451 batches 	Training Loss: 0.224372
Trained 4501 batches 	Training Loss: 0.172687
Trained 4551 batches 	Training Loss: 0.124057
Trained 4601 batches 	Training Loss: 0.083481
Trained 4651 batches 	Training Loss: 0.167657
Trained 4701 batches 	Training Loss: 0.222999
Trained 4751 batches 	Training Loss: 0.144821
Trained 4801 batches 	Training Loss: 0.230030
Trained 4851 batches 	Training Loss: 0.164432
Trained 4901 batches 	Training Loss: 0.136812
Epoch: 2 	Training Loss: 0.166773
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.745
The AUROC of Atelectasis is 0.7469261307770814
The AUROC of Cardiomegaly is 0.8170005014079699
The AUROC of Effusion is 0.8435437413621628
The AUROC of Infiltration is 0.6675991590362113
The AUROC of Mass is 0.7105554183506454
The AUROC of Nodule is 0.6519765111611936
The AUROC of Pneumonia is 0.6909073375621464
The AUROC of Pneumothorax is 0.8046456644092845
The AUROC of Consolidation is 0.7322623301106908
The AUROC of Edema is 0.8418137125074487
The AUROC of Emphysema is 0.7302325136322416
The AUROC of Fibrosis is 0.7098414673054803
The AUROC of Pleural_Thickening is 0.7217145946994566
The AUROC of Hernia is 0.7540000229655403
Started epoch 3
Trained 1 batches 	Training Loss: 0.183525
Trained 51 batches 	Training Loss: 0.166122
Trained 101 batches 	Training Loss: 0.106779
Trained 151 batches 	Training Loss: 0.133391
Trained 201 batches 	Training Loss: 0.155073
Trained 251 batches 	Training Loss: 0.148417
Trained 301 batches 	Training Loss: 0.234599
Trained 351 batches 	Training Loss: 0.153756
Trained 401 batches 	Training Loss: 0.202845
Trained 451 batches 	Training Loss: 0.152537
Trained 501 batches 	Training Loss: 0.145950
Trained 551 batches 	Training Loss: 0.226430
Trained 601 batches 	Training Loss: 0.146452
Trained 651 batches 	Training Loss: 0.124093
Trained 701 batches 	Training Loss: 0.129893
Trained 751 batches 	Training Loss: 0.147066
Trained 801 batches 	Training Loss: 0.134838
Trained 851 batches 	Training Loss: 0.138229
Trained 901 batches 	Training Loss: 0.162015
Trained 951 batches 	Training Loss: 0.117979
Trained 1001 batches 	Training Loss: 0.147768
Trained 1051 batches 	Training Loss: 0.203551
Trained 1101 batches 	Training Loss: 0.169021
Trained 1151 batches 	Training Loss: 0.092240
Trained 1201 batches 	Training Loss: 0.144592
Trained 1251 batches 	Training Loss: 0.111066
Trained 1301 batches 	Training Loss: 0.128982
Trained 1351 batches 	Training Loss: 0.154404
Trained 1401 batches 	Training Loss: 0.075109
Trained 1451 batches 	Training Loss: 0.184809
Trained 1501 batches 	Training Loss: 0.167372
Trained 1551 batches 	Training Loss: 0.155360
Trained 1601 batches 	Training Loss: 0.132850
Trained 1651 batches 	Training Loss: 0.107036
Trained 1701 batches 	Training Loss: 0.236658
Trained 1751 batches 	Training Loss: 0.162797
Trained 1801 batches 	Training Loss: 0.182676
Trained 1851 batches 	Training Loss: 0.202637
Trained 1901 batches 	Training Loss: 0.169490
Trained 1951 batches 	Training Loss: 0.155452
Trained 2001 batches 	Training Loss: 0.201004
Trained 2051 batches 	Training Loss: 0.220818
Trained 2101 batches 	Training Loss: 0.123672
Trained 2151 batches 	Training Loss: 0.225648
Trained 2201 batches 	Training Loss: 0.150461
Trained 2251 batches 	Training Loss: 0.121926
Trained 2301 batches 	Training Loss: 0.137622
Trained 2351 batches 	Training Loss: 0.188902
Trained 2401 batches 	Training Loss: 0.195231
Trained 2451 batches 	Training Loss: 0.217915
Trained 2501 batches 	Training Loss: 0.156471
Trained 2551 batches 	Training Loss: 0.190658
Trained 2601 batches 	Training Loss: 0.128005
Trained 2651 batches 	Training Loss: 0.158811
Trained 2701 batches 	Training Loss: 0.213896
Trained 2751 batches 	Training Loss: 0.183474
Trained 2801 batches 	Training Loss: 0.183305
Trained 2851 batches 	Training Loss: 0.161291
Trained 2901 batches 	Training Loss: 0.221383
Trained 2951 batches 	Training Loss: 0.129186
Trained 3001 batches 	Training Loss: 0.156225
Trained 3051 batches 	Training Loss: 0.229107
Trained 3101 batches 	Training Loss: 0.154072
Trained 3151 batches 	Training Loss: 0.152373
Trained 3201 batches 	Training Loss: 0.181051
Trained 3251 batches 	Training Loss: 0.140032
Trained 3301 batches 	Training Loss: 0.158576
Trained 3351 batches 	Training Loss: 0.177385
Trained 3401 batches 	Training Loss: 0.207658
Trained 3451 batches 	Training Loss: 0.207333
Trained 3501 batches 	Training Loss: 0.140138
Trained 3551 batches 	Training Loss: 0.114003
Trained 3601 batches 	Training Loss: 0.144484
Trained 3651 batches 	Training Loss: 0.175789
Trained 3701 batches 	Training Loss: 0.134814
Trained 3751 batches 	Training Loss: 0.221048
Trained 3801 batches 	Training Loss: 0.220777
Trained 3851 batches 	Training Loss: 0.094560
Trained 3901 batches 	Training Loss: 0.122717
Trained 3951 batches 	Training Loss: 0.070477
Trained 4001 batches 	Training Loss: 0.173847
Trained 4051 batches 	Training Loss: 0.192897
Trained 4101 batches 	Training Loss: 0.162411
Trained 4151 batches 	Training Loss: 0.289241
Trained 4201 batches 	Training Loss: 0.152397
Trained 4251 batches 	Training Loss: 0.170883
Trained 4301 batches 	Training Loss: 0.182202
Trained 4351 batches 	Training Loss: 0.211761
Trained 4401 batches 	Training Loss: 0.176042
Trained 4451 batches 	Training Loss: 0.155840
Trained 4501 batches 	Training Loss: 0.214283
Trained 4551 batches 	Training Loss: 0.111096
Trained 4601 batches 	Training Loss: 0.232966
Trained 4651 batches 	Training Loss: 0.122474
Trained 4701 batches 	Training Loss: 0.133581
Trained 4751 batches 	Training Loss: 0.139130
Trained 4801 batches 	Training Loss: 0.131883
Trained 4851 batches 	Training Loss: 0.147056
Trained 4901 batches 	Training Loss: 0.137205
Epoch: 3 	Training Loss: 0.161631
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.751
The AUROC of Atelectasis is 0.7400919349861027
The AUROC of Cardiomegaly is 0.8809479381860412
The AUROC of Effusion is 0.8505940986849269
The AUROC of Infiltration is 0.667161000304296
The AUROC of Mass is 0.7309810820545689
The AUROC of Nodule is 0.668954053223009
The AUROC of Pneumonia is 0.6757255176486274
The AUROC of Pneumothorax is 0.8089356709384838
The AUROC of Consolidation is 0.7419682719375342
The AUROC of Edema is 0.8743511222935841
The AUROC of Emphysema is 0.7147122213192948
The AUROC of Fibrosis is 0.7148518833298032
The AUROC of Pleural_Thickening is 0.7284781091210568
The AUROC of Hernia is 0.7138953001021966
Started epoch 4
Trained 1 batches 	Training Loss: 0.190902
Trained 51 batches 	Training Loss: 0.191839
Trained 101 batches 	Training Loss: 0.155797
Trained 151 batches 	Training Loss: 0.166295
Trained 201 batches 	Training Loss: 0.221042
Trained 251 batches 	Training Loss: 0.252792
Trained 301 batches 	Training Loss: 0.138053
Trained 351 batches 	Training Loss: 0.177712
Trained 401 batches 	Training Loss: 0.181953
Trained 451 batches 	Training Loss: 0.140250
Trained 501 batches 	Training Loss: 0.257060
Trained 551 batches 	Training Loss: 0.175541
Trained 601 batches 	Training Loss: 0.156743
Trained 651 batches 	Training Loss: 0.325272
Trained 701 batches 	Training Loss: 0.177949
Trained 751 batches 	Training Loss: 0.170916
Trained 801 batches 	Training Loss: 0.172041
Trained 851 batches 	Training Loss: 0.163616
Trained 901 batches 	Training Loss: 0.143115
Trained 951 batches 	Training Loss: 0.159346
Trained 1001 batches 	Training Loss: 0.163807
Trained 1051 batches 	Training Loss: 0.165210
Trained 1101 batches 	Training Loss: 0.136499
Trained 1151 batches 	Training Loss: 0.201425
Trained 1201 batches 	Training Loss: 0.175696
Trained 1251 batches 	Training Loss: 0.206079
Trained 1301 batches 	Training Loss: 0.212403
Trained 1351 batches 	Training Loss: 0.196993
Trained 1401 batches 	Training Loss: 0.177468
Trained 1451 batches 	Training Loss: 0.126946
Trained 1501 batches 	Training Loss: 0.155128
Trained 1551 batches 	Training Loss: 0.136680
Trained 1601 batches 	Training Loss: 0.175114
Trained 1651 batches 	Training Loss: 0.190608
Trained 1701 batches 	Training Loss: 0.106948
Trained 1751 batches 	Training Loss: 0.142453
Trained 1801 batches 	Training Loss: 0.136281
Trained 1851 batches 	Training Loss: 0.269416
Trained 1901 batches 	Training Loss: 0.090252
Trained 1951 batches 	Training Loss: 0.144673
Trained 2001 batches 	Training Loss: 0.167671
Trained 2051 batches 	Training Loss: 0.065647
Trained 2101 batches 	Training Loss: 0.179298
Trained 2151 batches 	Training Loss: 0.101123
Trained 2201 batches 	Training Loss: 0.186578
Trained 2251 batches 	Training Loss: 0.209891
Trained 2301 batches 	Training Loss: 0.164490
Trained 2351 batches 	Training Loss: 0.148248
Trained 2401 batches 	Training Loss: 0.194851
Trained 2451 batches 	Training Loss: 0.164548
Trained 2501 batches 	Training Loss: 0.156838
Trained 2551 batches 	Training Loss: 0.168973
Trained 2601 batches 	Training Loss: 0.189618
Trained 2651 batches 	Training Loss: 0.194952
Trained 2701 batches 	Training Loss: 0.173073
Trained 2751 batches 	Training Loss: 0.085754
Trained 2801 batches 	Training Loss: 0.127726
Trained 2851 batches 	Training Loss: 0.181836
Trained 2901 batches 	Training Loss: 0.210909
Trained 2951 batches 	Training Loss: 0.151985
Trained 3001 batches 	Training Loss: 0.145456
Trained 3051 batches 	Training Loss: 0.164355
Trained 3101 batches 	Training Loss: 0.152322
Trained 3151 batches 	Training Loss: 0.097526
Trained 3201 batches 	Training Loss: 0.181767
Trained 3251 batches 	Training Loss: 0.116161
Trained 3301 batches 	Training Loss: 0.194313
Trained 3351 batches 	Training Loss: 0.209807
Trained 3401 batches 	Training Loss: 0.221559
Trained 3451 batches 	Training Loss: 0.185840
Trained 3501 batches 	Training Loss: 0.137182
Trained 3551 batches 	Training Loss: 0.189961
Trained 3601 batches 	Training Loss: 0.063000
Trained 3651 batches 	Training Loss: 0.210560
Trained 3701 batches 	Training Loss: 0.152010
Trained 3751 batches 	Training Loss: 0.202415
Trained 3801 batches 	Training Loss: 0.126925
Trained 3851 batches 	Training Loss: 0.135855
Trained 3901 batches 	Training Loss: 0.182862
Trained 3951 batches 	Training Loss: 0.081461
Trained 4001 batches 	Training Loss: 0.192633
Trained 4051 batches 	Training Loss: 0.136947
Trained 4101 batches 	Training Loss: 0.132877
Trained 4151 batches 	Training Loss: 0.140227
Trained 4201 batches 	Training Loss: 0.163878
Trained 4251 batches 	Training Loss: 0.218114
Trained 4301 batches 	Training Loss: 0.158120
Trained 4351 batches 	Training Loss: 0.176744
Trained 4401 batches 	Training Loss: 0.184948
Trained 4451 batches 	Training Loss: 0.208398
Trained 4501 batches 	Training Loss: 0.155049
Trained 4551 batches 	Training Loss: 0.165316
Trained 4601 batches 	Training Loss: 0.115961
Trained 4651 batches 	Training Loss: 0.185545
Trained 4701 batches 	Training Loss: 0.094014
Trained 4751 batches 	Training Loss: 0.129143
Trained 4801 batches 	Training Loss: 0.157631
Trained 4851 batches 	Training Loss: 0.173765
Trained 4901 batches 	Training Loss: 0.161632
Epoch: 4 	Training Loss: 0.158608
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.776
The AUROC of Atelectasis is 0.7620685950906761
The AUROC of Cardiomegaly is 0.8542511968944482
The AUROC of Effusion is 0.8618684114867321
The AUROC of Infiltration is 0.6835286176657722
The AUROC of Mass is 0.7787377707387613
The AUROC of Nodule is 0.7016131989524053
The AUROC of Pneumonia is 0.711230522696535
The AUROC of Pneumothorax is 0.8258630310668409
The AUROC of Consolidation is 0.7380118174175552
The AUROC of Edema is 0.8964257928888301
The AUROC of Emphysema is 0.7741537999505353
The AUROC of Fibrosis is 0.7469852513385329
The AUROC of Pleural_Thickening is 0.7504632106172047
The AUROC of Hernia is 0.774205105239588
Started epoch 5
Trained 1 batches 	Training Loss: 0.171848
Trained 51 batches 	Training Loss: 0.172193
Trained 101 batches 	Training Loss: 0.128457
Trained 151 batches 	Training Loss: 0.148466
Trained 201 batches 	Training Loss: 0.150414
Trained 251 batches 	Training Loss: 0.145705
Trained 301 batches 	Training Loss: 0.152754
Trained 351 batches 	Training Loss: 0.156575
Trained 401 batches 	Training Loss: 0.184300
Trained 451 batches 	Training Loss: 0.164146
Trained 501 batches 	Training Loss: 0.118701
Trained 551 batches 	Training Loss: 0.187491
Trained 601 batches 	Training Loss: 0.113622
Trained 651 batches 	Training Loss: 0.187196
Trained 701 batches 	Training Loss: 0.140205
Trained 751 batches 	Training Loss: 0.127888
Trained 801 batches 	Training Loss: 0.103087
Trained 851 batches 	Training Loss: 0.142538
Trained 901 batches 	Training Loss: 0.193396
Trained 951 batches 	Training Loss: 0.145165
Trained 1001 batches 	Training Loss: 0.231812
Trained 1051 batches 	Training Loss: 0.150289
Trained 1101 batches 	Training Loss: 0.153514
Trained 1151 batches 	Training Loss: 0.192742
Trained 1201 batches 	Training Loss: 0.232470
Trained 1251 batches 	Training Loss: 0.130742
Trained 1301 batches 	Training Loss: 0.206435
Trained 1351 batches 	Training Loss: 0.098880
Trained 1401 batches 	Training Loss: 0.225108
Trained 1451 batches 	Training Loss: 0.234516
Trained 1501 batches 	Training Loss: 0.192090
Trained 1551 batches 	Training Loss: 0.070291
Trained 1601 batches 	Training Loss: 0.118825
Trained 1651 batches 	Training Loss: 0.167319
Trained 1701 batches 	Training Loss: 0.071005
Trained 1751 batches 	Training Loss: 0.113451
Trained 1801 batches 	Training Loss: 0.127786
Trained 1851 batches 	Training Loss: 0.154558
Trained 1901 batches 	Training Loss: 0.193664
Trained 1951 batches 	Training Loss: 0.123851
Trained 2001 batches 	Training Loss: 0.165192
Trained 2051 batches 	Training Loss: 0.320696
Trained 2101 batches 	Training Loss: 0.209317
Trained 2151 batches 	Training Loss: 0.208849
Trained 2201 batches 	Training Loss: 0.135835
Trained 2251 batches 	Training Loss: 0.105409
Trained 2301 batches 	Training Loss: 0.194772
Trained 2351 batches 	Training Loss: 0.149303
Trained 2401 batches 	Training Loss: 0.100903
Trained 2451 batches 	Training Loss: 0.209655
Trained 2501 batches 	Training Loss: 0.153416
Trained 2551 batches 	Training Loss: 0.148242
Trained 2601 batches 	Training Loss: 0.075149
Trained 2651 batches 	Training Loss: 0.144951
Trained 2701 batches 	Training Loss: 0.075775
Trained 2751 batches 	Training Loss: 0.155169
Trained 2801 batches 	Training Loss: 0.102829
Trained 2851 batches 	Training Loss: 0.211056
Trained 2901 batches 	Training Loss: 0.194383
Trained 2951 batches 	Training Loss: 0.128860
Trained 3001 batches 	Training Loss: 0.154978
Trained 3051 batches 	Training Loss: 0.286837
Trained 3101 batches 	Training Loss: 0.119340
Trained 3151 batches 	Training Loss: 0.151750
Trained 3201 batches 	Training Loss: 0.140183
Trained 3251 batches 	Training Loss: 0.105831
Trained 3301 batches 	Training Loss: 0.188546
Trained 3351 batches 	Training Loss: 0.130850
Trained 3401 batches 	Training Loss: 0.091797
Trained 3451 batches 	Training Loss: 0.181150
Trained 3501 batches 	Training Loss: 0.169337
Trained 3551 batches 	Training Loss: 0.133714
Trained 3601 batches 	Training Loss: 0.164016
Trained 3651 batches 	Training Loss: 0.207092
Trained 3701 batches 	Training Loss: 0.185644
Trained 3751 batches 	Training Loss: 0.160043
Trained 3801 batches 	Training Loss: 0.205542
Trained 3851 batches 	Training Loss: 0.104922
Trained 3901 batches 	Training Loss: 0.154233
Trained 3951 batches 	Training Loss: 0.080923
Trained 4001 batches 	Training Loss: 0.235171
Trained 4051 batches 	Training Loss: 0.201436
Trained 4101 batches 	Training Loss: 0.184404
Trained 4151 batches 	Training Loss: 0.106304
Trained 4201 batches 	Training Loss: 0.184134
Trained 4251 batches 	Training Loss: 0.222779
Trained 4301 batches 	Training Loss: 0.239430
Trained 4351 batches 	Training Loss: 0.175674
Trained 4401 batches 	Training Loss: 0.201335
Trained 4451 batches 	Training Loss: 0.151709
Trained 4501 batches 	Training Loss: 0.149202
Trained 4551 batches 	Training Loss: 0.084135
Trained 4601 batches 	Training Loss: 0.145279
Trained 4651 batches 	Training Loss: 0.179444
Trained 4701 batches 	Training Loss: 0.265753
Trained 4751 batches 	Training Loss: 0.083901
Trained 4801 batches 	Training Loss: 0.155444
Trained 4851 batches 	Training Loss: 0.154370
Trained 4901 batches 	Training Loss: 0.122817
Epoch: 5 	Training Loss: 0.156343
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.785
The AUROC of Atelectasis is 0.764402143876378
The AUROC of Cardiomegaly is 0.8926270119595223
The AUROC of Effusion is 0.865519174384211
The AUROC of Infiltration is 0.6832306011231292
The AUROC of Mass is 0.7619625944837888
The AUROC of Nodule is 0.7077886147192631
The AUROC of Pneumonia is 0.7379350248585633
The AUROC of Pneumothorax is 0.8431636690556451
The AUROC of Consolidation is 0.7489828733425044
The AUROC of Edema is 0.8963757200556182
The AUROC of Emphysema is 0.7907616536527342
The AUROC of Fibrosis is 0.7484220460449938
The AUROC of Pleural_Thickening is 0.746622404989042
The AUROC of Hernia is 0.8054037916106882
Started epoch 6
Trained 1 batches 	Training Loss: 0.186904
Trained 51 batches 	Training Loss: 0.143696
Trained 101 batches 	Training Loss: 0.139480
Trained 151 batches 	Training Loss: 0.170191
Trained 201 batches 	Training Loss: 0.098873
Trained 251 batches 	Training Loss: 0.150759
Trained 301 batches 	Training Loss: 0.107560
Trained 351 batches 	Training Loss: 0.176247
Trained 401 batches 	Training Loss: 0.228032
Trained 451 batches 	Training Loss: 0.180959
Trained 501 batches 	Training Loss: 0.135862
Trained 551 batches 	Training Loss: 0.125671
Trained 601 batches 	Training Loss: 0.176736
Trained 651 batches 	Training Loss: 0.159936
Trained 701 batches 	Training Loss: 0.212916
Trained 751 batches 	Training Loss: 0.174974
Trained 801 batches 	Training Loss: 0.125161
Trained 851 batches 	Training Loss: 0.168600
Trained 901 batches 	Training Loss: 0.109846
Trained 951 batches 	Training Loss: 0.213159
Trained 1001 batches 	Training Loss: 0.173960
Trained 1051 batches 	Training Loss: 0.117258
Trained 1101 batches 	Training Loss: 0.228357
Trained 1151 batches 	Training Loss: 0.220384
Trained 1201 batches 	Training Loss: 0.153747
Trained 1251 batches 	Training Loss: 0.148881
Trained 1301 batches 	Training Loss: 0.171329
Trained 1351 batches 	Training Loss: 0.120240
Trained 1401 batches 	Training Loss: 0.132309
Trained 1451 batches 	Training Loss: 0.166647
Trained 1501 batches 	Training Loss: 0.149509
Trained 1551 batches 	Training Loss: 0.165019
Trained 1601 batches 	Training Loss: 0.133184
Trained 1651 batches 	Training Loss: 0.160550
Trained 1701 batches 	Training Loss: 0.225617
Trained 1751 batches 	Training Loss: 0.120116
Trained 1801 batches 	Training Loss: 0.155949
Trained 1851 batches 	Training Loss: 0.146566
Trained 1901 batches 	Training Loss: 0.127396
Trained 1951 batches 	Training Loss: 0.180578
Trained 2001 batches 	Training Loss: 0.134332
Trained 2051 batches 	Training Loss: 0.232879
Trained 2101 batches 	Training Loss: 0.107866
Trained 2151 batches 	Training Loss: 0.120182
Trained 2201 batches 	Training Loss: 0.213011
Trained 2251 batches 	Training Loss: 0.178961
Trained 2301 batches 	Training Loss: 0.119347
Trained 2351 batches 	Training Loss: 0.193106
Trained 2401 batches 	Training Loss: 0.224680
Trained 2451 batches 	Training Loss: 0.177039
Trained 2501 batches 	Training Loss: 0.133857
Trained 2551 batches 	Training Loss: 0.083447
Trained 2601 batches 	Training Loss: 0.104879
Trained 2651 batches 	Training Loss: 0.157526
Trained 2701 batches 	Training Loss: 0.136383
Trained 2751 batches 	Training Loss: 0.120062
Trained 2801 batches 	Training Loss: 0.168077
Trained 2851 batches 	Training Loss: 0.146640
Trained 2901 batches 	Training Loss: 0.233444
Trained 2951 batches 	Training Loss: 0.205025
Trained 3001 batches 	Training Loss: 0.088147
Trained 3051 batches 	Training Loss: 0.155570
Trained 3101 batches 	Training Loss: 0.127316
Trained 3151 batches 	Training Loss: 0.137409
Trained 3201 batches 	Training Loss: 0.202094
Trained 3251 batches 	Training Loss: 0.141948
Trained 3301 batches 	Training Loss: 0.109609
Trained 3351 batches 	Training Loss: 0.109618
Trained 3401 batches 	Training Loss: 0.178811
Trained 3451 batches 	Training Loss: 0.160297
Trained 3501 batches 	Training Loss: 0.165447
Trained 3551 batches 	Training Loss: 0.188317
Trained 3601 batches 	Training Loss: 0.191611
Trained 3651 batches 	Training Loss: 0.144799
Trained 3701 batches 	Training Loss: 0.216978
Trained 3751 batches 	Training Loss: 0.134598
Trained 3801 batches 	Training Loss: 0.159607
Trained 3851 batches 	Training Loss: 0.096001
Trained 3901 batches 	Training Loss: 0.161055
Trained 3951 batches 	Training Loss: 0.174429
Trained 4001 batches 	Training Loss: 0.117660
Trained 4051 batches 	Training Loss: 0.193227
Trained 4101 batches 	Training Loss: 0.128894
Trained 4151 batches 	Training Loss: 0.162215
Trained 4201 batches 	Training Loss: 0.083735
Trained 4251 batches 	Training Loss: 0.199571
Trained 4301 batches 	Training Loss: 0.204523
Trained 4351 batches 	Training Loss: 0.135084
Trained 4401 batches 	Training Loss: 0.154151
Trained 4451 batches 	Training Loss: 0.087899
Trained 4501 batches 	Training Loss: 0.150307
Trained 4551 batches 	Training Loss: 0.134303
Trained 4601 batches 	Training Loss: 0.188875
Trained 4651 batches 	Training Loss: 0.210389
Trained 4701 batches 	Training Loss: 0.211989
Trained 4751 batches 	Training Loss: 0.136053
Trained 4801 batches 	Training Loss: 0.148321
Trained 4851 batches 	Training Loss: 0.119011
Trained 4901 batches 	Training Loss: 0.144485
Epoch: 6 	Training Loss: 0.154772
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.784
The AUROC of Atelectasis is 0.765432650911939
The AUROC of Cardiomegaly is 0.8901569218501916
The AUROC of Effusion is 0.8667946496613137
The AUROC of Infiltration is 0.6788734404824477
The AUROC of Mass is 0.7793441024106277
The AUROC of Nodule is 0.6812763477152888
The AUROC of Pneumonia is 0.7304709078614016
The AUROC of Pneumothorax is 0.8310095995677569
The AUROC of Consolidation is 0.7399095383214235
The AUROC of Edema is 0.8994446467589221
The AUROC of Emphysema is 0.7988909450117183
The AUROC of Fibrosis is 0.7243729606979407
The AUROC of Pleural_Thickening is 0.7373999352868406
The AUROC of Hernia is 0.845662383593418
Started epoch 7
Trained 1 batches 	Training Loss: 0.147981
Trained 51 batches 	Training Loss: 0.185823
Trained 101 batches 	Training Loss: 0.129919
Trained 151 batches 	Training Loss: 0.161667
Trained 201 batches 	Training Loss: 0.178485
Trained 251 batches 	Training Loss: 0.149199
Trained 301 batches 	Training Loss: 0.194607
Trained 351 batches 	Training Loss: 0.080855
Trained 401 batches 	Training Loss: 0.207081
Trained 451 batches 	Training Loss: 0.278702
Trained 501 batches 	Training Loss: 0.122381
Trained 551 batches 	Training Loss: 0.101880
Trained 601 batches 	Training Loss: 0.173114
Trained 651 batches 	Training Loss: 0.134577
Trained 701 batches 	Training Loss: 0.163279
Trained 751 batches 	Training Loss: 0.181670
Trained 801 batches 	Training Loss: 0.184490
Trained 851 batches 	Training Loss: 0.194227
Trained 901 batches 	Training Loss: 0.163164
Trained 951 batches 	Training Loss: 0.156494
Trained 1001 batches 	Training Loss: 0.103769
Trained 1051 batches 	Training Loss: 0.135229
Trained 1101 batches 	Training Loss: 0.153269
Trained 1151 batches 	Training Loss: 0.145520
Trained 1201 batches 	Training Loss: 0.198955
Trained 1251 batches 	Training Loss: 0.182854
Trained 1301 batches 	Training Loss: 0.091539
Trained 1351 batches 	Training Loss: 0.121493
Trained 1401 batches 	Training Loss: 0.112055
Trained 1451 batches 	Training Loss: 0.132896
Trained 1501 batches 	Training Loss: 0.129574
Trained 1551 batches 	Training Loss: 0.214299
Trained 1601 batches 	Training Loss: 0.137129
Trained 1651 batches 	Training Loss: 0.167268
Trained 1701 batches 	Training Loss: 0.132790
Trained 1751 batches 	Training Loss: 0.095525
Trained 1801 batches 	Training Loss: 0.086545
Trained 1851 batches 	Training Loss: 0.094062
Trained 1901 batches 	Training Loss: 0.173593
Trained 1951 batches 	Training Loss: 0.111569
Trained 2001 batches 	Training Loss: 0.180673
Trained 2051 batches 	Training Loss: 0.180326
Trained 2101 batches 	Training Loss: 0.193671
Trained 2151 batches 	Training Loss: 0.091148
Trained 2201 batches 	Training Loss: 0.117827
Trained 2251 batches 	Training Loss: 0.149514
Trained 2301 batches 	Training Loss: 0.140718
Trained 2351 batches 	Training Loss: 0.183010
Trained 2401 batches 	Training Loss: 0.120427
Trained 2451 batches 	Training Loss: 0.185661
Trained 2501 batches 	Training Loss: 0.150248
Trained 2551 batches 	Training Loss: 0.136607
Trained 2601 batches 	Training Loss: 0.256480
Trained 2651 batches 	Training Loss: 0.218857
Trained 2701 batches 	Training Loss: 0.189538
Trained 2751 batches 	Training Loss: 0.195934
Trained 2801 batches 	Training Loss: 0.110586
Trained 2851 batches 	Training Loss: 0.143374
Trained 2901 batches 	Training Loss: 0.137985
Trained 2951 batches 	Training Loss: 0.219459
Trained 3001 batches 	Training Loss: 0.165410
Trained 3051 batches 	Training Loss: 0.159016
Trained 3101 batches 	Training Loss: 0.140023
Trained 3151 batches 	Training Loss: 0.163274
Trained 3201 batches 	Training Loss: 0.106641
Trained 3251 batches 	Training Loss: 0.205921
Trained 3301 batches 	Training Loss: 0.089786
Trained 3351 batches 	Training Loss: 0.151853
Trained 3401 batches 	Training Loss: 0.115967
Trained 3451 batches 	Training Loss: 0.181548
Trained 3501 batches 	Training Loss: 0.133176
Trained 3551 batches 	Training Loss: 0.188691
Trained 3601 batches 	Training Loss: 0.232702
Trained 3651 batches 	Training Loss: 0.136887
Trained 3701 batches 	Training Loss: 0.145075
Trained 3751 batches 	Training Loss: 0.143551
Trained 3801 batches 	Training Loss: 0.140740
Trained 3851 batches 	Training Loss: 0.113426
Trained 3901 batches 	Training Loss: 0.141292
Trained 3951 batches 	Training Loss: 0.168587
Trained 4001 batches 	Training Loss: 0.100978
Trained 4051 batches 	Training Loss: 0.135475
Trained 4101 batches 	Training Loss: 0.196430
Trained 4151 batches 	Training Loss: 0.117451
Trained 4201 batches 	Training Loss: 0.132849
Trained 4251 batches 	Training Loss: 0.193870
Trained 4301 batches 	Training Loss: 0.167054
Trained 4351 batches 	Training Loss: 0.200518
Trained 4401 batches 	Training Loss: 0.144893
Trained 4451 batches 	Training Loss: 0.145439
Trained 4501 batches 	Training Loss: 0.112474
Trained 4551 batches 	Training Loss: 0.085504
Trained 4601 batches 	Training Loss: 0.127447
Trained 4651 batches 	Training Loss: 0.234762
Trained 4701 batches 	Training Loss: 0.101643
Trained 4751 batches 	Training Loss: 0.144361
Trained 4801 batches 	Training Loss: 0.157861
Trained 4851 batches 	Training Loss: 0.232590
Trained 4901 batches 	Training Loss: 0.170557
Epoch: 7 	Training Loss: 0.153354
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.788
The AUROC of Atelectasis is 0.7733114838877838
The AUROC of Cardiomegaly is 0.9035951727381863
The AUROC of Effusion is 0.872165211727346
The AUROC of Infiltration is 0.6828020692135328
The AUROC of Mass is 0.7948963525417264
The AUROC of Nodule is 0.69601116369061
The AUROC of Pneumonia is 0.723443125321447
The AUROC of Pneumothorax is 0.8344928672294938
The AUROC of Consolidation is 0.7416098246016279
The AUROC of Edema is 0.89418079189565
The AUROC of Emphysema is 0.8013763264200497
The AUROC of Fibrosis is 0.7300316636415064
The AUROC of Pleural_Thickening is 0.755266268591461
The AUROC of Hernia is 0.8262748745507366
Started epoch 8
Trained 1 batches 	Training Loss: 0.154222
Trained 51 batches 	Training Loss: 0.102813
Trained 101 batches 	Training Loss: 0.166373
Trained 151 batches 	Training Loss: 0.114033
Trained 201 batches 	Training Loss: 0.137151
Trained 251 batches 	Training Loss: 0.164341
Trained 301 batches 	Training Loss: 0.131295
Trained 351 batches 	Training Loss: 0.115067
Trained 401 batches 	Training Loss: 0.158747
Trained 451 batches 	Training Loss: 0.195729
Trained 501 batches 	Training Loss: 0.142414
Trained 551 batches 	Training Loss: 0.206103
Trained 601 batches 	Training Loss: 0.250591
Trained 651 batches 	Training Loss: 0.112826
Trained 701 batches 	Training Loss: 0.170433
Trained 751 batches 	Training Loss: 0.117873
Trained 801 batches 	Training Loss: 0.155055
Trained 851 batches 	Training Loss: 0.147903
Trained 901 batches 	Training Loss: 0.150950
Trained 951 batches 	Training Loss: 0.098169
Trained 1001 batches 	Training Loss: 0.160424
Trained 1051 batches 	Training Loss: 0.116637
Trained 1101 batches 	Training Loss: 0.093961
Trained 1151 batches 	Training Loss: 0.130068
Trained 1201 batches 	Training Loss: 0.092240
Trained 1251 batches 	Training Loss: 0.147297
Trained 1301 batches 	Training Loss: 0.115931
Trained 1351 batches 	Training Loss: 0.154847
Trained 1401 batches 	Training Loss: 0.157155
Trained 1451 batches 	Training Loss: 0.172552
Trained 1501 batches 	Training Loss: 0.157312
Trained 1551 batches 	Training Loss: 0.209481
Trained 1601 batches 	Training Loss: 0.132012
Trained 1651 batches 	Training Loss: 0.152785
Trained 1701 batches 	Training Loss: 0.118812
Trained 1751 batches 	Training Loss: 0.197443
Trained 1801 batches 	Training Loss: 0.104755
Trained 1851 batches 	Training Loss: 0.155325
Trained 1901 batches 	Training Loss: 0.223464
Trained 1951 batches 	Training Loss: 0.196696
Trained 2001 batches 	Training Loss: 0.164598
Trained 2051 batches 	Training Loss: 0.065999
Trained 2101 batches 	Training Loss: 0.138246
Trained 2151 batches 	Training Loss: 0.233070
Trained 2201 batches 	Training Loss: 0.177387
Trained 2251 batches 	Training Loss: 0.150597
Trained 2301 batches 	Training Loss: 0.087600
Trained 2351 batches 	Training Loss: 0.140247
Trained 2401 batches 	Training Loss: 0.139544
Trained 2451 batches 	Training Loss: 0.125660
Trained 2501 batches 	Training Loss: 0.161909
Trained 2551 batches 	Training Loss: 0.123641
Trained 2601 batches 	Training Loss: 0.119677
Trained 2651 batches 	Training Loss: 0.135208
Trained 2701 batches 	Training Loss: 0.149027
Trained 2751 batches 	Training Loss: 0.103074
Trained 2801 batches 	Training Loss: 0.166191
Trained 2851 batches 	Training Loss: 0.176557
Trained 2901 batches 	Training Loss: 0.108892
Trained 2951 batches 	Training Loss: 0.224784
Trained 3001 batches 	Training Loss: 0.118479
Trained 3051 batches 	Training Loss: 0.120414
Trained 3101 batches 	Training Loss: 0.104332
Trained 3151 batches 	Training Loss: 0.141221
Trained 3201 batches 	Training Loss: 0.219925
Trained 3251 batches 	Training Loss: 0.139980
Trained 3301 batches 	Training Loss: 0.206078
Trained 3351 batches 	Training Loss: 0.163125
Trained 3401 batches 	Training Loss: 0.181962
Trained 3451 batches 	Training Loss: 0.134417
Trained 3501 batches 	Training Loss: 0.215821
Trained 3551 batches 	Training Loss: 0.233285
Trained 3601 batches 	Training Loss: 0.158289
Trained 3651 batches 	Training Loss: 0.196749
Trained 3701 batches 	Training Loss: 0.126149
Trained 3751 batches 	Training Loss: 0.188368
Trained 3801 batches 	Training Loss: 0.140976
Trained 3851 batches 	Training Loss: 0.140740
Trained 3901 batches 	Training Loss: 0.157856
Trained 3951 batches 	Training Loss: 0.117037
Trained 4001 batches 	Training Loss: 0.185769
Trained 4051 batches 	Training Loss: 0.164562
Trained 4101 batches 	Training Loss: 0.186167
Trained 4151 batches 	Training Loss: 0.141664
Trained 4201 batches 	Training Loss: 0.130510
Trained 4251 batches 	Training Loss: 0.144584
Trained 4301 batches 	Training Loss: 0.138483
Trained 4351 batches 	Training Loss: 0.116132
Trained 4401 batches 	Training Loss: 0.123976
Trained 4451 batches 	Training Loss: 0.142364
Trained 4501 batches 	Training Loss: 0.141378
Trained 4551 batches 	Training Loss: 0.130803
Trained 4601 batches 	Training Loss: 0.178997
Trained 4651 batches 	Training Loss: 0.081802
Trained 4701 batches 	Training Loss: 0.172563
Trained 4751 batches 	Training Loss: 0.167450
Trained 4801 batches 	Training Loss: 0.120678
Trained 4851 batches 	Training Loss: 0.199761
Trained 4901 batches 	Training Loss: 0.074118
Epoch: 8 	Training Loss: 0.151932
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.793
The AUROC of Atelectasis is 0.7735512643113134
The AUROC of Cardiomegaly is 0.9041435068776398
The AUROC of Effusion is 0.8764479552428558
The AUROC of Infiltration is 0.6863845749536639
The AUROC of Mass is 0.7919230430533558
The AUROC of Nodule is 0.7188532817954366
The AUROC of Pneumonia is 0.7308709307198507
The AUROC of Pneumothorax is 0.8352285093180277
The AUROC of Consolidation is 0.7497760948068326
The AUROC of Edema is 0.9053445507515064
The AUROC of Emphysema is 0.8187923983323324
The AUROC of Fibrosis is 0.7589198435317428
The AUROC of Pleural_Thickening is 0.7459927666998919
The AUROC of Hernia is 0.806143282005351
Started epoch 9
Trained 1 batches 	Training Loss: 0.151636
Trained 51 batches 	Training Loss: 0.239046
Trained 101 batches 	Training Loss: 0.181717
Trained 151 batches 	Training Loss: 0.102281
Trained 201 batches 	Training Loss: 0.099496
Trained 251 batches 	Training Loss: 0.147289
Trained 301 batches 	Training Loss: 0.149616
Trained 351 batches 	Training Loss: 0.209373
Trained 401 batches 	Training Loss: 0.179497
Trained 451 batches 	Training Loss: 0.093535
Trained 501 batches 	Training Loss: 0.119683
Trained 551 batches 	Training Loss: 0.142461
Trained 601 batches 	Training Loss: 0.157462
Trained 651 batches 	Training Loss: 0.092943
Trained 701 batches 	Training Loss: 0.136321
Trained 751 batches 	Training Loss: 0.160792
Trained 801 batches 	Training Loss: 0.158898
Trained 851 batches 	Training Loss: 0.182696
Trained 901 batches 	Training Loss: 0.133032
Trained 951 batches 	Training Loss: 0.182757
Trained 1001 batches 	Training Loss: 0.073216
Trained 1051 batches 	Training Loss: 0.138029
Trained 1101 batches 	Training Loss: 0.156288
Trained 1151 batches 	Training Loss: 0.148007
Trained 1201 batches 	Training Loss: 0.195557
Trained 1251 batches 	Training Loss: 0.150063
Trained 1301 batches 	Training Loss: 0.189881
Trained 1351 batches 	Training Loss: 0.235241
Trained 1401 batches 	Training Loss: 0.179594
Trained 1451 batches 	Training Loss: 0.271609
Trained 1501 batches 	Training Loss: 0.094623
Trained 1551 batches 	Training Loss: 0.112179
Trained 1601 batches 	Training Loss: 0.142047
Trained 1651 batches 	Training Loss: 0.170285
Trained 1701 batches 	Training Loss: 0.160477
Trained 1751 batches 	Training Loss: 0.211466
Trained 1801 batches 	Training Loss: 0.135624
Trained 1851 batches 	Training Loss: 0.153710
Trained 1901 batches 	Training Loss: 0.093264
Trained 1951 batches 	Training Loss: 0.139899
Trained 2001 batches 	Training Loss: 0.226412
Trained 2051 batches 	Training Loss: 0.263523
Trained 2101 batches 	Training Loss: 0.137648
Trained 2151 batches 	Training Loss: 0.154784
Trained 2201 batches 	Training Loss: 0.189460
Trained 2251 batches 	Training Loss: 0.238674
Trained 2301 batches 	Training Loss: 0.103321
Trained 2351 batches 	Training Loss: 0.212974
Trained 2401 batches 	Training Loss: 0.070856
Trained 2451 batches 	Training Loss: 0.196592
Trained 2501 batches 	Training Loss: 0.135223
Trained 2551 batches 	Training Loss: 0.134510
Trained 2601 batches 	Training Loss: 0.157755
Trained 2651 batches 	Training Loss: 0.125201
Trained 2701 batches 	Training Loss: 0.216344
Trained 2751 batches 	Training Loss: 0.286257
Trained 2801 batches 	Training Loss: 0.181369
Trained 2851 batches 	Training Loss: 0.173609
Trained 2901 batches 	Training Loss: 0.201900
Trained 2951 batches 	Training Loss: 0.107781
Trained 3001 batches 	Training Loss: 0.146260
Trained 3051 batches 	Training Loss: 0.078668
Trained 3101 batches 	Training Loss: 0.105109
Trained 3151 batches 	Training Loss: 0.145790
Trained 3201 batches 	Training Loss: 0.194994
Trained 3251 batches 	Training Loss: 0.246004
Trained 3301 batches 	Training Loss: 0.150184
Trained 3351 batches 	Training Loss: 0.191729
Trained 3401 batches 	Training Loss: 0.169605
Trained 3451 batches 	Training Loss: 0.123859
Trained 3501 batches 	Training Loss: 0.244218
Trained 3551 batches 	Training Loss: 0.116977
Trained 3601 batches 	Training Loss: 0.188359
Trained 3651 batches 	Training Loss: 0.151663
Trained 3701 batches 	Training Loss: 0.166095
Trained 3751 batches 	Training Loss: 0.118852
Trained 3801 batches 	Training Loss: 0.089832
Trained 3851 batches 	Training Loss: 0.124462
Trained 3901 batches 	Training Loss: 0.144047
Trained 3951 batches 	Training Loss: 0.092522
Trained 4001 batches 	Training Loss: 0.133822
Trained 4051 batches 	Training Loss: 0.188854
Trained 4101 batches 	Training Loss: 0.208588
Trained 4151 batches 	Training Loss: 0.122165
Trained 4201 batches 	Training Loss: 0.169482
Trained 4251 batches 	Training Loss: 0.144406
Trained 4301 batches 	Training Loss: 0.143300
Trained 4351 batches 	Training Loss: 0.191912
Trained 4401 batches 	Training Loss: 0.221027
Trained 4451 batches 	Training Loss: 0.170596
Trained 4501 batches 	Training Loss: 0.181476
Trained 4551 batches 	Training Loss: 0.105738
Trained 4601 batches 	Training Loss: 0.086265
Trained 4651 batches 	Training Loss: 0.123880
Trained 4701 batches 	Training Loss: 0.177459
Trained 4751 batches 	Training Loss: 0.137761
Trained 4801 batches 	Training Loss: 0.136161
Trained 4851 batches 	Training Loss: 0.126821
Trained 4901 batches 	Training Loss: 0.238847
Epoch: 9 	Training Loss: 0.150991
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.794
The AUROC of Atelectasis is 0.7695325115722466
The AUROC of Cardiomegaly is 0.9046005751596505
The AUROC of Effusion is 0.8750984505864966
The AUROC of Infiltration is 0.6869682702149437
The AUROC of Mass is 0.7916986688843101
The AUROC of Nodule is 0.730671540793344
The AUROC of Pneumonia is 0.7283666209497686
The AUROC of Pneumothorax is 0.8443802227266745
The AUROC of Consolidation is 0.749619261658196
The AUROC of Edema is 0.8865751009733166
The AUROC of Emphysema is 0.8270552886030927
The AUROC of Fibrosis is 0.7309528852521429
The AUROC of Pleural_Thickening is 0.7569654112748082
The AUROC of Hernia is 0.8323653358136117
Started epoch 10
Trained 1 batches 	Training Loss: 0.192303
Trained 51 batches 	Training Loss: 0.172199
Trained 101 batches 	Training Loss: 0.082566
Trained 151 batches 	Training Loss: 0.150294
Trained 201 batches 	Training Loss: 0.095277
Trained 251 batches 	Training Loss: 0.169648
Trained 301 batches 	Training Loss: 0.169915
Trained 351 batches 	Training Loss: 0.116999
Trained 401 batches 	Training Loss: 0.096452
Trained 451 batches 	Training Loss: 0.124275
Trained 501 batches 	Training Loss: 0.095408
Trained 551 batches 	Training Loss: 0.226549
Trained 601 batches 	Training Loss: 0.068924
Trained 651 batches 	Training Loss: 0.159368
Trained 701 batches 	Training Loss: 0.122011
Trained 751 batches 	Training Loss: 0.089509
Trained 801 batches 	Training Loss: 0.197795
Trained 851 batches 	Training Loss: 0.195170
Trained 901 batches 	Training Loss: 0.150713
Trained 951 batches 	Training Loss: 0.185617
Trained 1001 batches 	Training Loss: 0.180971
Trained 1051 batches 	Training Loss: 0.113381
Trained 1101 batches 	Training Loss: 0.182879
Trained 1151 batches 	Training Loss: 0.177297
Trained 1201 batches 	Training Loss: 0.198954
Trained 1251 batches 	Training Loss: 0.111618
Trained 1301 batches 	Training Loss: 0.134806
Trained 1351 batches 	Training Loss: 0.110264
Trained 1401 batches 	Training Loss: 0.149614
Trained 1451 batches 	Training Loss: 0.129645
Trained 1501 batches 	Training Loss: 0.244185
Trained 1551 batches 	Training Loss: 0.142706
Trained 1601 batches 	Training Loss: 0.115449
Trained 1651 batches 	Training Loss: 0.205436
Trained 1701 batches 	Training Loss: 0.221070
Trained 1751 batches 	Training Loss: 0.158430
Trained 1801 batches 	Training Loss: 0.252286
Trained 1851 batches 	Training Loss: 0.147330
Trained 1901 batches 	Training Loss: 0.159543
Trained 1951 batches 	Training Loss: 0.130781
Trained 2001 batches 	Training Loss: 0.161016
Trained 2051 batches 	Training Loss: 0.172668
Trained 2101 batches 	Training Loss: 0.122655
Trained 2151 batches 	Training Loss: 0.261226
Trained 2201 batches 	Training Loss: 0.145344
Trained 2251 batches 	Training Loss: 0.182812
Trained 2301 batches 	Training Loss: 0.128640
Trained 2351 batches 	Training Loss: 0.124502
Trained 2401 batches 	Training Loss: 0.125743
Trained 2451 batches 	Training Loss: 0.079533
Trained 2501 batches 	Training Loss: 0.173302
Trained 2551 batches 	Training Loss: 0.182456
Trained 2601 batches 	Training Loss: 0.144500
Trained 2651 batches 	Training Loss: 0.207459
Trained 2701 batches 	Training Loss: 0.222938
Trained 2751 batches 	Training Loss: 0.108746
Trained 2801 batches 	Training Loss: 0.199151
Trained 2851 batches 	Training Loss: 0.192448
Trained 2901 batches 	Training Loss: 0.136038
Trained 2951 batches 	Training Loss: 0.140237
Trained 3001 batches 	Training Loss: 0.085933
Trained 3051 batches 	Training Loss: 0.125546
Trained 3101 batches 	Training Loss: 0.112941
Trained 3151 batches 	Training Loss: 0.097438
Trained 3201 batches 	Training Loss: 0.161235
Trained 3251 batches 	Training Loss: 0.141437
Trained 3301 batches 	Training Loss: 0.146747
Trained 3351 batches 	Training Loss: 0.128242
Trained 3401 batches 	Training Loss: 0.148614
Trained 3451 batches 	Training Loss: 0.141015
Trained 3501 batches 	Training Loss: 0.177059
Trained 3551 batches 	Training Loss: 0.114831
Trained 3601 batches 	Training Loss: 0.150718
Trained 3651 batches 	Training Loss: 0.152849
Trained 3701 batches 	Training Loss: 0.139391
Trained 3751 batches 	Training Loss: 0.164186
Trained 3801 batches 	Training Loss: 0.143060
Trained 3851 batches 	Training Loss: 0.205204
Trained 3901 batches 	Training Loss: 0.164537
Trained 3951 batches 	Training Loss: 0.138391
Trained 4001 batches 	Training Loss: 0.147852
Trained 4051 batches 	Training Loss: 0.143448
Trained 4101 batches 	Training Loss: 0.135605
Trained 4151 batches 	Training Loss: 0.131516
Trained 4201 batches 	Training Loss: 0.167919
Trained 4251 batches 	Training Loss: 0.101151
Trained 4301 batches 	Training Loss: 0.139763
Trained 4351 batches 	Training Loss: 0.233593
Trained 4401 batches 	Training Loss: 0.126092
Trained 4451 batches 	Training Loss: 0.168614
Trained 4501 batches 	Training Loss: 0.130835
Trained 4551 batches 	Training Loss: 0.164359
Trained 4601 batches 	Training Loss: 0.127957
Trained 4651 batches 	Training Loss: 0.139586
Trained 4701 batches 	Training Loss: 0.112533
Trained 4751 batches 	Training Loss: 0.117471
Trained 4801 batches 	Training Loss: 0.130259
Trained 4851 batches 	Training Loss: 0.183848
Trained 4901 batches 	Training Loss: 0.147008
Epoch: 10 	Training Loss: 0.150074
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
Evaluating time lapse: 0.0 min
The average AUROC is 0.800
The AUROC of Atelectasis is 0.7754770264945916
The AUROC of Cardiomegaly is 0.8998373841948966
The AUROC of Effusion is 0.8789100779227489
The AUROC of Infiltration is 0.6877851392846275
The AUROC of Mass is 0.8008956096488673
The AUROC of Nodule is 0.732276749374147
The AUROC of Pneumonia is 0.7369647313179801
The AUROC of Pneumothorax is 0.849926799578892
The AUROC of Consolidation is 0.7547465910683125
The AUROC of Edema is 0.9048264417665365
The AUROC of Emphysema is 0.8302623985678786
The AUROC of Fibrosis is 0.7405619022850909
The AUROC of Pleural_Thickening is 0.7618297561336951
The AUROC of Hernia is 0.844656492932355
Training time lapse: 132.0 min
Evaluating test data...	 test_loader: 1402
Evaluating time lapse: 1.0 min
The average AUROC is 0.803
The AUROC of Atelectasis is 0.7872099410604104
The AUROC of Cardiomegaly is 0.8942040956129902
The AUROC of Effusion is 0.8659846622354542
The AUROC of Infiltration is 0.6940528367666631
The AUROC of Mass is 0.8343512107809103
The AUROC of Nodule is 0.7258501195145368
The AUROC of Pneumonia is 0.7102397293043025
The AUROC of Pneumothorax is 0.8399510198767162
The AUROC of Consolidation is 0.8001061990828091
The AUROC of Edema is 0.8704102049561369
The AUROC of Emphysema is 0.8684485676318707
The AUROC of Fibrosis is 0.7667735240099012
The AUROC of Pleural_Thickening is 0.774243968448369
The AUROC of Hernia is 0.8085592449027688
