Started training, total epoch : 10
Training data size: 4906
Started epoch 1
Trained 50 batches 	Training Loss: 0.103711
Trained 100 batches 	Training Loss: 0.195165
Trained 150 batches 	Training Loss: 0.271419
Trained 200 batches 	Training Loss: 0.123045
Trained 250 batches 	Training Loss: 0.227768
Trained 300 batches 	Training Loss: 0.202499
Trained 350 batches 	Training Loss: 0.286259
Trained 400 batches 	Training Loss: 0.122552
Trained 450 batches 	Training Loss: 0.156001
Trained 500 batches 	Training Loss: 0.147392
Trained 550 batches 	Training Loss: 0.169983
Trained 600 batches 	Training Loss: 0.152146
Trained 650 batches 	Training Loss: 0.164434
Trained 700 batches 	Training Loss: 0.128404
Trained 750 batches 	Training Loss: 0.159096
Trained 800 batches 	Training Loss: 0.191747
Trained 850 batches 	Training Loss: 0.159457
Trained 900 batches 	Training Loss: 0.159168
Trained 950 batches 	Training Loss: 0.143862
Trained 1000 batches 	Training Loss: 0.179936
Trained 1050 batches 	Training Loss: 0.136864
Trained 1100 batches 	Training Loss: 0.111161
Trained 1150 batches 	Training Loss: 0.111974
Trained 1200 batches 	Training Loss: 0.113393
Trained 1250 batches 	Training Loss: 0.118035
Trained 1300 batches 	Training Loss: 0.186843
Trained 1350 batches 	Training Loss: 0.253444
Trained 1400 batches 	Training Loss: 0.165880
Trained 1450 batches 	Training Loss: 0.138622
Trained 1500 batches 	Training Loss: 0.172737
Trained 1550 batches 	Training Loss: 0.159221
Trained 1600 batches 	Training Loss: 0.248082
Trained 1650 batches 	Training Loss: 0.131910
Trained 1700 batches 	Training Loss: 0.205847
Trained 1750 batches 	Training Loss: 0.194406
Trained 1800 batches 	Training Loss: 0.176223
Trained 1850 batches 	Training Loss: 0.121229
Trained 1900 batches 	Training Loss: 0.128238
Trained 1950 batches 	Training Loss: 0.185882
Trained 2000 batches 	Training Loss: 0.169707
Trained 2050 batches 	Training Loss: 0.188131
Trained 2100 batches 	Training Loss: 0.142967
Trained 2150 batches 	Training Loss: 0.181585
Trained 2200 batches 	Training Loss: 0.270820
Trained 2250 batches 	Training Loss: 0.090731
Trained 2300 batches 	Training Loss: 0.265798
Trained 2350 batches 	Training Loss: 0.149083
Trained 2400 batches 	Training Loss: 0.183094
Trained 2450 batches 	Training Loss: 0.111545
Trained 2500 batches 	Training Loss: 0.230652
Trained 2550 batches 	Training Loss: 0.156796
Trained 2600 batches 	Training Loss: 0.201592
Trained 2650 batches 	Training Loss: 0.185685
Trained 2700 batches 	Training Loss: 0.225855
Trained 2750 batches 	Training Loss: 0.122058
Trained 2800 batches 	Training Loss: 0.153365
Trained 2850 batches 	Training Loss: 0.150041
Trained 2900 batches 	Training Loss: 0.151533
Trained 2950 batches 	Training Loss: 0.107557
Trained 3000 batches 	Training Loss: 0.248596
Trained 3050 batches 	Training Loss: 0.299601
Trained 3100 batches 	Training Loss: 0.173147
Trained 3150 batches 	Training Loss: 0.128676
Trained 3200 batches 	Training Loss: 0.104748
Trained 3250 batches 	Training Loss: 0.190958
Trained 3300 batches 	Training Loss: 0.199470
Trained 3350 batches 	Training Loss: 0.202438
Trained 3400 batches 	Training Loss: 0.137278
Trained 3450 batches 	Training Loss: 0.172635
Trained 3500 batches 	Training Loss: 0.162082
Trained 3550 batches 	Training Loss: 0.173539
Trained 3600 batches 	Training Loss: 0.224621
Trained 3650 batches 	Training Loss: 0.109567
Trained 3700 batches 	Training Loss: 0.158988
Trained 3750 batches 	Training Loss: 0.196111
Trained 3800 batches 	Training Loss: 0.222070
Trained 3850 batches 	Training Loss: 0.166447
Trained 3900 batches 	Training Loss: 0.273671
Trained 3950 batches 	Training Loss: 0.214702
Trained 4000 batches 	Training Loss: 0.137659
Trained 4050 batches 	Training Loss: 0.138548
Trained 4100 batches 	Training Loss: 0.141621
Trained 4150 batches 	Training Loss: 0.205922
Trained 4200 batches 	Training Loss: 0.125594
Trained 4250 batches 	Training Loss: 0.267475
Trained 4300 batches 	Training Loss: 0.130707
Trained 4350 batches 	Training Loss: 0.091778
Trained 4400 batches 	Training Loss: 0.131249
Trained 4450 batches 	Training Loss: 0.108759
Trained 4500 batches 	Training Loss: 0.162729
Trained 4550 batches 	Training Loss: 0.211568
Trained 4600 batches 	Training Loss: 0.193693
Trained 4650 batches 	Training Loss: 0.208759
Trained 4700 batches 	Training Loss: 0.226219
Trained 4750 batches 	Training Loss: 0.138564
Trained 4800 batches 	Training Loss: 0.200948
Trained 4850 batches 	Training Loss: 0.138585
Trained 4900 batches 	Training Loss: 0.187224
Epoch: 1 	Training Loss: 0.174805
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 1.0 min
The average AUROC is 0.723
The AUROC of Atelectasis is 0.7234165622084121
The AUROC of Cardiomegaly is 0.7927167493216626
The AUROC of Effusion is 0.8307764006144318
The AUROC of Infiltration is 0.6676670706313013
The AUROC of Mass is 0.6859125074663268
The AUROC of Nodule is 0.6243779999603857
The AUROC of Pneumonia is 0.709814180046906
The AUROC of Pneumothorax is 0.7624789901926387
The AUROC of Consolidation is 0.7585713075028575
The AUROC of Edema is 0.8281835509315414
The AUROC of Emphysema is 0.7056414687245081
The AUROC of Fibrosis is 0.6641166894482193
The AUROC of Pleural_Thickening is 0.6898058748240521
The AUROC of Hernia is 0.6749003238146298
Started epoch 2
Trained 50 batches 	Training Loss: 0.158540
Trained 100 batches 	Training Loss: 0.156851
Trained 150 batches 	Training Loss: 0.199104
Trained 200 batches 	Training Loss: 0.170622
Trained 250 batches 	Training Loss: 0.134513
Trained 300 batches 	Training Loss: 0.181836
Trained 350 batches 	Training Loss: 0.219576
Trained 400 batches 	Training Loss: 0.186578
Trained 450 batches 	Training Loss: 0.278522
Trained 500 batches 	Training Loss: 0.114599
Trained 550 batches 	Training Loss: 0.168335
Trained 600 batches 	Training Loss: 0.141498
Trained 650 batches 	Training Loss: 0.323702
Trained 700 batches 	Training Loss: 0.228572
Trained 750 batches 	Training Loss: 0.171496
Trained 800 batches 	Training Loss: 0.150604
Trained 850 batches 	Training Loss: 0.257650
Trained 900 batches 	Training Loss: 0.164677
Trained 950 batches 	Training Loss: 0.207843
Trained 1000 batches 	Training Loss: 0.122299
Trained 1050 batches 	Training Loss: 0.117365
Trained 1100 batches 	Training Loss: 0.163127
Trained 1150 batches 	Training Loss: 0.133947
Trained 1200 batches 	Training Loss: 0.211567
Trained 1250 batches 	Training Loss: 0.146112
Trained 1300 batches 	Training Loss: 0.131389
Trained 1350 batches 	Training Loss: 0.179388
Trained 1400 batches 	Training Loss: 0.237692
Trained 1450 batches 	Training Loss: 0.154981
Trained 1500 batches 	Training Loss: 0.094414
Trained 1550 batches 	Training Loss: 0.110656
Trained 1600 batches 	Training Loss: 0.197307
Trained 1650 batches 	Training Loss: 0.105374
Trained 1700 batches 	Training Loss: 0.122904
Trained 1750 batches 	Training Loss: 0.125148
Trained 1800 batches 	Training Loss: 0.159138
Trained 1850 batches 	Training Loss: 0.099646
Trained 1900 batches 	Training Loss: 0.141403
Trained 1950 batches 	Training Loss: 0.161620
Trained 2000 batches 	Training Loss: 0.115296
Trained 2050 batches 	Training Loss: 0.184655
Trained 2100 batches 	Training Loss: 0.098877
Trained 2150 batches 	Training Loss: 0.188496
Trained 2200 batches 	Training Loss: 0.201497
Trained 2250 batches 	Training Loss: 0.204815
Trained 2300 batches 	Training Loss: 0.158674
Trained 2350 batches 	Training Loss: 0.188748
Trained 2400 batches 	Training Loss: 0.114644
Trained 2450 batches 	Training Loss: 0.207926
Trained 2500 batches 	Training Loss: 0.165359
Trained 2550 batches 	Training Loss: 0.254072
Trained 2600 batches 	Training Loss: 0.186813
Trained 2650 batches 	Training Loss: 0.199020
Trained 2700 batches 	Training Loss: 0.281189
Trained 2750 batches 	Training Loss: 0.150238
Trained 2800 batches 	Training Loss: 0.255214
Trained 2850 batches 	Training Loss: 0.105973
Trained 2900 batches 	Training Loss: 0.175576
Trained 2950 batches 	Training Loss: 0.180518
Trained 3000 batches 	Training Loss: 0.116682
Trained 3050 batches 	Training Loss: 0.178078
Trained 3100 batches 	Training Loss: 0.127471
Trained 3150 batches 	Training Loss: 0.229786
Trained 3200 batches 	Training Loss: 0.160571
Trained 3250 batches 	Training Loss: 0.177635
Trained 3300 batches 	Training Loss: 0.119677
Trained 3350 batches 	Training Loss: 0.097872
Trained 3400 batches 	Training Loss: 0.148777
Trained 3450 batches 	Training Loss: 0.072475
Trained 3500 batches 	Training Loss: 0.169841
Trained 3550 batches 	Training Loss: 0.168471
Trained 3600 batches 	Training Loss: 0.151579
Trained 3650 batches 	Training Loss: 0.131754
Trained 3700 batches 	Training Loss: 0.212339
Trained 3750 batches 	Training Loss: 0.116768
Trained 3800 batches 	Training Loss: 0.187702
Trained 3850 batches 	Training Loss: 0.146729
Trained 3900 batches 	Training Loss: 0.119675
Trained 3950 batches 	Training Loss: 0.129664
Trained 4000 batches 	Training Loss: 0.198905
Trained 4050 batches 	Training Loss: 0.205033
Trained 4100 batches 	Training Loss: 0.125803
Trained 4150 batches 	Training Loss: 0.171741
Trained 4200 batches 	Training Loss: 0.123886
Trained 4250 batches 	Training Loss: 0.124662
Trained 4300 batches 	Training Loss: 0.113474
Trained 4350 batches 	Training Loss: 0.118282
Trained 4400 batches 	Training Loss: 0.199009
Trained 4450 batches 	Training Loss: 0.156848
Trained 4500 batches 	Training Loss: 0.090440
Trained 4550 batches 	Training Loss: 0.218474
Trained 4600 batches 	Training Loss: 0.151665
Trained 4650 batches 	Training Loss: 0.231990
Trained 4700 batches 	Training Loss: 0.138652
Trained 4750 batches 	Training Loss: 0.142231
Trained 4800 batches 	Training Loss: 0.258336
Trained 4850 batches 	Training Loss: 0.223183
Trained 4900 batches 	Training Loss: 0.131920
Epoch: 2 	Training Loss: 0.164235
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.776
The AUROC of Atelectasis is 0.7614662239198031
The AUROC of Cardiomegaly is 0.8855665863385558
The AUROC of Effusion is 0.8554136983236168
The AUROC of Infiltration is 0.6839686528252088
The AUROC of Mass is 0.7671571722952478
The AUROC of Nodule is 0.6851339147837021
The AUROC of Pneumonia is 0.7290089888863116
The AUROC of Pneumothorax is 0.7938107530115449
The AUROC of Consolidation is 0.7711781182649404
The AUROC of Edema is 0.8643539480325758
The AUROC of Emphysema is 0.7610836206719923
The AUROC of Fibrosis is 0.7313099961621616
The AUROC of Pleural_Thickening is 0.7283457760272538
The AUROC of Hernia is 0.8496368255102952
Started epoch 3
Trained 50 batches 	Training Loss: 0.135944
Trained 100 batches 	Training Loss: 0.153842
Trained 150 batches 	Training Loss: 0.150315
Trained 200 batches 	Training Loss: 0.203012
Trained 250 batches 	Training Loss: 0.127181
Trained 300 batches 	Training Loss: 0.124356
Trained 350 batches 	Training Loss: 0.170980
Trained 400 batches 	Training Loss: 0.174448
Trained 450 batches 	Training Loss: 0.123133
Trained 500 batches 	Training Loss: 0.145604
Trained 550 batches 	Training Loss: 0.118219
Trained 600 batches 	Training Loss: 0.173435
Trained 650 batches 	Training Loss: 0.174934
Trained 700 batches 	Training Loss: 0.107367
Trained 750 batches 	Training Loss: 0.154340
Trained 800 batches 	Training Loss: 0.254888
Trained 850 batches 	Training Loss: 0.128746
Trained 900 batches 	Training Loss: 0.098250
Trained 950 batches 	Training Loss: 0.249567
Trained 1000 batches 	Training Loss: 0.156303
Trained 1050 batches 	Training Loss: 0.211002
Trained 1100 batches 	Training Loss: 0.159618
Trained 1150 batches 	Training Loss: 0.172117
Trained 1200 batches 	Training Loss: 0.142375
Trained 1250 batches 	Training Loss: 0.133429
Trained 1300 batches 	Training Loss: 0.236769
Trained 1350 batches 	Training Loss: 0.112635
Trained 1400 batches 	Training Loss: 0.177705
Trained 1450 batches 	Training Loss: 0.142845
Trained 1500 batches 	Training Loss: 0.111252
Trained 1550 batches 	Training Loss: 0.181242
Trained 1600 batches 	Training Loss: 0.145711
Trained 1650 batches 	Training Loss: 0.167109
Trained 1700 batches 	Training Loss: 0.219565
Trained 1750 batches 	Training Loss: 0.111417
Trained 1800 batches 	Training Loss: 0.204686
Trained 1850 batches 	Training Loss: 0.090780
Trained 1900 batches 	Training Loss: 0.143266
Trained 1950 batches 	Training Loss: 0.136759
Trained 2000 batches 	Training Loss: 0.215820
Trained 2050 batches 	Training Loss: 0.156415
Trained 2100 batches 	Training Loss: 0.202589
Trained 2150 batches 	Training Loss: 0.115118
Trained 2200 batches 	Training Loss: 0.186156
Trained 2250 batches 	Training Loss: 0.117676
Trained 2300 batches 	Training Loss: 0.196948
Trained 2350 batches 	Training Loss: 0.128408
Trained 2400 batches 	Training Loss: 0.181014
Trained 2450 batches 	Training Loss: 0.154561
Trained 2500 batches 	Training Loss: 0.157855
Trained 2550 batches 	Training Loss: 0.176439
Trained 2600 batches 	Training Loss: 0.164280
Trained 2650 batches 	Training Loss: 0.202171
Trained 2700 batches 	Training Loss: 0.136718
Trained 2750 batches 	Training Loss: 0.178052
Trained 2800 batches 	Training Loss: 0.219247
Trained 2850 batches 	Training Loss: 0.153164
Trained 2900 batches 	Training Loss: 0.164749
Trained 2950 batches 	Training Loss: 0.145940
Trained 3000 batches 	Training Loss: 0.192918
Trained 3050 batches 	Training Loss: 0.182469
Trained 3100 batches 	Training Loss: 0.227809
Trained 3150 batches 	Training Loss: 0.156448
Trained 3200 batches 	Training Loss: 0.142694
Trained 3250 batches 	Training Loss: 0.194598
Trained 3300 batches 	Training Loss: 0.165603
Trained 3350 batches 	Training Loss: 0.115066
Trained 3400 batches 	Training Loss: 0.136863
Trained 3450 batches 	Training Loss: 0.211287
Trained 3500 batches 	Training Loss: 0.181368
Trained 3550 batches 	Training Loss: 0.161362
Trained 3600 batches 	Training Loss: 0.182613
Trained 3650 batches 	Training Loss: 0.147273
Trained 3700 batches 	Training Loss: 0.130963
Trained 3750 batches 	Training Loss: 0.162551
Trained 3800 batches 	Training Loss: 0.141802
Trained 3850 batches 	Training Loss: 0.185581
Trained 3900 batches 	Training Loss: 0.098996
Trained 3950 batches 	Training Loss: 0.111500
Trained 4000 batches 	Training Loss: 0.208569
Trained 4050 batches 	Training Loss: 0.154857
Trained 4100 batches 	Training Loss: 0.143846
Trained 4150 batches 	Training Loss: 0.192911
Trained 4200 batches 	Training Loss: 0.156109
Trained 4250 batches 	Training Loss: 0.122448
Trained 4300 batches 	Training Loss: 0.160940
Trained 4350 batches 	Training Loss: 0.183578
Trained 4400 batches 	Training Loss: 0.165098
Trained 4450 batches 	Training Loss: 0.140035
Trained 4500 batches 	Training Loss: 0.142956
Trained 4550 batches 	Training Loss: 0.090953
Trained 4600 batches 	Training Loss: 0.153180
Trained 4650 batches 	Training Loss: 0.226417
Trained 4700 batches 	Training Loss: 0.123769
Trained 4750 batches 	Training Loss: 0.200771
Trained 4800 batches 	Training Loss: 0.148316
Trained 4850 batches 	Training Loss: 0.232620
Trained 4900 batches 	Training Loss: 0.082027
Epoch: 3 	Training Loss: 0.159998
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 1.0 min
The average AUROC is 0.786
The AUROC of Atelectasis is 0.7720416251294485
The AUROC of Cardiomegaly is 0.8866976041179793
The AUROC of Effusion is 0.8564377010813033
The AUROC of Infiltration is 0.690819276669985
The AUROC of Mass is 0.7898781076036528
The AUROC of Nodule is 0.7002667365052677
The AUROC of Pneumonia is 0.745771157100762
The AUROC of Pneumothorax is 0.8016776101448548
The AUROC of Consolidation is 0.7770516309822015
The AUROC of Edema is 0.8614425518129836
The AUROC of Emphysema is 0.8021573927094405
The AUROC of Fibrosis is 0.754106517080776
The AUROC of Pleural_Thickening is 0.7123234802518117
The AUROC of Hernia is 0.8474794796839297
Started epoch 4
Trained 50 batches 	Training Loss: 0.138116
Trained 100 batches 	Training Loss: 0.162189
Trained 150 batches 	Training Loss: 0.096408
Trained 200 batches 	Training Loss: 0.184367
Trained 250 batches 	Training Loss: 0.153894
Trained 300 batches 	Training Loss: 0.210516
Trained 350 batches 	Training Loss: 0.166244
Trained 400 batches 	Training Loss: 0.246981
Trained 450 batches 	Training Loss: 0.116591
Trained 500 batches 	Training Loss: 0.146792
Trained 550 batches 	Training Loss: 0.140783
Trained 600 batches 	Training Loss: 0.129248
Trained 650 batches 	Training Loss: 0.192774
Trained 700 batches 	Training Loss: 0.077361
Trained 750 batches 	Training Loss: 0.256301
Trained 800 batches 	Training Loss: 0.105266
Trained 850 batches 	Training Loss: 0.243420
Trained 900 batches 	Training Loss: 0.133737
Trained 950 batches 	Training Loss: 0.164035
Trained 1000 batches 	Training Loss: 0.122252
Trained 1050 batches 	Training Loss: 0.188038
Trained 1100 batches 	Training Loss: 0.231838
Trained 1150 batches 	Training Loss: 0.145071
Trained 1200 batches 	Training Loss: 0.191802
Trained 1250 batches 	Training Loss: 0.239311
Trained 1300 batches 	Training Loss: 0.164937
Trained 1350 batches 	Training Loss: 0.238515
Trained 1400 batches 	Training Loss: 0.211677
Trained 1450 batches 	Training Loss: 0.133659
Trained 1500 batches 	Training Loss: 0.117940
Trained 1550 batches 	Training Loss: 0.149157
Trained 1600 batches 	Training Loss: 0.181359
Trained 1650 batches 	Training Loss: 0.224285
Trained 1700 batches 	Training Loss: 0.147806
Trained 1750 batches 	Training Loss: 0.200365
Trained 1800 batches 	Training Loss: 0.160211
Trained 1850 batches 	Training Loss: 0.207288
Trained 1900 batches 	Training Loss: 0.173432
Trained 1950 batches 	Training Loss: 0.170231
Trained 2000 batches 	Training Loss: 0.110957
Trained 2050 batches 	Training Loss: 0.168512
Trained 2100 batches 	Training Loss: 0.181730
Trained 2150 batches 	Training Loss: 0.217937
Trained 2200 batches 	Training Loss: 0.112745
Trained 2250 batches 	Training Loss: 0.134432
Trained 2300 batches 	Training Loss: 0.149264
Trained 2350 batches 	Training Loss: 0.138189
Trained 2400 batches 	Training Loss: 0.123284
Trained 2450 batches 	Training Loss: 0.129587
Trained 2500 batches 	Training Loss: 0.139384
Trained 2550 batches 	Training Loss: 0.214421
Trained 2600 batches 	Training Loss: 0.147046
Trained 2650 batches 	Training Loss: 0.170427
Trained 2700 batches 	Training Loss: 0.114673
Trained 2750 batches 	Training Loss: 0.141871
Trained 2800 batches 	Training Loss: 0.133469
Trained 2850 batches 	Training Loss: 0.189258
Trained 2900 batches 	Training Loss: 0.125650
Trained 2950 batches 	Training Loss: 0.151538
Trained 3000 batches 	Training Loss: 0.138420
Trained 3050 batches 	Training Loss: 0.131945
Trained 3100 batches 	Training Loss: 0.139518
Trained 3150 batches 	Training Loss: 0.100507
Trained 3200 batches 	Training Loss: 0.118126
Trained 3250 batches 	Training Loss: 0.186210
Trained 3300 batches 	Training Loss: 0.174711
Trained 3350 batches 	Training Loss: 0.149031
Trained 3400 batches 	Training Loss: 0.176008
Trained 3450 batches 	Training Loss: 0.150344
Trained 3500 batches 	Training Loss: 0.236666
Trained 3550 batches 	Training Loss: 0.178358
Trained 3600 batches 	Training Loss: 0.189176
Trained 3650 batches 	Training Loss: 0.184183
Trained 3700 batches 	Training Loss: 0.153300
Trained 3750 batches 	Training Loss: 0.192244
Trained 3800 batches 	Training Loss: 0.133077
Trained 3850 batches 	Training Loss: 0.152044
Trained 3900 batches 	Training Loss: 0.165624
Trained 3950 batches 	Training Loss: 0.170569
Trained 4000 batches 	Training Loss: 0.154687
Trained 4050 batches 	Training Loss: 0.165764
Trained 4100 batches 	Training Loss: 0.158839
Trained 4150 batches 	Training Loss: 0.095344
Trained 4200 batches 	Training Loss: 0.112317
Trained 4250 batches 	Training Loss: 0.151483
Trained 4300 batches 	Training Loss: 0.151782
Trained 4350 batches 	Training Loss: 0.227307
Trained 4400 batches 	Training Loss: 0.170790
Trained 4450 batches 	Training Loss: 0.146971
Trained 4500 batches 	Training Loss: 0.133611
Trained 4550 batches 	Training Loss: 0.186143
Trained 4600 batches 	Training Loss: 0.072539
Trained 4650 batches 	Training Loss: 0.157859
Trained 4700 batches 	Training Loss: 0.174499
Trained 4750 batches 	Training Loss: 0.134051
Trained 4800 batches 	Training Loss: 0.177570
Trained 4850 batches 	Training Loss: 0.119265
Trained 4900 batches 	Training Loss: 0.199020
Epoch: 4 	Training Loss: 0.156862
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.789
The AUROC of Atelectasis is 0.7719522288896741
The AUROC of Cardiomegaly is 0.882074286303956
The AUROC of Effusion is 0.8535640905077531
The AUROC of Infiltration is 0.6921534036165135
The AUROC of Mass is 0.7999108312808217
The AUROC of Nodule is 0.700748344084223
The AUROC of Pneumonia is 0.7124193534530896
The AUROC of Pneumothorax is 0.8173678846507275
The AUROC of Consolidation is 0.7742423439510471
The AUROC of Edema is 0.869012784679479
The AUROC of Emphysema is 0.8115434609199167
The AUROC of Fibrosis is 0.7578252089107305
The AUROC of Pleural_Thickening is 0.729701891353312
The AUROC of Hernia is 0.8705890362578773
Started epoch 5
Trained 50 batches 	Training Loss: 0.101968
Trained 100 batches 	Training Loss: 0.164613
Trained 150 batches 	Training Loss: 0.130123
Trained 200 batches 	Training Loss: 0.091521
Trained 250 batches 	Training Loss: 0.104123
Trained 300 batches 	Training Loss: 0.142722
Trained 350 batches 	Training Loss: 0.133630
Trained 400 batches 	Training Loss: 0.194590
Trained 450 batches 	Training Loss: 0.169544
Trained 500 batches 	Training Loss: 0.136002
Trained 550 batches 	Training Loss: 0.140510
Trained 600 batches 	Training Loss: 0.197059
Trained 650 batches 	Training Loss: 0.142568
Trained 700 batches 	Training Loss: 0.136925
Trained 750 batches 	Training Loss: 0.108200
Trained 800 batches 	Training Loss: 0.183425
Trained 850 batches 	Training Loss: 0.137598
Trained 900 batches 	Training Loss: 0.137320
Trained 950 batches 	Training Loss: 0.187283
Trained 1000 batches 	Training Loss: 0.180619
Trained 1050 batches 	Training Loss: 0.203503
Trained 1100 batches 	Training Loss: 0.084474
Trained 1150 batches 	Training Loss: 0.195213
Trained 1200 batches 	Training Loss: 0.120358
Trained 1250 batches 	Training Loss: 0.155775
Trained 1300 batches 	Training Loss: 0.179410
Trained 1350 batches 	Training Loss: 0.182433
Trained 1400 batches 	Training Loss: 0.171667
Trained 1450 batches 	Training Loss: 0.079068
Trained 1500 batches 	Training Loss: 0.187545
Trained 1550 batches 	Training Loss: 0.250459
Trained 1600 batches 	Training Loss: 0.122183
Trained 1650 batches 	Training Loss: 0.184745
Trained 1700 batches 	Training Loss: 0.146116
Trained 1750 batches 	Training Loss: 0.158679
Trained 1800 batches 	Training Loss: 0.228593
Trained 1850 batches 	Training Loss: 0.165429
Trained 1900 batches 	Training Loss: 0.194304
Trained 1950 batches 	Training Loss: 0.131674
Trained 2000 batches 	Training Loss: 0.182005
Trained 2050 batches 	Training Loss: 0.123645
Trained 2100 batches 	Training Loss: 0.134968
Trained 2150 batches 	Training Loss: 0.154833
Trained 2200 batches 	Training Loss: 0.092291
Trained 2250 batches 	Training Loss: 0.139134
Trained 2300 batches 	Training Loss: 0.212542
Trained 2350 batches 	Training Loss: 0.104561
Trained 2400 batches 	Training Loss: 0.118169
Trained 2450 batches 	Training Loss: 0.164847
Trained 2500 batches 	Training Loss: 0.116517
Trained 2550 batches 	Training Loss: 0.074671
Trained 2600 batches 	Training Loss: 0.136574
Trained 2650 batches 	Training Loss: 0.115768
Trained 2700 batches 	Training Loss: 0.270132
Trained 2750 batches 	Training Loss: 0.177956
Trained 2800 batches 	Training Loss: 0.224394
Trained 2850 batches 	Training Loss: 0.156183
Trained 2900 batches 	Training Loss: 0.167478
Trained 2950 batches 	Training Loss: 0.165094
Trained 3000 batches 	Training Loss: 0.244692
Trained 3050 batches 	Training Loss: 0.175735
Trained 3100 batches 	Training Loss: 0.051536
Trained 3150 batches 	Training Loss: 0.164245
Trained 3200 batches 	Training Loss: 0.186078
Trained 3250 batches 	Training Loss: 0.141863
Trained 3300 batches 	Training Loss: 0.119815
Trained 3350 batches 	Training Loss: 0.105813
Trained 3400 batches 	Training Loss: 0.124268
Trained 3450 batches 	Training Loss: 0.147858
Trained 3500 batches 	Training Loss: 0.186903
Trained 3550 batches 	Training Loss: 0.132594
Trained 3600 batches 	Training Loss: 0.175782
Trained 3650 batches 	Training Loss: 0.149388
Trained 3700 batches 	Training Loss: 0.111001
Trained 3750 batches 	Training Loss: 0.193488
Trained 3800 batches 	Training Loss: 0.153692
Trained 3850 batches 	Training Loss: 0.156047
Trained 3900 batches 	Training Loss: 0.182487
Trained 3950 batches 	Training Loss: 0.183181
Trained 4000 batches 	Training Loss: 0.139563
Trained 4050 batches 	Training Loss: 0.168892
Trained 4100 batches 	Training Loss: 0.185225
Trained 4150 batches 	Training Loss: 0.140130
Trained 4200 batches 	Training Loss: 0.075350
Trained 4250 batches 	Training Loss: 0.121876
Trained 4300 batches 	Training Loss: 0.200615
Trained 4350 batches 	Training Loss: 0.140542
Trained 4400 batches 	Training Loss: 0.198284
Trained 4450 batches 	Training Loss: 0.102915
Trained 4500 batches 	Training Loss: 0.204773
Trained 4550 batches 	Training Loss: 0.201198
Trained 4600 batches 	Training Loss: 0.222622
Trained 4650 batches 	Training Loss: 0.088941
Trained 4700 batches 	Training Loss: 0.191654
Trained 4750 batches 	Training Loss: 0.139811
Trained 4800 batches 	Training Loss: 0.176153
Trained 4850 batches 	Training Loss: 0.151131
Trained 4900 batches 	Training Loss: 0.142185
Epoch: 5 	Training Loss: 0.154559
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.801
The AUROC of Atelectasis is 0.7825741314181578
The AUROC of Cardiomegaly is 0.8962386412611312
The AUROC of Effusion is 0.8673576253658772
The AUROC of Infiltration is 0.6924742858451132
The AUROC of Mass is 0.8149272164213361
The AUROC of Nodule is 0.7039810936930314
The AUROC of Pneumonia is 0.7422560701471039
The AUROC of Pneumothorax is 0.8301760981070291
The AUROC of Consolidation is 0.7810187106514739
The AUROC of Edema is 0.8836108244121835
The AUROC of Emphysema is 0.8390526141422364
The AUROC of Fibrosis is 0.7693315395479972
The AUROC of Pleural_Thickening is 0.7406570742486773
The AUROC of Hernia is 0.8686061503504091
Started epoch 6
Trained 50 batches 	Training Loss: 0.151529
Trained 100 batches 	Training Loss: 0.198185
Trained 150 batches 	Training Loss: 0.213836
Trained 200 batches 	Training Loss: 0.176061
Trained 250 batches 	Training Loss: 0.154784
Trained 300 batches 	Training Loss: 0.175782
Trained 350 batches 	Training Loss: 0.171860
Trained 400 batches 	Training Loss: 0.189781
Trained 450 batches 	Training Loss: 0.147386
Trained 500 batches 	Training Loss: 0.154862
Trained 550 batches 	Training Loss: 0.124050
Trained 600 batches 	Training Loss: 0.183277
Trained 650 batches 	Training Loss: 0.159955
Trained 700 batches 	Training Loss: 0.141965
Trained 750 batches 	Training Loss: 0.155855
Trained 800 batches 	Training Loss: 0.148133
Trained 850 batches 	Training Loss: 0.143778
Trained 900 batches 	Training Loss: 0.079150
Trained 950 batches 	Training Loss: 0.260018
Trained 1000 batches 	Training Loss: 0.126396
Trained 1050 batches 	Training Loss: 0.113254
Trained 1100 batches 	Training Loss: 0.139401
Trained 1150 batches 	Training Loss: 0.272663
Trained 1200 batches 	Training Loss: 0.192322
Trained 1250 batches 	Training Loss: 0.125394
Trained 1300 batches 	Training Loss: 0.167019
Trained 1350 batches 	Training Loss: 0.156856
Trained 1400 batches 	Training Loss: 0.164076
Trained 1450 batches 	Training Loss: 0.207825
Trained 1500 batches 	Training Loss: 0.080210
Trained 1550 batches 	Training Loss: 0.202478
Trained 1600 batches 	Training Loss: 0.164794
Trained 1650 batches 	Training Loss: 0.153012
Trained 1700 batches 	Training Loss: 0.209688
Trained 1750 batches 	Training Loss: 0.142640
Trained 1800 batches 	Training Loss: 0.152968
Trained 1850 batches 	Training Loss: 0.081674
Trained 1900 batches 	Training Loss: 0.202523
Trained 1950 batches 	Training Loss: 0.181967
Trained 2000 batches 	Training Loss: 0.085113
Trained 2050 batches 	Training Loss: 0.208603
Trained 2100 batches 	Training Loss: 0.144137
Trained 2150 batches 	Training Loss: 0.138711
Trained 2200 batches 	Training Loss: 0.183939
Trained 2250 batches 	Training Loss: 0.106720
Trained 2300 batches 	Training Loss: 0.090410
Trained 2350 batches 	Training Loss: 0.189130
Trained 2400 batches 	Training Loss: 0.113880
Trained 2450 batches 	Training Loss: 0.109654
Trained 2500 batches 	Training Loss: 0.116825
Trained 2550 batches 	Training Loss: 0.186944
Trained 2600 batches 	Training Loss: 0.217420
Trained 2650 batches 	Training Loss: 0.251937
Trained 2700 batches 	Training Loss: 0.246285
Trained 2750 batches 	Training Loss: 0.097780
Trained 2800 batches 	Training Loss: 0.204534
Trained 2850 batches 	Training Loss: 0.129088
Trained 2900 batches 	Training Loss: 0.223650
Trained 2950 batches 	Training Loss: 0.153168
Trained 3000 batches 	Training Loss: 0.139828
Trained 3050 batches 	Training Loss: 0.232468
Trained 3100 batches 	Training Loss: 0.144551
Trained 3150 batches 	Training Loss: 0.170081
Trained 3200 batches 	Training Loss: 0.123181
Trained 3250 batches 	Training Loss: 0.149178
Trained 3300 batches 	Training Loss: 0.180871
Trained 3350 batches 	Training Loss: 0.202693
Trained 3400 batches 	Training Loss: 0.125867
Trained 3450 batches 	Training Loss: 0.178243
Trained 3500 batches 	Training Loss: 0.178255
Trained 3550 batches 	Training Loss: 0.126516
Trained 3600 batches 	Training Loss: 0.235895
Trained 3650 batches 	Training Loss: 0.094312
Trained 3700 batches 	Training Loss: 0.139329
Trained 3750 batches 	Training Loss: 0.144210
Trained 3800 batches 	Training Loss: 0.192156
Trained 3850 batches 	Training Loss: 0.098010
Trained 3900 batches 	Training Loss: 0.151472
Trained 3950 batches 	Training Loss: 0.144462
Trained 4000 batches 	Training Loss: 0.129584
Trained 4050 batches 	Training Loss: 0.175819
Trained 4100 batches 	Training Loss: 0.230302
Trained 4150 batches 	Training Loss: 0.190285
Trained 4200 batches 	Training Loss: 0.143740
Trained 4250 batches 	Training Loss: 0.157592
Trained 4300 batches 	Training Loss: 0.156039
Trained 4350 batches 	Training Loss: 0.222995
Trained 4400 batches 	Training Loss: 0.160767
Trained 4450 batches 	Training Loss: 0.204321
Trained 4500 batches 	Training Loss: 0.126857
Trained 4550 batches 	Training Loss: 0.120349
Trained 4600 batches 	Training Loss: 0.128783
Trained 4650 batches 	Training Loss: 0.137369
Trained 4700 batches 	Training Loss: 0.202744
Trained 4750 batches 	Training Loss: 0.136266
Trained 4800 batches 	Training Loss: 0.127321
Trained 4850 batches 	Training Loss: 0.147870
Trained 4900 batches 	Training Loss: 0.112475
Epoch: 6 	Training Loss: 0.152534
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.7841573400974086
The AUROC of Cardiomegaly is 0.8928530238374175
The AUROC of Effusion is 0.8725071965940152
The AUROC of Infiltration is 0.7034249348960149
The AUROC of Mass is 0.8238267672377969
The AUROC of Nodule is 0.7224945585312766
The AUROC of Pneumonia is 0.7487328505279074
The AUROC of Pneumothorax is 0.8353474345479758
The AUROC of Consolidation is 0.785992247245045
The AUROC of Edema is 0.8818486932806979
The AUROC of Emphysema is 0.8480066759573508
The AUROC of Fibrosis is 0.7834752615088753
The AUROC of Pleural_Thickening is 0.7481152430714038
The AUROC of Hernia is 0.8915199714055938
Started epoch 7
Trained 50 batches 	Training Loss: 0.133341
Trained 100 batches 	Training Loss: 0.244246
Trained 150 batches 	Training Loss: 0.151208
Trained 200 batches 	Training Loss: 0.196626
Trained 250 batches 	Training Loss: 0.132884
Trained 300 batches 	Training Loss: 0.143716
Trained 350 batches 	Training Loss: 0.175695
Trained 400 batches 	Training Loss: 0.160813
Trained 450 batches 	Training Loss: 0.122721
Trained 500 batches 	Training Loss: 0.161790
Trained 550 batches 	Training Loss: 0.158758
Trained 600 batches 	Training Loss: 0.146034
Trained 650 batches 	Training Loss: 0.139244
Trained 700 batches 	Training Loss: 0.211015
Trained 750 batches 	Training Loss: 0.140104
Trained 800 batches 	Training Loss: 0.138820
Trained 850 batches 	Training Loss: 0.144386
Trained 900 batches 	Training Loss: 0.127379
Trained 950 batches 	Training Loss: 0.161265
Trained 1000 batches 	Training Loss: 0.143868
Trained 1050 batches 	Training Loss: 0.090158
Trained 1100 batches 	Training Loss: 0.086727
Trained 1150 batches 	Training Loss: 0.109421
Trained 1200 batches 	Training Loss: 0.100177
Trained 1250 batches 	Training Loss: 0.153305
Trained 1300 batches 	Training Loss: 0.096499
Trained 1350 batches 	Training Loss: 0.114275
Trained 1400 batches 	Training Loss: 0.182773
Trained 1450 batches 	Training Loss: 0.082017
Trained 1500 batches 	Training Loss: 0.206509
Trained 1550 batches 	Training Loss: 0.204129
Trained 1600 batches 	Training Loss: 0.185814
Trained 1650 batches 	Training Loss: 0.170395
Trained 1700 batches 	Training Loss: 0.196579
Trained 1750 batches 	Training Loss: 0.150706
Trained 1800 batches 	Training Loss: 0.167374
Trained 1850 batches 	Training Loss: 0.154666
Trained 1900 batches 	Training Loss: 0.216304
Trained 1950 batches 	Training Loss: 0.126632
Trained 2000 batches 	Training Loss: 0.220665
Trained 2050 batches 	Training Loss: 0.116931
Trained 2100 batches 	Training Loss: 0.169134
Trained 2150 batches 	Training Loss: 0.122228
Trained 2200 batches 	Training Loss: 0.133725
Trained 2250 batches 	Training Loss: 0.090626
Trained 2300 batches 	Training Loss: 0.106855
Trained 2350 batches 	Training Loss: 0.138018
Trained 2400 batches 	Training Loss: 0.112265
Trained 2450 batches 	Training Loss: 0.118620
Trained 2500 batches 	Training Loss: 0.174239
Trained 2550 batches 	Training Loss: 0.179182
Trained 2600 batches 	Training Loss: 0.160366
Trained 2650 batches 	Training Loss: 0.140096
Trained 2700 batches 	Training Loss: 0.199818
Trained 2750 batches 	Training Loss: 0.199588
Trained 2800 batches 	Training Loss: 0.136327
Trained 2850 batches 	Training Loss: 0.122638
Trained 2900 batches 	Training Loss: 0.169569
Trained 2950 batches 	Training Loss: 0.105258
Trained 3000 batches 	Training Loss: 0.181074
Trained 3050 batches 	Training Loss: 0.151593
Trained 3100 batches 	Training Loss: 0.204605
Trained 3150 batches 	Training Loss: 0.117181
Trained 3200 batches 	Training Loss: 0.115574
Trained 3250 batches 	Training Loss: 0.097214
Trained 3300 batches 	Training Loss: 0.198148
Trained 3350 batches 	Training Loss: 0.089082
Trained 3400 batches 	Training Loss: 0.124154
Trained 3450 batches 	Training Loss: 0.150233
Trained 3500 batches 	Training Loss: 0.129169
Trained 3550 batches 	Training Loss: 0.111375
Trained 3600 batches 	Training Loss: 0.193726
Trained 3650 batches 	Training Loss: 0.161778
Trained 3700 batches 	Training Loss: 0.106378
Trained 3750 batches 	Training Loss: 0.186135
Trained 3800 batches 	Training Loss: 0.168586
Trained 3850 batches 	Training Loss: 0.194114
Trained 3900 batches 	Training Loss: 0.192502
Trained 3950 batches 	Training Loss: 0.089616
Trained 4000 batches 	Training Loss: 0.158029
Trained 4050 batches 	Training Loss: 0.098583
Trained 4100 batches 	Training Loss: 0.141968
Trained 4150 batches 	Training Loss: 0.178960
Trained 4200 batches 	Training Loss: 0.178248
Trained 4250 batches 	Training Loss: 0.119375
Trained 4300 batches 	Training Loss: 0.117207
Trained 4350 batches 	Training Loss: 0.157933
Trained 4400 batches 	Training Loss: 0.271083
Trained 4450 batches 	Training Loss: 0.119287
Trained 4500 batches 	Training Loss: 0.163125
Trained 4550 batches 	Training Loss: 0.148499
Trained 4600 batches 	Training Loss: 0.145318
Trained 4650 batches 	Training Loss: 0.133922
Trained 4700 batches 	Training Loss: 0.146846
Trained 4750 batches 	Training Loss: 0.149147
Trained 4800 batches 	Training Loss: 0.164961
Trained 4850 batches 	Training Loss: 0.187600
Trained 4900 batches 	Training Loss: 0.104624
Epoch: 7 	Training Loss: 0.150513
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.807
The AUROC of Atelectasis is 0.7866995536212817
The AUROC of Cardiomegaly is 0.8858168276263952
The AUROC of Effusion is 0.8722727781271922
The AUROC of Infiltration is 0.6994271985842168
The AUROC of Mass is 0.8159838812782196
The AUROC of Nodule is 0.7225064428310163
The AUROC of Pneumonia is 0.7169839551672494
The AUROC of Pneumothorax is 0.8469476885035003
The AUROC of Consolidation is 0.7826741745058566
The AUROC of Edema is 0.8821564446989159
The AUROC of Emphysema is 0.8615695584123695
The AUROC of Fibrosis is 0.7791075498844158
The AUROC of Pleural_Thickening is 0.7571780885170798
The AUROC of Hernia is 0.8913795524464811
Started epoch 8
Trained 50 batches 	Training Loss: 0.161390
Trained 100 batches 	Training Loss: 0.125530
Trained 150 batches 	Training Loss: 0.184145
Trained 200 batches 	Training Loss: 0.100981
Trained 250 batches 	Training Loss: 0.161839
Trained 300 batches 	Training Loss: 0.189481
Trained 350 batches 	Training Loss: 0.190659
Trained 400 batches 	Training Loss: 0.177444
Trained 450 batches 	Training Loss: 0.217450
Trained 500 batches 	Training Loss: 0.157872
Trained 550 batches 	Training Loss: 0.185970
Trained 600 batches 	Training Loss: 0.131751
Trained 650 batches 	Training Loss: 0.149280
Trained 700 batches 	Training Loss: 0.120469
Trained 750 batches 	Training Loss: 0.161497
Trained 800 batches 	Training Loss: 0.079115
Trained 850 batches 	Training Loss: 0.098461
Trained 900 batches 	Training Loss: 0.142247
Trained 950 batches 	Training Loss: 0.120970
Trained 1000 batches 	Training Loss: 0.128587
Trained 1050 batches 	Training Loss: 0.105355
Trained 1100 batches 	Training Loss: 0.170364
Trained 1150 batches 	Training Loss: 0.150353
Trained 1200 batches 	Training Loss: 0.147017
Trained 1250 batches 	Training Loss: 0.171988
Trained 1300 batches 	Training Loss: 0.121072
Trained 1350 batches 	Training Loss: 0.130291
Trained 1400 batches 	Training Loss: 0.117937
Trained 1450 batches 	Training Loss: 0.163076
Trained 1500 batches 	Training Loss: 0.112483
Trained 1550 batches 	Training Loss: 0.124691
Trained 1600 batches 	Training Loss: 0.095437
Trained 1650 batches 	Training Loss: 0.144776
Trained 1700 batches 	Training Loss: 0.079332
Trained 1750 batches 	Training Loss: 0.213665
Trained 1800 batches 	Training Loss: 0.086133
Trained 1850 batches 	Training Loss: 0.165974
Trained 1900 batches 	Training Loss: 0.074801
Trained 1950 batches 	Training Loss: 0.181217
Trained 2000 batches 	Training Loss: 0.135112
Trained 2050 batches 	Training Loss: 0.156900
Trained 2100 batches 	Training Loss: 0.075466
Trained 2150 batches 	Training Loss: 0.175506
Trained 2200 batches 	Training Loss: 0.210839
Trained 2250 batches 	Training Loss: 0.158947
Trained 2300 batches 	Training Loss: 0.154035
Trained 2350 batches 	Training Loss: 0.174795
Trained 2400 batches 	Training Loss: 0.166143
Trained 2450 batches 	Training Loss: 0.162086
Trained 2500 batches 	Training Loss: 0.300311
Trained 2550 batches 	Training Loss: 0.200299
Trained 2600 batches 	Training Loss: 0.146440
Trained 2650 batches 	Training Loss: 0.096097
Trained 2700 batches 	Training Loss: 0.123529
Trained 2750 batches 	Training Loss: 0.104155
Trained 2800 batches 	Training Loss: 0.194007
Trained 2850 batches 	Training Loss: 0.176640
Trained 2900 batches 	Training Loss: 0.112362
Trained 2950 batches 	Training Loss: 0.102809
Trained 3000 batches 	Training Loss: 0.087124
Trained 3050 batches 	Training Loss: 0.182829
Trained 3100 batches 	Training Loss: 0.195657
Trained 3150 batches 	Training Loss: 0.137368
Trained 3200 batches 	Training Loss: 0.180042
Trained 3250 batches 	Training Loss: 0.192296
Trained 3300 batches 	Training Loss: 0.127838
Trained 3350 batches 	Training Loss: 0.131179
Trained 3400 batches 	Training Loss: 0.115954
Trained 3450 batches 	Training Loss: 0.150344
Trained 3500 batches 	Training Loss: 0.112987
Trained 3550 batches 	Training Loss: 0.119681
Trained 3600 batches 	Training Loss: 0.191799
Trained 3650 batches 	Training Loss: 0.079517
Trained 3700 batches 	Training Loss: 0.115028
Trained 3750 batches 	Training Loss: 0.136941
Trained 3800 batches 	Training Loss: 0.129142
Trained 3850 batches 	Training Loss: 0.117027
Trained 3900 batches 	Training Loss: 0.104509
Trained 3950 batches 	Training Loss: 0.183529
Trained 4000 batches 	Training Loss: 0.169221
Trained 4050 batches 	Training Loss: 0.142289
Trained 4100 batches 	Training Loss: 0.110425
Trained 4150 batches 	Training Loss: 0.120515
Trained 4200 batches 	Training Loss: 0.178000
Trained 4250 batches 	Training Loss: 0.110379
Trained 4300 batches 	Training Loss: 0.159719
Trained 4350 batches 	Training Loss: 0.191656
Trained 4400 batches 	Training Loss: 0.079933
Trained 4450 batches 	Training Loss: 0.103950
Trained 4500 batches 	Training Loss: 0.150128
Trained 4550 batches 	Training Loss: 0.106076
Trained 4600 batches 	Training Loss: 0.202351
Trained 4650 batches 	Training Loss: 0.181633
Trained 4700 batches 	Training Loss: 0.252606
Trained 4750 batches 	Training Loss: 0.096228
Trained 4800 batches 	Training Loss: 0.151809
Trained 4850 batches 	Training Loss: 0.165416
Trained 4900 batches 	Training Loss: 0.131488
Epoch: 8 	Training Loss: 0.148811
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.814
The AUROC of Atelectasis is 0.7871515837772172
The AUROC of Cardiomegaly is 0.9086435677821549
The AUROC of Effusion is 0.8754195493359782
The AUROC of Infiltration is 0.7064522381077822
The AUROC of Mass is 0.8301630280702817
The AUROC of Nodule is 0.7174653723050333
The AUROC of Pneumonia is 0.73041394981229
The AUROC of Pneumothorax is 0.8485056722302976
The AUROC of Consolidation is 0.7839003411156609
The AUROC of Edema is 0.8869676503160385
The AUROC of Emphysema is 0.8612243700225067
The AUROC of Fibrosis is 0.7790937791692186
The AUROC of Pleural_Thickening is 0.7565822861574897
The AUROC of Hernia is 0.9291097012480266
Started epoch 9
Trained 50 batches 	Training Loss: 0.142128
Trained 100 batches 	Training Loss: 0.152237
Trained 150 batches 	Training Loss: 0.204036
Trained 200 batches 	Training Loss: 0.130203
Trained 250 batches 	Training Loss: 0.158382
Trained 300 batches 	Training Loss: 0.115821
Trained 350 batches 	Training Loss: 0.123916
Trained 400 batches 	Training Loss: 0.194624
Trained 450 batches 	Training Loss: 0.135156
Trained 500 batches 	Training Loss: 0.120390
Trained 550 batches 	Training Loss: 0.178523
Trained 600 batches 	Training Loss: 0.099669
Trained 650 batches 	Training Loss: 0.252078
Trained 700 batches 	Training Loss: 0.137279
Trained 750 batches 	Training Loss: 0.180344
Trained 800 batches 	Training Loss: 0.161518
Trained 850 batches 	Training Loss: 0.192339
Trained 900 batches 	Training Loss: 0.122032
Trained 950 batches 	Training Loss: 0.158225
Trained 1000 batches 	Training Loss: 0.144404
Trained 1050 batches 	Training Loss: 0.210565
Trained 1100 batches 	Training Loss: 0.160398
Trained 1150 batches 	Training Loss: 0.188313
Trained 1200 batches 	Training Loss: 0.167647
Trained 1250 batches 	Training Loss: 0.107576
Trained 1300 batches 	Training Loss: 0.119638
Trained 1350 batches 	Training Loss: 0.143932
Trained 1400 batches 	Training Loss: 0.194450
Trained 1450 batches 	Training Loss: 0.196703
Trained 1500 batches 	Training Loss: 0.117091
Trained 1550 batches 	Training Loss: 0.226413
Trained 1600 batches 	Training Loss: 0.158495
Trained 1650 batches 	Training Loss: 0.184057
Trained 1700 batches 	Training Loss: 0.154151
Trained 1750 batches 	Training Loss: 0.194731
Trained 1800 batches 	Training Loss: 0.168949
Trained 1850 batches 	Training Loss: 0.094699
Trained 1900 batches 	Training Loss: 0.246206
Trained 1950 batches 	Training Loss: 0.163989
Trained 2000 batches 	Training Loss: 0.218336
Trained 2050 batches 	Training Loss: 0.145088
Trained 2100 batches 	Training Loss: 0.170687
Trained 2150 batches 	Training Loss: 0.129131
Trained 2200 batches 	Training Loss: 0.113497
Trained 2250 batches 	Training Loss: 0.216913
Trained 2300 batches 	Training Loss: 0.064941
Trained 2350 batches 	Training Loss: 0.131947
Trained 2400 batches 	Training Loss: 0.129274
Trained 2450 batches 	Training Loss: 0.141987
Trained 2500 batches 	Training Loss: 0.142148
Trained 2550 batches 	Training Loss: 0.139254
Trained 2600 batches 	Training Loss: 0.175424
Trained 2650 batches 	Training Loss: 0.165585
Trained 2700 batches 	Training Loss: 0.194407
Trained 2750 batches 	Training Loss: 0.110938
Trained 2800 batches 	Training Loss: 0.256106
Trained 2850 batches 	Training Loss: 0.171314
Trained 2900 batches 	Training Loss: 0.150657
Trained 2950 batches 	Training Loss: 0.138374
Trained 3000 batches 	Training Loss: 0.123020
Trained 3050 batches 	Training Loss: 0.096667
Trained 3100 batches 	Training Loss: 0.165738
Trained 3150 batches 	Training Loss: 0.120527
Trained 3200 batches 	Training Loss: 0.170898
Trained 3250 batches 	Training Loss: 0.155278
Trained 3300 batches 	Training Loss: 0.153434
Trained 3350 batches 	Training Loss: 0.176569
Trained 3400 batches 	Training Loss: 0.107841
Trained 3450 batches 	Training Loss: 0.144837
Trained 3500 batches 	Training Loss: 0.202616
Trained 3550 batches 	Training Loss: 0.182098
Trained 3600 batches 	Training Loss: 0.138077
Trained 3650 batches 	Training Loss: 0.156700
Trained 3700 batches 	Training Loss: 0.109446
Trained 3750 batches 	Training Loss: 0.203740
Trained 3800 batches 	Training Loss: 0.136310
Trained 3850 batches 	Training Loss: 0.102073
Trained 3900 batches 	Training Loss: 0.099074
Trained 3950 batches 	Training Loss: 0.224142
Trained 4000 batches 	Training Loss: 0.112159
Trained 4050 batches 	Training Loss: 0.199057
Trained 4100 batches 	Training Loss: 0.102036
Trained 4150 batches 	Training Loss: 0.111851
Trained 4200 batches 	Training Loss: 0.084870
Trained 4250 batches 	Training Loss: 0.102258
Trained 4300 batches 	Training Loss: 0.103189
Trained 4350 batches 	Training Loss: 0.144983
Trained 4400 batches 	Training Loss: 0.164908
Trained 4450 batches 	Training Loss: 0.168587
Trained 4500 batches 	Training Loss: 0.152957
Trained 4550 batches 	Training Loss: 0.142935
Trained 4600 batches 	Training Loss: 0.190739
Trained 4650 batches 	Training Loss: 0.175442
Trained 4700 batches 	Training Loss: 0.187397
Trained 4750 batches 	Training Loss: 0.178898
Trained 4800 batches 	Training Loss: 0.133292
Trained 4850 batches 	Training Loss: 0.210065
Trained 4900 batches 	Training Loss: 0.174896
Epoch: 9 	Training Loss: 0.146980
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.812
The AUROC of Atelectasis is 0.794984086026761
The AUROC of Cardiomegaly is 0.8915388397545236
The AUROC of Effusion is 0.8665517126684243
The AUROC of Infiltration is 0.7072401412657716
The AUROC of Mass is 0.8280559898776416
The AUROC of Nodule is 0.7406766568144355
The AUROC of Pneumonia is 0.7388898911540079
The AUROC of Pneumothorax is 0.8439262860675322
The AUROC of Consolidation is 0.7831085328136514
The AUROC of Edema is 0.8790751636695119
The AUROC of Emphysema is 0.8664169549313843
The AUROC of Fibrosis is 0.7570717112987522
The AUROC of Pleural_Thickening is 0.7522696346135636
The AUROC of Hernia is 0.9133061856679049
Started epoch 10
Trained 50 batches 	Training Loss: 0.091097
Trained 100 batches 	Training Loss: 0.140600
Trained 150 batches 	Training Loss: 0.114137
Trained 200 batches 	Training Loss: 0.126991
Trained 250 batches 	Training Loss: 0.220110
Trained 300 batches 	Training Loss: 0.186294
Trained 350 batches 	Training Loss: 0.184357
Trained 400 batches 	Training Loss: 0.175077
Trained 450 batches 	Training Loss: 0.225970
Trained 500 batches 	Training Loss: 0.129438
Trained 550 batches 	Training Loss: 0.125216
Trained 600 batches 	Training Loss: 0.120530
Trained 650 batches 	Training Loss: 0.189579
Trained 700 batches 	Training Loss: 0.111953
Trained 750 batches 	Training Loss: 0.138148
Trained 800 batches 	Training Loss: 0.184738
Trained 850 batches 	Training Loss: 0.169744
Trained 900 batches 	Training Loss: 0.183261
Trained 950 batches 	Training Loss: 0.112797
Trained 1000 batches 	Training Loss: 0.162163
Trained 1050 batches 	Training Loss: 0.177890
Trained 1100 batches 	Training Loss: 0.094569
Trained 1150 batches 	Training Loss: 0.203245
Trained 1200 batches 	Training Loss: 0.110742
Trained 1250 batches 	Training Loss: 0.203283
Trained 1300 batches 	Training Loss: 0.127782
Trained 1350 batches 	Training Loss: 0.131863
Trained 1400 batches 	Training Loss: 0.125997
Trained 1450 batches 	Training Loss: 0.210686
Trained 1500 batches 	Training Loss: 0.114538
Trained 1550 batches 	Training Loss: 0.098577
Trained 1600 batches 	Training Loss: 0.195595
Trained 1650 batches 	Training Loss: 0.147484
Trained 1700 batches 	Training Loss: 0.151790
Trained 1750 batches 	Training Loss: 0.217429
Trained 1800 batches 	Training Loss: 0.086710
Trained 1850 batches 	Training Loss: 0.186106
Trained 1900 batches 	Training Loss: 0.120221
Trained 1950 batches 	Training Loss: 0.162613
Trained 2000 batches 	Training Loss: 0.115327
Trained 2050 batches 	Training Loss: 0.143401
Trained 2100 batches 	Training Loss: 0.188301
Trained 2150 batches 	Training Loss: 0.110575
Trained 2200 batches 	Training Loss: 0.153036
Trained 2250 batches 	Training Loss: 0.166263
Trained 2300 batches 	Training Loss: 0.189826
Trained 2350 batches 	Training Loss: 0.128390
Trained 2400 batches 	Training Loss: 0.106736
Trained 2450 batches 	Training Loss: 0.138263
Trained 2500 batches 	Training Loss: 0.149581
Trained 2550 batches 	Training Loss: 0.148285
Trained 2600 batches 	Training Loss: 0.134840
Trained 2650 batches 	Training Loss: 0.197166
Trained 2700 batches 	Training Loss: 0.183837
Trained 2750 batches 	Training Loss: 0.121210
Trained 2800 batches 	Training Loss: 0.166473
Trained 2850 batches 	Training Loss: 0.102874
Trained 2900 batches 	Training Loss: 0.143928
Trained 2950 batches 	Training Loss: 0.103459
Trained 3000 batches 	Training Loss: 0.157601
Trained 3050 batches 	Training Loss: 0.082536
Trained 3100 batches 	Training Loss: 0.130333
Trained 3150 batches 	Training Loss: 0.142861
Trained 3200 batches 	Training Loss: 0.131666
Trained 3250 batches 	Training Loss: 0.158532
Trained 3300 batches 	Training Loss: 0.100886
Trained 3350 batches 	Training Loss: 0.179601
Trained 3400 batches 	Training Loss: 0.126817
Trained 3450 batches 	Training Loss: 0.175689
Trained 3500 batches 	Training Loss: 0.154380
Trained 3550 batches 	Training Loss: 0.259151
Trained 3600 batches 	Training Loss: 0.140951
Trained 3650 batches 	Training Loss: 0.175863
Trained 3700 batches 	Training Loss: 0.191680
Trained 3750 batches 	Training Loss: 0.115655
Trained 3800 batches 	Training Loss: 0.170093
Trained 3850 batches 	Training Loss: 0.127298
Trained 3900 batches 	Training Loss: 0.127244
Trained 3950 batches 	Training Loss: 0.182214
Trained 4000 batches 	Training Loss: 0.139443
Trained 4050 batches 	Training Loss: 0.128344
Trained 4100 batches 	Training Loss: 0.128352
Trained 4150 batches 	Training Loss: 0.076986
Trained 4200 batches 	Training Loss: 0.089749
Trained 4250 batches 	Training Loss: 0.081555
Trained 4300 batches 	Training Loss: 0.243695
Trained 4350 batches 	Training Loss: 0.098510
Trained 4400 batches 	Training Loss: 0.156945
Trained 4450 batches 	Training Loss: 0.070786
Trained 4500 batches 	Training Loss: 0.111477
Trained 4550 batches 	Training Loss: 0.181693
Trained 4600 batches 	Training Loss: 0.123949
Trained 4650 batches 	Training Loss: 0.151696
Trained 4700 batches 	Training Loss: 0.133894
Trained 4750 batches 	Training Loss: 0.143484
Trained 4800 batches 	Training Loss: 0.111121
Trained 4850 batches 	Training Loss: 0.142089
Trained 4900 batches 	Training Loss: 0.146383
Epoch: 10 	Training Loss: 0.144638
AUROCs on validation dataset:
Evaluating test data...	 test_loader: 701
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700Evaluating time lapse: 0.0 min
The average AUROC is 0.816
The AUROC of Atelectasis is 0.7930819869164976
The AUROC of Cardiomegaly is 0.9007214354653669
The AUROC of Effusion is 0.8738247326979365
The AUROC of Infiltration is 0.707299318013932
The AUROC of Mass is 0.819860701061869
The AUROC of Nodule is 0.7260265430699494
The AUROC of Pneumonia is 0.729896709420433
The AUROC of Pneumothorax is 0.8455056555226972
The AUROC of Consolidation is 0.7836922421045924
The AUROC of Edema is 0.8828307103520495
The AUROC of Emphysema is 0.8639978580261682
The AUROC of Fibrosis is 0.7844715428170453
The AUROC of Pleural_Thickening is 0.7579990456522919
The AUROC of Hernia is 0.9570232882716129
Training time lapse: 193.0 min
Evaluating test data...	 test_loader: 1402
batch: 0batch: 50batch: 100batch: 150batch: 200batch: 250batch: 300batch: 350batch: 400batch: 450batch: 500batch: 550batch: 600batch: 650batch: 700batch: 750batch: 800batch: 850batch: 900batch: 950batch: 1000batch: 1050batch: 1100batch: 1150batch: 1200batch: 1250batch: 1300batch: 1350batch: 1400Evaluating time lapse: 1.0 min
The average AUROC is 0.809
The AUROC of Atelectasis is 0.7917808758329281
The AUROC of Cardiomegaly is 0.8894232053667541
The AUROC of Effusion is 0.874267367671846
The AUROC of Infiltration is 0.6956585651831015
The AUROC of Mass is 0.8166990096048538
The AUROC of Nodule is 0.7101460760641891
The AUROC of Pneumonia is 0.7151066673248768
The AUROC of Pneumothorax is 0.8574697596955824
The AUROC of Consolidation is 0.7982187740410935
The AUROC of Edema is 0.8949200747209745
The AUROC of Emphysema is 0.8675964470985337
The AUROC of Fibrosis is 0.7975232672003557
The AUROC of Pleural_Thickening is 0.7705941528140343
The AUROC of Hernia is 0.8481116794232888
